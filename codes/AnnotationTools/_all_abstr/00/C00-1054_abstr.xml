<?xml version="1.0" standalone="yes"?>

<Paper uid="C00-1054">
<Title>Finite-state Multimodal Parsing and Understanding</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> Multimodal interfaces require effective parsing and nn(lerstanding of utterances whose content is distributed across multiple input modes. Johnston 1998 presents an approach in which strategies lbr multimodal integration are stated declaratively using a unification-based grammar that is used by a mnltidilnensional chart parser to compose inputs. This approach is highly expressive and supports a broad class of interfaces, but offers only limited potential for lnutual compensation among the input modes, is subject to signilicant concerns in terms o1' COml)utational complexity, and complicates selection among alternative multimodal interpretations of the input.</Paragraph>
<Paragraph position="1"> In tiffs papeh we l)resent an alternative approacla in which multimodal lmrsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation. This approach is significantly more efficienl, enables tight-coupling of multimodal understanding with speech recognition, and provides a general probabilistic fralnework for multimodal ambiguity resolution.</Paragraph>
</Section>
</Paper>

