<?xml version="1.0" standalone="yes"?>

<Paper uid="C00-2087">
<Title>Interaction Grammars</Title>
<Section position="1" start_page="0" end_page="600" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> Interaction Grammars (IG) are a new linguistic formalism which is based on descriptions of under~ specified trees in the fl'amework of intuitionistic linear logic (ILL). Syntactic composition, which is expressed by deduction in linear logic, is controlled by a system of polarized features. In this way, parsing amounts to generating models of tree descriptions and it is implemented as a constraint satisfaction problem.</Paragraph>
<Paragraph position="1"> Introduction IG can be presented as an attempt to bring together flmdalnental ideas, some coming froln Tree Adjoining Grammars (TAG) and others from Categorial GraInmars (CG), in order to overcome the specific limitations of each of these formalisms. The computational and linguistic relevance of TAG lies in its adjunction operation (Joshi et al., 1975; Kroch and Joshi, 1985) but the simplicity of its mechanism has a counterpart in the inflation of the lexicons that arc required for expressing all grammatical phenomena of a language. Every time that a word is used in a new syntactic context, wlfieh can differ only by word order for example, a new elementary tree, which encodes this context, must be added to the lexicon in a direct manner or by means of a lexical rule. In this way, lexicons quickly become colossal, very awkward to use and very hard to maintain.</Paragraph>
<Paragraph position="2"> Recent works aim to solve tiffs problem by factorizing linguistic information with the notions of nnderspecified trees and tree descriptions. These notions were introduced for TAG by Vijay-Shanker with the motivation of making adjunetion monotone (Vijay-Shanker, 1992). Now, they are exploited fruitfully in various directions: for structuring TAG lexicons in hierarchical systems of modules (Candito, 1999) or for expressing semantic ambiguity (Muskens and Krahmer, 1998; Egg et al., 1998) for instance. At the same time, these notions are a way of relaxing the primitive adjunetion operation in order to capture linguistic phenomena: such a possibility is exploited by (Rambow et 31., 1995) within D-Tree Grammars and by (Kalhneyer, 1999) within Tree Description ~l'alnlilars.</Paragraph>
<Paragraph position="3"> Unfortunately, tim counterpart of a more flexible framework is often over-generation and a loss of computational efficiency in the absence of principles for controlling syntactic composition. By looking at CG, we can find some answers to this preoccupation: a fundamental idea of CG is that grammatical constituents are viewed as consumable resources (Retord, 2000); these resources are divided into positive and negative resources which are complementary and search to neutralize themselves mutually. The core of CG is the Lambek Calculus (Lmnbek, 1958): by combining resource sensitivity and order sensitivity, this logic is a good candidate for representing the syntax of natural languages but at the same time, this combination entails a rigidity which limits its expressive power greatly. An appropriate way of relaxing this rigidity constitutes an important research area in CG (Moortgart, 1996).</Paragraph>
<Paragraph position="4"> The principle of IG is to combine the powerflfl notion of under-specified tree description linked to the TAG ptfilosophy with control of syntactic composition by a system of polarities in accordance with the CG philosot)hy. More precisely, the basic objects of IG are syntactic dcscriptions which express dependencies between syntactic constituents in the shape of under-specified trees. Word order is referred to the same level as morphological information, which is represented by a system of features which are linked to the nodes of syntactic descriptions. Whereas a feature is usually a pair (attribute, value), an IG feature is a triplet (attribute, polarity, value) where polarity can take one of the three values -1, 0 or -t-1 and behaves like an electrostatic charge: for instance, a noun phrase which is waiting to receive a syntactic function in a sentence, carries a negative feature of type fltnct while a finite verb which is searching for its subject, carries a positive t'eature funct with value s@j. Attraction between these dual features will make possible the fact that the verb finds its subject and, simultaneously, the noun phrase finds its flmction in the sentence. (Muskens and Krahmer, 1998) also recognized the necessity of introducing the notion of polarity in tree descrip- null tions as a mechanism for controlling syntactic composition; the difference with respect to IG lies in the granularity of the polarization wlfich is finer in li(l: in their proposal, the polarized objects are constituents, that is description nodes, whereas in IG one constituent can include several features with opposite polarities.</Paragraph>
<Paragraph position="5"> Tile frmnework which is chosen tbr tel)resenting syntactic descriptions in this patmr is that of linear logic (Girard, 1987), more precisely a fragment of ILL (Lincoln, 1992). The resource sensitivity of linear logic allows one to express the fact that 1)olarized features behave as consumable resources in \[G: a positive feature has to find its dual fea.ture once and only once. If we try to use classical or intuitionistic logic for modelling IG, the contraction and weakening rules, which are inherent in these logics, entail a loss of resource-sensitivity: tbr instance, a verb could take two subjects by appealing to the contraction rule and some noun phrases wouhl not need to find their syntactic role in a sentence by appealing to the weakening rule. By discarding these two rules, linear logic provides a Kamework that exactly corresponds to the &amp;quot;electrostati(-&amp;quot; laws that control 1)olarized features.</Paragraph>
<Paragraph position="6"> In this framework, i)arsing takes the shatm of logical deduction of closed syntactic dcscriptions from airy syntactic descriptions: a description is said to be. closed when it represents a completely specified syntactic tree where all features are neutralized.</Paragraph>
<Paragraph position="7"> If linear logic provides an elegant t Yamework tbr representing IG, it gives no method for parsing efficiently and avoiding the coufl)inatory explosion that may follow from the flexibility of the fi)rmalism. An ai~swer to this problem is given by the paradigm of constraint solving. Parsing a phrase can be regarded as generating models of the partial descrit)tion which is provided by a l('xicon for the words of this phrase.</Paragraph>
<Paragraph position="8"> The process is monotone and can be expressed as a constraint satisfactio'n problem. This constraint-based approach was inspired by the work of (l)uchier and C., 1999; l)uchier and Thater, 1999) on dominance constraints. (Blache, 1999) shows the advantages of such an apI/roaeh t)oth from a linguistic and emnputational viewpoint with the formalism that lie prol)oses mid lie calls Property Grammars.</Paragraph>
</Section>
</Paper>

