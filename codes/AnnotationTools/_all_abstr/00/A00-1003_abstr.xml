<?xml version="1.0" standalone="yes"?>

<Paper uid="A00-1003">
<Title>Language Raw Precision (%) 80 86 C/M French (Human) French (AltaVista) French (Transparent Language)</Title>
<Section position="2" start_page="13" end_page="13" type="abstr">
<SectionTitle>
4 Evaluation
</SectionTitle>
<Paragraph position="0"> of businesses.</Paragraph>
<Paragraph position="1"> The use of phrase recognition has been shown to be helpful, and, optimally, we would like to include it. Hull and Grefenstette 1996 showed the upper bound of the improvements possible by using lexicalized phrases. Every phrase that appeared was added to the dictionary, and that tactic did aid retrieval. Both statistical co-occurrence and syntactic phrases are also possible approaches. Unfortunately, the extra-system approach we take here relies heavily on the external machine translation to preserve phrases intact. If AltaVista (or, in the case of Langenscheidt, the user) recognizes a phrase and translates it as a unit, the translation is better and retrieval is likely to be better. If, however, the translation mistakenly misses a phrase, retrieval quality is likely to be worse. As for compositional noun phrases, if the translation preserves normal word order, then the PicmreQuest-internal noun phrase recognition will take effect. That is, ifjeune fille translates as young girl, then PictureQuest will understand that young is an adjective modifying girl. In the more difficult case, if the translation preserves the correct order in translating la selva africana, i.e. the African jungle, then noun phrase recognition will work. If, however, it comes out as the jungle African, then retrieval will Evaluating precision and recall on a large corpus is a difficult task. We used the evaluation methods detailed in Flank 1998. Precision was evaluated using a crossing measure, whereby any image ranked higher than a better match was penalized. Recall per se was measured only with respect to a defined subset of the images. Ranking incorporates some recall measures into the precision score, since images ranked too low are a recall problem, and images marked too high are a precision problem. If there are three good matches, and the third shows up as #4, the bogus #3 is a precision problem, and the too-low #4 is a recall problem.</Paragraph>
<Paragraph position="2"> For evaluation of the overall cross-language retrieval performance, we simply measured the ratio between the cross-language and monolingual retrieval accuracy (C/M%). This is standard; see, for example, Jang et al. 1999. Table 1 illustrates the percentage of monolingual retrieval performance we achieved for the translation tests performed. In this instance, we take the precision performance of the human-translated queries and normalize it to 100%, and adjust the other translation modalities relative to the human baseline.</Paragraph>
<Paragraph position="3">  simplifies word choice issues. We used a variety o f machine translation systems, none of them high-end and all of them free, and nonetheless achieved commercially viable results. 5 Appendix: Data Source Example Score 100 0 100 100 90 (2 of 20 bad) wearing red is lost 75 (5 of 20 bad) 100 Human men repairing road AV men repairing wagon Lang. man repair road Human woman wearing red shopping in store AV woman dressed red buying in one tends Lang. woman clothe red buy in shop Several other factors make the PictureQuest application a particularly good application for machine translation technology. Unlike document translation, there is no need to match every word in the description; useful images may be retrieved even if a word or two is lost. There are no discourse issues at all: searches never use anaphora, and no one cares if the translated query sounds good or not. In addition, the fact that the objects being retrieved were images greatly simplified the endeavor. Under normal circumstances, developing a user-friendly interface is a major challenge. Users with only limited (or nonexistent) reading knowledge of the language of the documents need a way to determine, first, which ones are useful, and second, what they say. In the PictureQuest application, however, the retrieved assets are images. Users can instantly assess which images meet their needs. In conclusion, it appears that simple on-line translation of queries can support effective cross-language information retrieval, for certain applications. We showed how an image retrieval application eliminates some of the problems of cross-language retrieval, and how carefully tuned WordNet expansion Human cars driving on highway AV cars handling by freeway Lang. cart handle for expressway the the 80' (4 of 20 bad) the 0 Human lions hunting in the 80 (1 of 5 African forest bad) AV lions hunting in the 80 (1 of 5 African forest bad) Lang. lion hunt in thejungle 45 (11 of gSt ] I 20 bad) Human juggler using colorful balls AV Lang.</Paragraph>
<Paragraph position="4"> 67 (1 of 3 bad) juggler with using balls of 50 (4 of 8 colors bad) juggler by means of use (0; 1 ball colour should be there) Source Example Score Human blonde children playing 90(#3 with marbles should be #1; remainder of top 20 Langenscheidt Langenscheidt, word-by-word: 63% (70% normalized) For the Langenscheidt word-by-word, we used the bilingual dictionary to translate each word separately as if we k n e w no English at all, and always took the first translation. We made the following adjustments: 1. Left out &amp;quot;una,&amp;quot; since Langenscheidt m a p p e d it to &amp;quot;unir&amp;quot; rather than to either a or  play the violin gamble any violin pleasures of the body desoxyribonucleic acid the synthesis of the Desoxynribonukleinsaeure black cars black auto playing together young together play</Paragraph>
</Section>
</Paper>

