<?xml version="1.0" standalone="yes"?>

<Paper uid="N06-2001">
<Title>Factored Neural Language Models</Title>
<Section position="1" start_page="0" end_page="0" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model signi cantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models.</Paragraph>
</Section>
</Paper>

