<?xml version="1.0" standalone="yes"?>

<Paper uid="P06-2113">
<Title>Combining Statistical and Knowledge-based Spoken Language Understanding in Conditional Models</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance. The traditional knowledge-based approach to this problem is very expensive -- it requires joint expertise in natural language processing and speech recognition, and best practices in language engineering for every new domain.</Paragraph>
<Paragraph position="1"> On the other hand, a statistical learning approach needs a large amount of annotated data for model training, which is seldom available in practical applications outside of large research labs. A generative HMM/CFG composite model, which integrates easy-toobtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement.</Paragraph>
<Paragraph position="2"> The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework. We also study and compare conditional random fields (CRFs) with perceptron learning for SLU. Experimental results show that the conditional models achieve more than 20% relative reduction in slot error rate over the HMM/CFG model, which had already achieved an SLU accuracy at the same level as the best results reported on the ATIS data.</Paragraph>
</Section>
</Paper>

