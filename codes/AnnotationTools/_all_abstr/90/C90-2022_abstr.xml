<?xml version="1.0" standalone="yes"?>

<Paper uid="C90-2022">
<Title>Generating from a Deep Structure *</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
1 Introduction
</SectionTitle>
<Paragraph position="0"> not derivable under G. Representations that cannot be derived under a grammar G are said to be noncanonical with respect to G.</Paragraph>
<Paragraph position="1"> In this paper, we present a generation algorithm for Unification Categorial Grammar \[6\] (,ca) which addresses the problem raised by non-canonicM input (section 2 and 3). An interesting upshot of the strategy we propose is that it allows :\['or language independent generation. Section 4 illustrates this point by considering how, starting from one semantic representation, two sentences can be generated: one in English and one in French. Section 5 relates our work to previous proposals by Van Noord and Shieber.</Paragraph>
<Paragraph position="2"> Two major requirements on a generator is that it be correct and complete. A generator is said to be correct if given two semantic representations R1 and R.~ which are not semantically equivalent, R1 and R2 do not generate the same string. A generator is said to be complete if any two senmn tically equivalent representations generate the same set of strings.</Paragraph>
<Paragraph position="3"> An obvious case of incompleteness occurs when the generator fails to terminate on some well-defined input. Another less obvious cause for incompleteness can be explMned as follows. Consider a grammar G and its associated semantic representation language L. It is often the case that syntactically different strings of L will have equivalent semantics. A simple case in point is the semantic equivalence holding between C/ A C/ and C/ A C/ in e.g. propositional logic. On the other hand, it is also often the case that the same grammar G will not derive for a given string all the formulae which may represent its meaning. From the point of view of generation, this means that given two semanticMly equivalent representations R1 and R2 there is always a possibility that R1 generates a string S but that R2 doesn't because R2 is *The work reported here has been carried out ,as part of the ESPRIT project P393 ACORD on &amp;quot;The Construction and Interrogation of Knowledge-B~es using Natural Language Text and Graphics&amp;quot;. It is the result of joint work with Michael Reape of the Center for Cognitive Science, University of Edinburgh (Scotland, UK).</Paragraph>
<Paragraph position="4"> 2 Generating from a deep structure It is sometimes the case that a grammar will assign to a string several possible derivations with equivalent sementies. This phenomena is particularly acute in categorial grammars \[2\] and is referred to in the literature as that of spurious ambiguity. In grammars where the semantics is built by unification, the syntactic differences holding between these equivalent semantics resides in the relative ordering of the subformulae within the formula. That is, there is a direct relationship between the syntactic shape of a semantic formula and the derivational history of the corresponding string. Consequently, a given formula will be non-canonk:al wrt to a particular grammar G if the relative sequencing of its subformulae does not reflect a possible derivation in C. Hence, to allow for generation from non-canonical input, we need to abstract away from the derivational information reflected in the linear ordering of the input formula. Three major alternatives come to mind. First, we could try to generate all sentences whose semantics are logically equivalent to the input semantics. In uc(~, this means that generation is carried out with the two additional logical axioms of associativity and commutativity. However, this solution produces a search space factorial in the number of conjunction s and must thus be rejected as computationally intractable.</Paragraph>
<Paragraph position="6"> The second possibility is to define a semantic representation language for which all well-formed formulas are in normal form. This approach is essentially unavailable to any grammar framework in which the semantics of a given expression results from the unification of partially specified semantic representations because normal forms can only be defined on languages with fully instantiated formulae.</Paragraph>
<Paragraph position="7"> A third possibility consists in generating from an alternative representation i.e. one that is related to but not identical with the semantic representation used by the grammar. This is what we chose to do. The alternative representation we adopted is closely related to D-structure in cn theory where D-structure is a level of syntactic structure which mirrors semantic functor-argument dependencies. Syntactic information is encoded in terms of schematic X theory familiar from modern generative grammar. The deep structures (DS) we generate from consist of four types: heads, complements, modifiers and specifiers (we follow LEG f-structure and ucc subcategorisation structure in treating subjects as ordinary complements rather than specifiers of clauses) whereby Specifiers are of the form: specifier(Semantics, Head).</Paragraph>
<Paragraph position="8"> That is, they specify their own semantics and the properties of their head. In contrast, Heads are of the form: head(Semantics, ArgList, AdjunctList). That is, they specify their own head semantics and a list of arguments and adjuncts which are also either specifier or head structures. All of these structures also allow the encoding of syntactic requirements on arguments and adjuncts. null The use of DSs has two other consequences. First, by allowing for the association of syntactic with semantic information, D-structures offer a way to mediate the results of linguistic decisions made by an eventual planner to the generator. This may be useful. For instance, NP planning could be accounted for. In the present context, a planner is any system which given some information about what to say will return some decision about how to say it. For instance, if we want to expre,~s the fact that Jon runs, the planner will have to decide on how to refer to Jon, i.e. it could decide to describe him using a complex NP as in 'the man with the red scarf who stands negt to Irene', or a pronoun e.g. 'he' or simply his name i.e. '.Ion'. The point is that the syntactic decision made by the planner must be communicated to the generator. Since DSs contain syntactic information, they are a good candidate for the necessary interface between planner and generator.</Paragraph>
<Paragraph position="9"> A second advantage of DSs is that because they are language independent, they allow for language independent generation. That is, for any acceptable input deep structure, the algorithm presented below will generate e.g., a French sentence if coupled with a UCG grammar for French and an English sentence if coupled with a coo grammar for English. This is only possible because the input deep structure the generation algorithm relies on is both sufficiently abstract to be language-independent and general enough that it can be mapped onto language dependent surface syntactic structures. Language independent generation is discussed in more detail in section  In relation with the problem raised by non-canonical input, an important property of DSs is that they contain no indication of either surface syntactic order of the complements and adjuncts or of the relative scope of quantifiers occurring in either complements or modifiers. Instead, thematic dependencies between subformulae are kept track of by the X schema where no reference is made to derivational history. The generator is thus free to realize both scope and surface syntactic structure in any way which is consistent with the deep structure specification and the particular grammar used. The reader might object to this elimination of scope distinctions. However, within UOG any scope distinctions which are produced by the individual grammars or as a result of some semantics construction process are in fact artefactual. Furthermore, it might reasonably be argued that it should be possible to generate all possible scopes. This is typically done with quantifier shifting rules. Our solution is simply not to specify scope.</Paragraph>
<Paragraph position="10"> An immediate consequence of using DSs is that non-canonical input is no longer a problem. The reason for this simply is that the generation algorithm no longer relies on the assumption that the input semantic representation i8 canonical i.e. derivable under the grammar used. Rather, the assumption is that the input will be some well-formed DS that will contain all the information contained in the corresponding semantics but none of the information embodied in the linear ordering of the formula about the derivational history of the corresponding string.</Paragraph>
</Section>
</Paper>

