<?xml version="1.0" standalone="yes"?>

<Paper uid="W90-0107">
<Title>Using Bidirectional Semantic Rules for Generation</Title>
<Section position="1" start_page="0" end_page="48" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> This paper describes the use of a system of semantic rules to generate noun compounds, vague or polysemous words, and cases of metonymy. The rules are bi-directional and are used by the understanding system to interpret the same constructions.</Paragraph>
<Paragraph position="1"> Introduction In generation systems that are paired with understanding systems, bidirectionality is desirable for reasons that are both theoretical (a single model of linguistic behaviour) and practical (shorter development time, greater consistency, etc.) 1. Recently, \[Shieber et al. 89\] and \[Calder at al. 89\] have presented generation algorithms that share both semantics and syntax with the understanding system. This paper presents an extension of these algorithms to deal with phenomena that have often been lumped together under 'pragmatics', namely noun compounding, metonymy (the use of a word to refer to a related concept), and vague or polysemous words like &amp;quot;have.&amp;quot; The difficulty with these constructions is that they are productive, and cannot be handled easily by simply listing meanings in a lexicon. Taking noun compounding as an example, we have &amp;quot;corn oil&amp;quot; and &amp;quot;olive oil&amp;quot; referring to oil made from corn or olives. We could add a lexical sense for &amp;quot;corn&amp;quot; meaning &amp;quot;made from corn,&amp;quot; but then we face an explosion in the size of the lexicon, and an inability to understand or generate novel compounds: if we acquire &amp;quot;safflower&amp;quot; as the name of a plant, we would like the system to be able to handle &amp;quot;safflower oil&amp;quot; immediately, but this won't be possible if we need a separate lexical sense to handle compounding. The system will be more robust (and the lexicon more compact) if we can derive the desired sense of &amp;quot;safflower&amp;quot; from the basic noun sense when we need it. We have therefore developed a system of bidirectional semantic rules to handle these phenomena at the appropriate level of generality.</Paragraph>
<Paragraph position="2"> IF or more detailed arguments along these lines, see \[Appelt 87\], \[Shieber 88\], \[Jacobs 88a\].</Paragraph>
<Paragraph position="3"> We have implemented these rules in Common Lisp as part of the KBNL system \[Barnett et M. 90\] at MCC, but nothing depends on the idiosyncracies of our formalisms or implementation, so the technique is compatible with a wide variety of theories of the kinds of relations that are likely to occur in these constructions, as in, e.g., \[Finin 80\] for noun compounds and \[Nunberg 78\] for oblique reference.</Paragraph>
<Paragraph position="4"> The Framework The algorithms for recognition and generation use an agenda-based blackboard for communication and control \[Cohen et al. 89\]. Our syntax component uses an extension of Categorial Unification Grammar \[Wittenburg 86\] as the phrase-structure component of an LFG-style functional representation (f-struCture), and the semantic component maps from this representation to sets of assertions in the interface language of the CYC knowledge base \[Lenat et al. 90\].</Paragraph>
<Paragraph position="5"> Semantic rules map partial semantic interpretations onto other partial interpretations. They consist of a left-hand side and a right-hand side, each consisting of one or more templates, plus a mechanism for mapping an instantiation of either set of templates onto an instantiation of the other set. The intuitive semantics of these rules is that any interpretation that matches the left-hand side licenses a second interpretation matching the right-hand side. For example, we can use the name of an author to refer to his works (&amp;quot;I read Shakespeare&amp;quot;), and the corresponding semantic rule states that the existence of an NP denoting an artist licences the use of the same NP to refer to his works. The generation system applies the rules in a backward-chaining direction, while the understanding system runs them forward. A later section contains a fuller discussion of the implementation of the rules, while the next sections discuss their use at runtime.</Paragraph>
<Paragraph position="6"> Generation The generator is divided into strategic and tactical components. The former takes a frame as input and creates a description of it based on a set of discriminative  properties which are recorded in the KB and indicate which aspects of a frame are likely to be salient. If a comparison class is available, the resulting description uniquely identifies the frame with respect to that class, otherwise it contains default 'interesting' properties. Once it has generated this set of assertions, the strategic component calls the tactical component with a goal Semantics : Syntax, where Semantics consists of the assertions plus the distinguished variable that the utterance is 'about', and Syntax is an f-structure (which may specify no more than the category.) Given this input, the tactical component uses a variant of the semantic-head driven algorithms described by \[Calder at al. 89\] and \[Shieber et al. 89\] to generate a phrase whose syntax and semantics match the goal.</Paragraph>
<Paragraph position="7"> Before examining this algorithm, we note that in categorial grammars, most of the syntactic information is contained in the lexical definitions of words. For example, the lexical entry for a transitive verb like &amp;quot;read&amp;quot; specifies that it takes an object NP to its right and then a subject NP to its left. Any such constituent that takes at least one argument is called a funclor, while a constituent with no arguments is called atomic. Functors and their arguments are combined by a small number of binary rules, and there is also a set of unary rules, which can change the category of a constituent (forming passive verbs out of actives, for example.) Next we define two relationships between constituents and goals: first, a constituent matches a goal if its semantics subsumes the goal's semantics and its syntactic category is the same as the goal's, with possible extra arguments. Thus the transitive verb &amp;quot;eat&amp;quot;, with category S\NP/NP, is a syntactic match for the goal category S because it will he an S once it gets its arguments. Second, a constituent satisfies a goal if it has identical semantics and its f-structure is a supergraph of the goal's f-structure.</Paragraph>
<Paragraph position="8"> Given this syntactic framework, the algorithm works by peeling off lexical functors and recursing on their arguments until it bottoms out in an atomic constituent.</Paragraph>
<Paragraph position="9"> Given a goal, the first step consists of lexical look-up to find an item that matches the goal. Once this item, called the semantic head, is found, the algorithm proceeds both top-down and bottom up. If the semantic head is a functor, it proceeds top-down trying to solve the sub-goal for its argument. Once this sub-goal is satisfied, the algorithm works bottom-up by applying unary grammar rules to to the argument constituent alone, or binary rules to combine it with the functor.</Paragraph>
<Paragraph position="10"> When a complete constituent is found which satisfies the goal, we are done.</Paragraph>
<Paragraph position="11"> Extension: Goal Revision The algorithm described above assumes a fixed set of choices in the lexicon. It can generate metonymic expressions and noun compounds, but only at the cost of massive lexical ambiguity. We therefore extend it by considering the possibility of goal revision as an alternative to the lexical look-up step s. By running a semantic rule backward, we can map the current goal onto one or more new goals to which the algorithm recursively applies. Satisfying the new goals will generate an expression with the desired meaning and thus indirectly satisfy the original goal.</Paragraph>
<Paragraph position="12"> Revision using noun compounding rules leads to a binary decomposition of the original goal, as shown in Figure 2, while metonymy rules result in a unary decomposition, as shown in Figure 3. From this perspective, we note that the lexical look-up of a functor can be viewed as a kind of guided binary decomposition (Figure 1), splitting the original goal into two sub-goals with the knowledge that one of them will be satisfied immediately. [</Paragraph>
<Paragraph position="14"> Our extension to the algorithms of \[Calder at al. 89\] and \[Shieber et al. 89\] thus amounts to the decision to allow top-down decomposition to be guided by rules as well as lexical items. As we would expect, this is the mirror image of the situation during understanding, where semantic rules are used as an extension to the lexicon in the process of merging translations bottomup. The extended algorithm is shown in the Appendix.</Paragraph>
<Paragraph position="15"> Controlling Rule Application The strategic component can control the choice among alternatives through its specification of the goal's syntax. For example, the strategic component can force the use of a compound by providing an appropriately detailed f-structure (i.e., one that specifies the presence of a modifier of category N.) If it does so, no matter whether we are in best-first or all-paths mode, only the compounding alternative will succeed and satisfy the syntactic goal. On the other hand, if the syntactic goal is underspecified, the output (in best-first mode) will aThe notion of goal revision in generation dates back to \[Appelt 83\] where various conditions could lead to replanning of the input; for recent work incorporating goal revision see \[Vaughan et al. 86\].</Paragraph>
<Paragraph position="16">  depend on the tactical component's heuristic ordering.</Paragraph>
<Paragraph position="17"> In this case, given the default ordering which prefers lexical look-up to noun compounding to metonymy, the tactical component will use a noun compound when lexical look-up fails (i.e., there is no corresponding adjective or preposition). Another result of this default ordering is that metonymy will never fire in the absence of a syntactic specification since there is always another way (unless the lexicon is incomplete) of saying the same thing using words that are in the lexicon. However, the literal alternative is usually more verbose than the metonymous expression, ,so the strategic component can force the use of metonymy by specifying a limit on the number of words the tactical component is allowed to use. Given a limit of 3 words, descriptive phrases like &amp;quot;a book by Joyce&amp;quot; will fail, and only the metonymous expression &amp;quot;Joyce&amp;quot; will succeed.</Paragraph>
<Paragraph position="18"> In best-first mode, substantial improvements in efficiency are possible by re-ordering the alternatives based on the syntactic properties of the goal. For example, it makes sense to try metonymy first if the desired length is significantly less than the number of assertions in the goal's semantics, since each lexical item normally covers only a few assertions.</Paragraph>
<Section position="1" start_page="48" end_page="48" type="sub_section">
<SectionTitle>
Generating Noun Compounds
</SectionTitle>
<Paragraph position="0"> Suppose we have a Software-Machine rule, stating that if &amp;quot;y&amp;quot; denotes any kind of Software and &amp;quot;x&amp;quot; a computer, &amp;quot;a y x&amp;quot; means a Computer x that CanRunLanguage y. Now consider a goal with semantics (W</Paragraph>
</Section>
</Section>
</Paper>

