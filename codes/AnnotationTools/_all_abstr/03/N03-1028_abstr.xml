<?xml version="1.0" standalone="yes"?>

<Paper uid="N03-1028">
<Title>Shallow Parsing with Conditional Random Fields</Title>
<Section position="1" start_page="0" end_page="0" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> Conditional random elds for sequence labeling offer advantages over both generative models like HMMs and classi ers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random eld to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that con rm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.</Paragraph>
</Section>
</Paper>

