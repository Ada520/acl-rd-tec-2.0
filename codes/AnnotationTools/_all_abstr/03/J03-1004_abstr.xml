<?xml version="1.0" standalone="yes"?>

<Paper uid="J03-1004">
<Title>Bies, Ann, Mark Ferguson, Karen Katz, and</Title>
<Section position="2" start_page="0" end_page="77" type="abstr">
<SectionTitle>
1. Overview
</SectionTitle>
<Paragraph position="0"> This article addresses the issue of determining the most accessible quantifier scope reading for a sentence. Quantifiers are elements of natural and logical languages (such as each, no, and some in English and [?] and [?] in predicate calculus) that have certain semantic properties. Loosely speaking, they express that a proposition holds for some proportion of a set of individuals. One peculiarity of these expressions is that there can be semantic differences that depend on the order in which the quantifiers are interpreted. These are known as scope differences.</Paragraph>
<Paragraph position="1"> (1) Everyone likes two songs on this album.</Paragraph>
<Paragraph position="2"> As an example of the sort of interpretive differences we are talking about, consider the sentence in (1). There are two readings of this sentence; which reading is meant depends on which of the two quantified expressions everyone and two songs on this album takes wide scope. The first reading, in which everyone takes wide scope, simply implies that every person has a certain preference, not necessarily related to anyone else's. This reading can be paraphrased as &amp;quot;Pick any person, and that person will like two songs on this album.&amp;quot; The second reading, in which everyone takes narrow scope, implies that there are two specific songs on the album of which everyone is fond, say, &amp;quot;Blue Moon&amp;quot; and &amp;quot;My Way.&amp;quot; In theoretical linguistics, attention has been primarily focused on the issue of scope generation. Researchers applying the techniques of quantifier raising and Cooper storage have been concerned mainly with enumerating all of the scope readings for a [?] Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail: dchiggin@alumni.uchicago.edu.</Paragraph>
<Paragraph position="3"> + Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail: j-sadock@uchicago.edu.</Paragraph>
<Paragraph position="4">  Computational Linguistics Volume 29, Number 1 sentence that are possible, without regard to their relative likelihood or naturalness. Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn their attention to scope prediction, or determining the relative accessibility of different scope readings.</Paragraph>
<Paragraph position="5"> In computational linguistics, more attention has been paid to the factors that determine scope preferences. Systems such as the SRI Core Language Engine (Moran 1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt, and Pereira 1986) have employed scope critics that use heuristics to decide between alternative scopings. However, the rules that these systems use in making quantifier scope decisions are motivated only by the researchers' intuitions, and no empirical results have been published regarding their accuracy.</Paragraph>
<Paragraph position="6"> In this article, we use the tools of machine learning to construct a data-driven model of quantifier scope preferences. For theoretical linguistics, this model serves as an illustration that Kuno, Takami, and Wu's approach can capture some of the clearest generalizations about quantifier scoping. For computational linguistics, this article provides a baseline result on the task of scope prediction, with which other scope critics can be compared. In addition, it is the most extensive empirical investigation of which we are aware that collects data of any kind regarding the relative frequency of different quantifier scope readings in English text.</Paragraph>
<Paragraph position="7">  Section 2 briefly discusses treatments of scoping issues in theoretical linguistics, and Section 3 reviews the computational work that has been done on natural language quantifier scope. In Section 4 we introduce the models that we use to predict quantifier scoping, as well as the data on which they are trained and tested. Section 5 combines the scope model of the previous section with a probabilistic context-free grammar (PCFG) model of syntax and addresses the issue of whether these two modules of grammar ought to be combined in serial, with information from the syntax feeding the quantifier scope module, or in parallel, with each module constraining the structures provided by the other.</Paragraph>
<Paragraph position="8"> 2. Approaches to Quantifier Scope in Theoretical Linguistics Most, if not all, linguistic treatments of quantifier scope have closely integrated it with the way in which the syntactic structure of a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998).</Paragraph>
<Paragraph position="9"> The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings.</Paragraph>
<Paragraph position="10">  Higgins and Sadock Modeling Scope Preferences Figure 1 Simple illustration of the QR approach to quantifier scope generation.</Paragraph>
<Paragraph position="11"> follows from this claim differ in the precise characterization of this QR transformation, what conditions are placed upon it, and what tree-configurational relationship is required for one operator to take scope over another. The general idea of QR is represented in Figure 1, a schematic analysis of the reading of the sentence Someone saw everyone in which someone takes wide scope (i.e., 'there is some person x such that for all persons y, x saw y').</Paragraph>
<Paragraph position="12"> In the Cooper storage approach, quantifiers are gathered into a store and passed upward through a syntactic tree. At certain nodes along the way, quantifiers may be retrieved from the store and take scope. The relative scope of quantifiers is determined by where each quantifier is retrieved from the store, with quantifiers higher in the tree taking wide scope over lower ones. As with QR, different authors implement this scheme in slightly different ways, but the simplest case is represented in Figure 2, the Cooper storage analog of Figure 1.</Paragraph>
<Paragraph position="13"> These structural approaches, QR and Cooper storage, have in common that they allow syntactic factors to have an effect only on the scope readings that are available for a given sentence. They are also similar in addressing only the issue of scope generation, or identifying all and only the accessible readings for each sentence. That is to say, they do not address the issue of the relative salience of these readings.</Paragraph>
<Paragraph position="14"> Kuno, Takami, and Wu (1999, 2001) propose to model the scope of quantified elements with a set of interacting expert systems that basically consists of a weighted vote taken of the various factors that may influence scope readings. This model is meant to account not only for scope generation, but also for &amp;quot;the relative strengths of the potential scope interpretations of a given sentence&amp;quot; (1999, page 63). They illustrate the plausibility of this approach in their paper by presenting a number of examples that are accounted for fairly well by the approach even when an unweighted vote of the factors is allowed to be taken.</Paragraph>
<Paragraph position="15"> So, for example, in Kuno, Takami and Wu's (49b) (1999), repeated here as (2), the correct prediction is made: that the sentence is unambiguous with the first quantified noun phrase (NP) taking wide scope over the second (the reading in which we don't all have to hate the same people). Table 1 illustrates how the votes of each of Kuno, Takami, and Wu's &amp;quot;experts&amp;quot; contribute to this outcome. Since the expression many of us/you receives more votes, and the numbers for the two competing quantified expressions are quite far apart, the first one is predicted to take wide scope unambiguously. (2) Many of us/you hate some of them.</Paragraph>
<Paragraph position="16">  Computational Linguistics Volume 29, Number 1 Figure 2 Simple illustration of the Cooper storage approach to quantifier scope generation. Table 1 Voting to determine optimal scope readings for quantifiers, according to Kuno, Takami, and Wu (1999).</Paragraph>
<Paragraph position="17"> many of us/you some of them  Some adherents of the structural approaches also seem to acknowledge the necessity of eventually coming to terms with the factors that play a role in determining scope preferences in language. Aoun and Li (2000) claim that the lexical scope preferences of quantifiers &amp;quot;are not ruled out under a structural account&amp;quot; (page 140). It is clear from the surrounding discussion, though, that they intend such lexical requirements to be taken care of in some nonsyntactic component of grammar. Although Kuno, Takami, and Wu's dialogue with Aoun and Li in Language has been portrayed by both sides as a debate over the correct way of modeling quantifier scope, they are not really modeling the same things. Whereas Aoun and Li (1993) provide an account of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope generation and scope prediction. The model of scope preferences provided in this article is an empirically based refinement of the approach taken by Kuno, Takami, and Wu, but in principle it is consistent with a structural account of scope generation.  Higgins and Sadock Modeling Scope Preferences 3. Approaches to Quantifier Scope in Computational Linguistics Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of scope generation from a computational perspective. Attempts have also been made in computational work to extend a pure Cooper storage approach to handle scope prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort of ordering heuristics into the SRI scope generation system, in the hopes of producing a ranked list of possible scope readings, but ultimately are forced to acknowledge that &amp;quot;[t]he modifications turn out to be quite complicated if we wish to order quantifiers according to lexical heuristics, such as having each out-scope some. Because of the recursive nature of the algorithm, there are limits to the amount of ordering that can be done in this manner&amp;quot; (page 55). The stepwise nature of these scope mechanisms makes it hard to state the factors that influence the preference for one quantifier to take scope over another.</Paragraph>
<Paragraph position="18"> Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate system of heuristics (or scope critics) that apply postsyntactically to determine the most likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992) all employ scope rules of this sort. By and large, these rules are of an ad hoc nature, implementing a linguist's intuitive idea of what factors determine scope possibilities, and no results have been published regarding the accuracy of these methods. For example, Moran (1988) incorporates rules from other NLP systems and from VanLehn (1978), such as a preference for a logically weaker interpretation, the tendency for each to take wide scope, and a ban on raising a quantifier across multiple major clause boundaries. The testing of Moran's system is &amp;quot;limited to checking conformance to the stated rules&amp;quot; (pages 40-41). In addition, these systems are generally incapable of handling unrestricted text such as that found in the Wall Street Journal corpus in a robust way, because they need to do a full semantic analysis of a sentence in order to make scope predictions. The statistical basis of the model presented in this article offers increased robustness and the possibility of more serious evaluation on the basis of corpus data.</Paragraph>
</Section>
</Paper>

