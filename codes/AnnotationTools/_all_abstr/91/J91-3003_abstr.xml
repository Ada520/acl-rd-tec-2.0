<?xml version="1.0" standalone="yes"?>

<Paper uid="J91-3003">
<Title>The Generative Power of Categorial Grammars and Head-Driven Phrase Structure Grammars with Lexical Rules</Title>
<Section position="2" start_page="0" end_page="303" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> In recent grammatical theories, there has been an increasing trend toward the lexicalization of syntactic information. This is particularly evident in the case of categorial grammars (CG) and head-driven phrase structure grammars (HPSG), where a small number of highly schematic syntactic rules are assumed to apply universally. With this assumption of a universal syntax, the task of explaining the variations between languages must be carried out in the lexicon. Rather than assuming that the lexicon is simply an unstructured list associating words with syntactic categories, organization is usually imposed by means of hierarchical inheritance systems, linking theories relating lexical semantics to grammatical categories and finally with lexical rules. It is the lexical rule component of these grammars that we investigate in this paper.</Paragraph>
<Paragraph position="1"> We present a straightforward formalization of categorial grammars with lexical rules based in part on the systems of Dowty (1978, 1979), Bach (1984), Keenan and Faltz (1985), Keenan and Timberlake (1988), Hoeksema and Janda (1988), and the HPSG lexical rule systems of Flickinger et al. (1985), Flickinger (1987), and Pollard and Sag (1987). We show that even using such a simple form of lexical rules, any recursively enumerable language can be recognized. Thus the addition of lexical rules leads to systems in which it is not possible to effectively decide whether a string is accepted by a grammar. We first introduce a pared-down formalism that captures the way in which heads combine with complements in CG and HPSG. We then provide examples to motivate a very straightforward and natural system of lexical rules.</Paragraph>
<Paragraph position="2"> To show that arbitrary recursively enumerable languages may be generated by the resulting system, we provide a reduction of generalized rewriting systems (Type 0 grammars). Though the details of our reduction are different, our method is reminiscent of that used by Uszkoreit and Peters (1986) to show that context-free grammars with metarules could generate arbitrary recursively enumerable languages. The analogy between CG lexical rules and GPSG metarules is strengthened by the fact that  GPSG as presented in Gazdar et al. (1985) restricts the application of metarules to lexical phrase structure rules.</Paragraph>
<Paragraph position="3"> Along the way to proving that categorial grammars with lexical rules can generate arbitrary recursively enumerable languages, we consider two restrictions that have the effect of reducing the generative power of the system to context-free languages.</Paragraph>
<Paragraph position="4"> The first of these restrictions limits the recursive application of lexical rules, while the second puts a bound on the number of complements that can be specified by a category.</Paragraph>
<Paragraph position="5"> 2. CGs and HPSGs In Generalized Phrase Structure Grammar (GPSG) as presented in Gazdar et al. (1985), each lexical head category is assigned a simple integer subcategorization value. Similarly, each lexical phrase structure rule specifies the possible value(s) of the subcategorization feature occurring on its head daughter. In both CG and HPSG, the subcategorization value and correspondingly indexed phrase structure rule are replaced with a lexical encoding of head/complement structure by means of a subcategorization or complement list. The exact characterization of the notion of head has been the subject of some debate, but which items are labeled as heads is not important here; we use the term head to refer to any category that specifies its complements. Thus heads in our sense may be categories traditionally classified as specifiers or adjuncts. The subcategorization list of a category determines the number, form, and order of its complements. The only syntactic rule scheme that we consider is one that allows a head to combine with a complement. The result of such a construction is a category just like the head, only with the complement removed from its subcategorization list. Many extended categorial grammar systems and the HPSG system allow more sophisticated rules than this to deal with adjuncts, coordination, and long-distance dependencies, but we only need the simple head-complement construction to show that the addition of lexical rules leads to undecidability.</Paragraph>
<Paragraph position="6"> We now turn to a more formal presentation of a framework that incorporates the core of both CG and HPSG. We begin with a finite set BasCat of basic categories out of which complex categories are constructed. The collection of basic categories is usually assumed to contain &amp;quot;saturated&amp;quot; syntactic categories, including nouns, noun phrases, sentences, and so forth, but the choice of basic categories is ultimately up to the grammar writer. We are not concerned with the details of any particular syntactic analysis here. The full set Cat of categories is defined to be the least collection of objects of the form b\[co,..., cn-1\] where b E BasCat is a basic category and ci E Cat for i &lt; n. The categories between the brackets specify the arguments of the category, with the assumption being that these arguments must be attached in the order in which they occur on the subcategorization list. While our notation follows the usage of HPSG and Unification Categorial Grammar (Calder et al. 1988), it should be clear how it relates to more traditional categorial grammar notation (see Bar-Hillel et al. 1960).</Paragraph>
<Paragraph position="7"> We assume the following schematic phrase structure rule that allows heads to combine with a single complement: Definition 1 b\[Cl,..., c,\] ~ b\[co,..., On\] CO In most categorial grammars and in HPSG, there are rules in which the complement category precedes the head, but we do not even need this much power. For instance, a simple transitive verb might be given the category s\[np\[\];np\[\]\], a noun phrase the  Carpenter CG and HI~G with Lexical Rules category np\[\], a noun n\[\], and a determiner the category np\[n\[\]\]. Thus the rule instance s\[np\[\]\] &gt; s\[np\[\], np\[\]\] np\[\] allows the combination of a transitive verb with an object noun phrase, while the rule np\[\] ~ np\[n\[\]\] n\[\] would allow a determiner followed by a noun to be analyzed as a noun phrase.</Paragraph>
<Paragraph position="8"> Let Rule be the set of all instances of the schematic rule applied to Cat. Note that this set is totally determined by the choice BasCat of basic categories.</Paragraph>
<Paragraph position="9"> A lexicon for our simple grammar formalism consists of a finite relation between categories and basic expressions: Definition 2 Lex C BasExp x Cat.</Paragraph>
<Paragraph position="10"> We write e := c if (e, c&gt; E Lex. There is no restriction preventing a single expression from having more than one lexical entry. What we have with Lex and Rule is a simple phrase structure grammar, albeit one with an infinite set of rules. We interpret admissibility (well-formedness or grammaticality) in this phrase structure grammar in the usual way (see Hopcroft and Ullman 1979:79-87). In particular, we take PSLex(C) to be the set of strings of basic expressions of category c. By mutual recursion over Cat we define the PSLex(C) as the minimal sets such that:</Paragraph>
<Paragraph position="12"> It should be fairly obvious at this point how our simple categorial grammar formalism represents the core of both categorial grammars and the subcategorization component of HPSG. It should also be noted that such a grammar and lexical assignment reduces to a context-free grammar. This is because only finitely many sub-categories of the lexical categories may ever be used in a derivation and thus only finitely many instances of the application scheme are necessary for a finite categorial lexicon. Somewhat surprisingly, the converse result also holds (Bar-Hillel et al. 1960); every context-free language is generated by some categorial grammar in the form we have presented. This latter result can be deduced from the fact that every context-free language can be expressed with a Greibach normal form grammar where every production is of the form Co ~ aC1 ... Cn where n &gt; 0, the C i are nonterminal category symbols, and a is a terminal expression (see Hopcroft and Ullman 1979). Taking a basic category for every nonterminal of the context-free grammar and a lexical entry of the form a := Co \[C1 \[\],..., C, \[\]\] for every production of the above form in the context-free grammar, we produce a categorial grammar that can be shown by induction on derivation length to generate exactly the same language as the Greibach normal form context-free grammar.</Paragraph>
<Paragraph position="13"> Not only is recognition decidable for context-free languages, but Earley's (1970) algorithm is known to decide them in O(n 3) time where n is the length of the input string (in fact, general CFG parsing algorithms can be constructed from matrix multiplication algorithms with slightly better worst-case asymptotic performance than Earley's algorithm \[Valiant 1975\]). Unfortunatel~ the situation is quite different after the addition of lexical rules.</Paragraph>
<Paragraph position="14"> Before going on, it should be noted that one of the primary reasons for employing categorial grammars is its natural relation to a compositional semantics (Montague 1970). While we do not consider the semantic effects of lexical rules here, the interested  Computational Linguistics Volume 17, Number 3 reader is urged to consult Carpenter (1991) for details of how a higher-order typed semantics can be naturally added to the system of lexical rules presented below.</Paragraph>
</Section>
</Paper>

