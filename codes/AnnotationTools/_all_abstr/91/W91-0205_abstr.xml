<?xml version="1.0" standalone="yes"?>

<Paper uid="W91-0205">
<Title>TYPE EB SUBCLASS SEB TEMPLATE (SCRIPT $DEMONSTRATE ACTOR NIL OBJECT NIL DEMANDS NIL METHOD (SCENE $OCCUPY ACTOR NIL LOCATION NIL)) FILL (((ACTOR) (TOP-OF *ACTOR-STACK*)) ((METHOD ACTOR) (TOP-OF *ACTOR-STACK*))) REQS (FIND-DEMON-OBJECT FIND-OCCUPY-LOC RECOGNIZE-DEMANDS) </Title>
<Section position="1" start_page="0" end_page="38" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> The lexical entry for a word must contain all the information needed to construct a semantic representation for sentences that contain the word. Because of that requirement, the formats for lexical representations must be as detailed as the semantic forms. Simple representations, such as features and frames, are adequate for resolving many syntactic ambiguities. But since those notations cannot represent all of logic, they are incapable of supporting all the function needed for semantics. Richer semantic-based approaches have been developed in both the model-theoretic tradition and the more computational AI tradition. Although superficially in conflict, these two traditions have a great deal in common at a deeper level. Both of them have developed semantic structures that are capable of representing a wide range of linguistic phenomena. This paper compares these approaches and evaluates their adequacy for various kinds of semantic information that must be stored in the lexicon. It presents conceptual graphs as a synthesis of the logicist and AI representations designed to support the requirements of both.</Paragraph>
<Paragraph position="1"> 1 Semantics from the Point of View of the Lexicon To understand a semantic theory, start by looking at what goes into the lexicon. In one of the early semantic theories in the Chomskyan tradition, Katz and Fodor (1963) did in fact start with the lexicon. More recent theories, however, almost treat the lexicon as an afterthought. Yet the essence of the theory is still in the lexicon: every element of the semantic representation of a sentence ultimately derives from something in the lexicon.</Paragraph>
<Paragraph position="2"> That principle is just as true for Richard Montague's highly formalized grammar as for Roger Schank's &amp;quot;scruffy&amp;quot; conceptual dependencies, scripts, and MOPs.</Paragraph>
<Paragraph position="3"> Context and background knowledge are also important, since most sentences cannot be understood in isolation. This fact contradicts Frege's principle of compositionality, which says that the meaning of a sentence is derived from the meanings of the words it contains. Yet context can also be stated in words and sentences. Even when nonlinguistic surroundings are necessary for understanding a sentence, every significant feature could be encoded in a sentence; people normally encode such information when they tell a story.</Paragraph>
<Paragraph position="4"> More general encyclopedic knowledge is contextual information that was learned at an earlier time; it too could be stated in sentences. An extended Fregean principle should therefore say that the meaning of a sentence must be derivable from the meanings of the words in the sentence together with the meanings of the words in the sentences that describe the relevant context and background knowledge.</Paragraph>
<Paragraph position="5">  Besides the meanings of words, grammar and logic are necessary to combine those meanings into a complete semantic representation. But there are competing theories about how much grammar and logic is necessary, how much is expressed in the lexicon, and how much is expressed in the linguistic system outside the lexicon. Lexically based theories suggest that the grammar rules should be simple and that most of the syntactic complexity should be encoded in the lexicon. One might even go further and say that most of the syntactic complexity isn't syntactic at M1. It is the result of interactions among the logical structures of the underlying concepts. In his work on semanticMly based syntax, Dixon (1991) maintained that syntactic irregularities and idiosyncrasies are not accidental. Instead, he showed that many of them can be predicted from the semantics of the words. Such theories imply that a system would only need a simple grammar to represent a language if it had sufficiently rich semantic structures.</Paragraph>
<Paragraph position="6"> A rich theory of semantics in the lexicon must also explain how the semantics got into the lexicon. A child could learn an initial stock of meanings by associating prelinguistic structures with words. But even those prelinguistic structures are shaped, polished, and refmed by long usage in the context of sentences. They are combined with the structures learned from other words, and they are molded into patterns that are traditional in the language and culture. More complex, abstract, and sophisticated concepts are either learned exclusively through language or through experiences that are highly colored and shaped by language. For these reasons, the meaning representations in the lexicon should be derivable from the semantic representations for sentences. As a working hypothesis, the two should be identical: the same knowledge representation language should be used for representing meanings in the lexicon and for representing the semantics of sentences and extended discourse structures.</Paragraph>
<Paragraph position="7"> This paper explores the implications of that hypothesis. Section 2 reviews several different lexical representations and their implications. Section 3 compares the underlying assumptions of the model-theoretic and AI traditions and shows that they are computationaUy more compatible than their metaphysics would suggest. Section 4 illustrates the use of conceptual graphs for representing the semantic content of lexical entries. Section 5 shows how such representations can be used to handle aspects of language that require both logic and background knowledge.</Paragraph>
</Section>
</Paper>

