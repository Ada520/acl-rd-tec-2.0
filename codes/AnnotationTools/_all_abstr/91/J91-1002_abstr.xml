<?xml version="1.0" standalone="yes"?>

<Paper uid="J91-1002">
<Title>Word Sentence Lexical Chain</Title>
<Section position="2" start_page="0" end_page="38" type="abstr">
<SectionTitle>
1. Lexical Cohesion
</SectionTitle>
<Paragraph position="0"> A text or discourse is not just a set of sentences, each on some random topic. Rather, the sentences and phrases of any sensible text will each tend to be about the same things -- that is, the text will have a quality of unity. This is the property of cohesion -- the sentences &amp;quot;stick together&amp;quot; to function as a whole. Cohesion is achieved through back-reference, conjunction, and semantic word relations. Cohesion is not a guarantee of unity in text but rather a device for creating it. As aptly stated by Halliday and Hasan (1976), it is a way of getting text to &amp;quot;hang together as a whole.&amp;quot; Their work on cohesion has underscored its importance as an indicator of text unity.</Paragraph>
<Paragraph position="1"> Lexical cohesion is the cohesion that arises from semantic relationships between words. All that is required is that there be some recognizable relation between the words.</Paragraph>
<Paragraph position="2"> Halliday and Hasan have provided a classification of lexical cohesion based on the type of dependency relationship that exists between words. There are five basic classes:  1. Reiteration with identity of reference: Example 1 1. Mary bit into a peach.</Paragraph>
<Paragraph position="3"> 2. Unfortunately the peach wasn't ripe.</Paragraph>
<Paragraph position="4"> * Department of Computer Science, York University, North York, Ontario, Canada M3J 1P3 t Department of Computer Science, University of Toronto, Toronto, Ontario, Canada M5S 1A4 (~) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 1 2. Reiteration without identity of reference: Example 2 1. Mary ate some peaches.</Paragraph>
<Paragraph position="5"> 2. She likes peaches very much.</Paragraph>
<Paragraph position="6"> 3. Reiteration by means of superordinate: Example 3 1. Mary ate a peach.</Paragraph>
<Paragraph position="7"> 2. She likes fruit.</Paragraph>
<Paragraph position="8"> 4. Systematic semantic relation (systematically classifiable): Example 4 1. Mary likes green apples.</Paragraph>
<Paragraph position="9"> 2. She does not like red ones.</Paragraph>
<Paragraph position="10"> 5. Nonsystematic semantic relation (not systematically classifiable): Example 5 1. Mary spent three hours in the garden yesterday.</Paragraph>
<Paragraph position="11"> 2. She was digging potatoes.</Paragraph>
<Paragraph position="12">  Examples 1, 2, and 3 fall into the class of reiteration. Note that reiteration includes not only identity of reference or repetition of the same word, but also the use of superordinates, subordinates, and synonyms.</Paragraph>
<Paragraph position="13"> Examples 4 and 5 fall into the class of collocation, that is, semantic relationships between words that often co-occur. They can be further divided into two categories of relationship: systematic semantic, and nonsystematic semantic. Systematic semantic relationships can be classified in a fairly straightforward way. This type of relation includes antonyms, members of an ordered set such as {one, two, three}, members of an unordered set such as {white, black, red}, and part-to-whole relationships like {eyes, mouth, face}. Example 5 is an illustration of collocation where the word relationship, {garden, digging}, is nonsystematic. This type of relationship is the most problematic, especially from a knowledge representation point of view. Such collocation relationships exist between words that tend to occur in similar lexical environments. Words tend to occur in similar lexical environments because they describe things that tend to occur in similar situations or contexts in the world. Hence, context-specific examples such as {post office, service, stamps, pay, leave} are included in the class. (This example is from Ventola (1987), who analyzed the patterns of lexical cohesion specific to the context of service encounters.) Another example of this type is {car, lights, turning}, taken from example 14 in Section 4.2. These words are related in the situation of driving a car, but taken out of that situation, they are not related in a systematic way. Also contained in the class of collocation are word associations. Examples from Postman and Keppel (1970) are {priest, church}, {citizen, U.S.A.}, and {whistle, stop}. Again, the exact relationship between these words can be hard to classify, but there does exist a recognizable relationship.</Paragraph>
<Section position="1" start_page="0" end_page="22" type="sub_section">
<SectionTitle>
1.1 Lexical Chains
</SectionTitle>
<Paragraph position="0"> Often, lexical cohesion occurs not simply between pairs of words but over a succession of a number of nearby related words spanning a topical unit of the text. These  Morris and Hirst Lexical Cohesion sequences of related words will be called lexical chains. There is a distance relation between each word in the chain, and the words co-occur within a given span. Lexical chains do not stop at sentence boundaries. They can connect a pair of adjacent words or range over an entire text.</Paragraph>
<Paragraph position="1"> Lexical chains tend to delineate portions of text that have a strong unity of meaning. Consider this example (sentences 31-33 from the long example given in Section 4.2): Example 6 In front of me lay a virgin crescent cut out of pine bush. A dozen houses were going up, in various stages of construction, surrounded by hummocks of dry earth and stands of precariously tall trees nude halfway up their trunks. They were the kind of trees you might see in the mountains.</Paragraph>
<Paragraph position="2"> A lexical chain spanning these three sentences is {virgin, pine, bush, trees, trunks, trees}. Section 3 will explain how such chains are formed. Section 4 is an analysis of the correspondence between lexical chains and the structure of the text.</Paragraph>
</Section>
<Section position="2" start_page="22" end_page="23" type="sub_section">
<SectionTitle>
1.2 Why Lexical Cohesion Is Important
</SectionTitle>
<Paragraph position="0"> There are two major reasons why lexical cohesion is important for computational text understanding systems: . Lexical chains provide an easy-to-determine context to aid in the resolution of ambiguity and in the narrowing to a specific meaning of a word.</Paragraph>
<Paragraph position="1">  2. Lexical chains provide a clue for the determination of coherence and discourse structure, and hence the larger meaning of the text. 1.2.1 Word Interpretation in Context. Word meanings do not exist in isolation. Each  word must be interpreted in its context. For example, in the context {gin, alcohol, sober, drinks}, the meaning of the noun drinks is narrowed down to alcoholic drinks. In the context {hair, curl, comb, wave} (Halliday and Hasan 1976), wave means a hair wave, not a water wave, a physics wave, or a friendly hand wave. In these examples, lexical chains can be used as a contextual aid to interpreting word meanings. In earlier work, Hirst (1987) used a system called &amp;quot;Polaroid Words&amp;quot; to provide for intrasentential lexical disambiguation. Polaroid Words relied on a variety of cues, including syntax, selectional restrictions, case frames, and -- most relevant here -a notion of semantic distance or relatedness to other words in the sentences; a sense that had such a relationship was preferred over one that didn't. Relationships were determined by marker passing along the arcs in a knowledge base. The intuition was that semantically related concepts will be physically close in the knowledge base, and can thus be found by traversing the arcs for a limited distance. But Polaroid Words looked only for possible relatedness between words in the same sentence; trying to find connections with all the words in preceding sentences was too complicated and too likely to be led astray. The idea of lexical chains, however, can address this weakness in Polaroid Words; lexical chains provide a constrained easy-to-determine representation of context for consideration of semantic distance.</Paragraph>
<Paragraph position="2">  chains is that they provide a clue for the determination of coherence and discourse structure.</Paragraph>
<Paragraph position="3">  Computational Linguistics Volume 17, Number 1 When a chunk of text forms a unit within a discourse, there is a tendency for related words to be used. It follows that if lexical chains can be determined, they will tend to indicate the structure of the text.</Paragraph>
<Paragraph position="4"> We will describe the application of lexical cohesion to the determination of the discourse structure that was proposed by Grosz and Sidner (1986). Grosz and Sidner propose a structure common to all discourse, which could be used along with a structurally dependent focus of attention to delineate and constrain referring expressions. In this theory there are three interacting components: linguistic structure, intentional structure, and attentional state.</Paragraph>
<Paragraph position="5"> Linguistic structure is the segmentation of discourse into groups of sentences, each fulfilling a distinct role in the discourse. Boundaries of segments can be fuzzy, but some factors aiding in their determination are clue words, changes in intonation (not helpful in written text), and changes in aspect and tense. When found, these segments indicate changes in the topics or ideas being discussed, and hence will have an effect on potential referents.</Paragraph>
<Paragraph position="6"> The second major component of the theory is the intentional structure. It is based on the idea that people have definite purposes for engaging in discourse. There is an overall discourse purpose, and also a discourse segment purpose for each of the segments in the linguistic structure described above. Each segment purpose specifies how the segment contributes to the overall discourse purpose. There are two structural relationships between these segments. The first is called a dominance relation, which occurs when the satisfaction (i.e., successful completion) of one segment's intention contributes to the satisfaction of another segment's intention. The second relation is called satisfaction precedence, which occurs when the satisfaction of one discourse segment purpose must occur before the satisfaction of another discourse segment purpose can occur.</Paragraph>
<Paragraph position="7"> The third component of this theory is the attentional state. This is a stack-based model of the set of things that attention is focused on at any given point in the discourse. It is &amp;quot;parasitic&amp;quot; on the intentional and linguistic structures, since for each discourse segment there exists a separate focus space. The dominance relations and satisfaction precedence relations determine the pushes and pops of this stack space. When a discourse segment purpose contributes to a discourse segment purpose of the immediately preceding discourse segment, the new focus space is pushed onto the stack. If the new discourse segment purpose contributes to a discourse segment purpose earlier in the discourse, focus spaces are popped off the stack until the discourse segment that the new one contributes to is on the top of the stack.</Paragraph>
<Paragraph position="8"> It is crucial to this theory that the linguistic segments be identified, and as stated by Grosz and Sidner, this is a problem area. This paper will show that lexical chains are a good indication of the linguistic segmentation. When a lexical chain ends, there is a tendency for a linguistic segment to end, as the lexical chains tend to indicate the topicality of segments. If a new lexical chain begins, this is an indication or clue that a new segment has begun. If an old chain is referred to again (a chain return), it is a strong indication that a previous segment is being returned to. We will demonstrate this in Section 4.</Paragraph>
</Section>
<Section position="3" start_page="23" end_page="25" type="sub_section">
<SectionTitle>
1.3 Cohesion and Coherence
</SectionTitle>
<Paragraph position="0"> The theory of coherence relations (Hobbs 1978; Hirst 1981; McKeown 1985) will now be considered in relation to cohesion. There has been some confusion as to the differences between the phenomena of cohesion and coherence, e.g., Reichman (1985). There is a danger of lumping the two together and losing the distinct contributions of each to the understanding of the unity of text.</Paragraph>
<Paragraph position="1">  Morris and Hirst Lexical Cohesion Ultimately, the difference between cohesion and coherence is this: cohesion is a term for sticking together; it means that the text all hangs together. Coherence is a term for making sense; it means that there is sense in the text. Hence the term coherence relations refers to the relations between sentences that contribute to their making sense. Cohesion and coherence relations may be distinguished in the following way. A coherence relation is a relation among clauses or sentences, such as elaboration, support, cause, or exemplification. There have been various attempts to classify all possible coherence relations, but there is as yet no widespread agreement. There does not exist a general computationally feasible mechanism for identifying coherence relations. In contrast, cohesion relations are relations among elements in a text: reference, ellipsis, substitution, conjunction, and lexical cohesion.</Paragraph>
<Paragraph position="2"> Since cohesion is well'defined, one might expect that it would be computationally easier to identify, because the identification of ellipsis, reference, substitution, conjunction, and lexical cohesion is a straightforward task for people. We will show below that lexical cohesion is computationally feasible to identify. In contrast, the identification of a specific coherence relation from a given set is not a straightforward task, even for people. Consider this example from Hobbs (1978):  Example 7 1. John can open Bill's safe.</Paragraph>
<Paragraph position="3"> 2. He knows the combination.</Paragraph>
<Paragraph position="4">  Hobbs identifies the coherence relation as elaboration. But it could just as easily be explanation. This distinction depends on context, knowledge, and beliefs. For example, if you questioned John's ability to open Bill's safe, you would probably identify the relation as explanation. Otherwise you could identify it as elaboration. Here is another example: Example 8  1. John bought a raincoat.</Paragraph>
<Paragraph position="5"> 2. He went shopping yesterday on Queen Street and it rained.</Paragraph>
<Paragraph position="6">  The coherence relation here could be elaboration (on the buying), or explanation (of when, how, or why), or cause (he bought the raincoat because it was raining out). The point is that the identity of coherence relations is &amp;quot;interpretative,&amp;quot; whereas the identity of cohesion relations is not. At a general level, even if the precise coherence relation is not known, the relation &amp;quot;is about the same thing&amp;quot; exists if coherence exists. In the example from Hobbs above, safe and combination are lexically related, which in a general sense means they &amp;quot;are about the same thing in some way.&amp;quot; In example 8, bought and shopping are lexically related, as are raincoat and rained. This shows how cohesion can be useful in identifying sentences that are coherently related. Cohesion and coherence are independent, in that cohesion can exist in sentences that are not related coherently: Example 9 Wash and core six apples. Use them to cut out the material for your new suit. They tend to add a lot to the color and texture of clothing. Actually, maybe you should use five of them instead of six, since they are quite large.</Paragraph>
<Paragraph position="7">  Computational Linguistics Volume 17, Number 1 Similarly, coherence can exist without textual cohesion: Example 10 I came home from work at 6:00 p.m. Dinner consisted of two chicken breasts and a bowl of rice.</Paragraph>
<Paragraph position="8"> Of course, most sentences that relate coherently do exhibit cohesion as well. 1</Paragraph>
</Section>
<Section position="4" start_page="25" end_page="26" type="sub_section">
<SectionTitle>
1.4 The Importance of Both Cohesion and Coherence
</SectionTitle>
<Paragraph position="0"> Halliday and Hasan (1976) give two examples of lexical cohesion involving identity  of reference: Example 11 1. Wash and core six cooking apples. 2. Put them into a fireproof dish. Example 12 1. Wash and core six cooking apples. 2. Put the apples into a fireproof dish.  Reichman (1985, p. 180) writes &amp;quot;It is not the use of a pronoun that gives cohesion to the wash-and-core-apples text. These utterances form a coherent piece of text not because the pronoun them is used but because they jointly describe a set of cooking instructions&amp;quot; (emphasis added). This is an example of lumping cohesion and coherence together as one phenomenon. Pronominal reference is defined as a type of cohesion (Halliday and Hasan 1976). Therefore the them in example 11 is an instance of it. The important point is that both cohesion and coherence are distinct phenomena creating unity in text.</Paragraph>
<Paragraph position="1"> Reichman also writes (1985, p. 1179) &amp;quot;that similar words (apples, them, apples) appear in a given stretch of discourse is an artifact of the content of discussion.&amp;quot; It follows that if content is related in a stretch of discourse, there will be coherence. Lexical cohesion is a computationally feasible clue to identifying a coherent stretch of text. In example 12, it is computationally trivial to get the word relationship between apples and apples, and this relation fits the definition of lexical cohesion. Surely this simple indicator of coherence is useful, since as stated above, there does not exist a computationally feasible method of identifying coherence in non-domain-specific text. Cohesion is a useful indicator of coherence regardless of whether it is used intentionally by writers to create coherence, or is a result of the coherence of text.</Paragraph>
<Paragraph position="2"> Hobbs (1978) sees the resolution of coreference (which is a form of cohesion) as being subsumed by the identification of coherence. He uses a formal definition of coherence relations, an extensive knowledge base of assertions and properties of objects and actions, and a mechanism that searches this knowledge source and makes simple inferences. Also, certain elements must be assumed to be coreferential. He shows how, in example (7), an assumption of coherence allows the combination to be identified as the combination of Bill's safe and John and he to be found to be coreferential.</Paragraph>
<Paragraph position="3">  Morris and Hirst Lexical Cohesion But lexical cohesion would also indicate that safe and combination can be assumed to be coreferential. And more importantly, one should not be misled by chicken-and-egg questions when dealing with cohesion and coherence. Rather, one should use each where applicable. Since the lexical cohesion between combination and safe is easy to compute, we argue that it makes sense to use this information as an indicator of coherence.</Paragraph>
<Paragraph position="4"> 2. The Thesaurus and Lexical Cohesion The thesaurus was conceived by Peter Mark Roget, who described it as being the &amp;quot;converse&amp;quot; of a dictionary. A dictionary explains the meaning of words, whereas a thesaurus aids in finding the words that best express an idea or meaning. In Section 3, we will show how a thesaurus can be used to find lexical chains in text.</Paragraph>
</Section>
<Section position="5" start_page="26" end_page="27" type="sub_section">
<SectionTitle>
2.1 The Structure of the Thesaurus
</SectionTitle>
<Paragraph position="0"> Roget's International Thesaurus, 4th Edition (1977) is composed of 1042 sequentially numbered basic categories. There is a hierarchical structure both above and below this level (see Figure 1). Three structure levels are above the category level. The topmost level consists of eight major classes developed by Roget in 1852: abstract relations, space, physics, matter, sensation, intellect, volition, and affections. Each class is divided into (roman-numbered) subclasses, and under each subclass there is a (capitalletter-sequenced) sub-subclass. These in turn are divided into the basic categories.</Paragraph>
<Paragraph position="1"> Where applicable, categories are organized into antonym pairs. For example, category 407 is Life, and category 408 is Death.</Paragraph>
<Paragraph position="2"> Each category contains a series of numbered paragraphs to group closely related words. Within each paragraph, still finer groups are marked by semicolons. In addition, a semicolon group may have cross-references or pointers to other related categories or paragraphs. A paragraph contains words of only one syntactic category. The noun paragraphs are grouped at the start of a category, followed by the paragraphs for  407 Life 1. NOUNS life, living, vitality, being alive, having life, animation, animate existence; liveliaess, animal spirits, vivacity, spriteliness; long llfe, longevity; viability; lifetime 110.5; immortality 112.3; birth 167; existence 1; bio-, organ-; -biosis.</Paragraph>
<Paragraph position="3"> 2....</Paragraph>
<Paragraph position="4"> 408 Death...</Paragraph>
<Paragraph position="6"> The structure of Roget's Thesaurus  verbs, adjectives, and so on. The thesaurus has an index, which allows for retrieval of words related to a given one. For each entry, a list of words suggesting its various distinct subsenses is given, and a category or paragraph number for each of these. Figure 2 shows the index entry for lid. To find words related to lid in its sense of cover, one would turn to paragraph 5 of category 228. An index entry may be a pointer to a category or paragraph if there are no subsenses to be distinguished.</Paragraph>
</Section>
<Section position="6" start_page="27" end_page="28" type="sub_section">
<SectionTitle>
2.2 Differences from Traditional Knowledge Bases
</SectionTitle>
<Paragraph position="0"> In the structure of traditional artificial intelligence knowledge bases, such as frames or semantic networks, words or ideas that are related are actually &amp;quot;physically close&amp;quot; in the representation. In a thesaurus this need not be true. Physical closeness has some importance, as can be seen clearly from the hierarchy, but words in the index of the thesaurus often have widely scattered categories, and each category often points to a widely scattered selection of categories.</Paragraph>
<Paragraph position="1"> The thesaurus simply groups words by idea. It does not have to name or classify the idea or relationship. In traditional knowledge bases, the relationships must be named. For example, in a semantic net, a relationship might be isa or color-of, and in a frame database, there might be a slot for color or location.</Paragraph>
<Paragraph position="2"> In Section 1, different types of word relationships were discussed: systematic semantic, nonsystematic semantic, word association, and words related by a common situation. A factor common to all but situational relationships is that there is a strong tendency for the word relationships to be captured in the thesaurus. This holds even for the nonsystematic semantic relations, which are the most problematic by definition. A thesaurus simply groups related words without attempting to explicitly name each relationship. In a traditional computer database, a systematic semantic relationship can be represented by a slot value for a frame, or by a named link in a semantic network. If it is hard to classify a relationship in a systematic semantic way, it will be hard to represent the relationship in a traditional frame or semantic network formalism. Of the 16 nonsystematic semantic lexical chains given as examples in Halliday and Hasan (1976), 14 were found in Roget's Thesaurus (1977) using the relations given in Section 3.2.2. This represents an 87% hit rate (but not a big sample space). Word associations show a strong tendency to be findable in a thesaurus. Of the 16 word association pairs given in Hirst (1987), 14 were found in Roget's Thesaurus (1977). Since two of the word senses were not contained in the thesaurus at all, this represents a 100% hit rate among those that were. Situational word relationships are not as likely to be found in a general thesaurus. An example of a situational relationship is between car and lights, where the two words are clearly related in the situation involving a car's lights, but the relationship will not be found between them in a general thesaurus.</Paragraph>
<Paragraph position="3">  Morris and Hirst Lexical Cohesion 3. Finding Lexical Chains</Paragraph>
</Section>
<Section position="7" start_page="28" end_page="28" type="sub_section">
<SectionTitle>
3.1 General Methodology
</SectionTitle>
<Paragraph position="0"> We now describe a method of building lexical chains for use as an aid in determining the structure of text. This section details how these lexical chains are formed, using a thesaurus as the main knowledge base. The method is intended to be useful for text in any general domain. Unlike methods that depend on a full understanding of text, our method is the basis of a computationally feasible approach to determining discourse structure.</Paragraph>
<Paragraph position="1"> We developed our method in the following way. First, we took five texts, totaling 183 sentences, from general-interest magazines (Reader's Digest, Equinox, The New Yorker, Toronto, and The Toronto Star). Using our intuition (i.e., common sense and a knowledge of English), we identified the lexical chains in each text. We then formalized our intuitions into an algorithm, using our experience with the texts to set values for the following parameters (to be discussed below).</Paragraph>
<Paragraph position="2">  * thesaural relations * transitivity of word relations * distance (in sentences) allowable between words in a chain  The aim was to find efficient, plausible methods that will cover enough cases to ensure the production of meaningful results.</Paragraph>
</Section>
<Section position="8" start_page="28" end_page="32" type="sub_section">
<SectionTitle>
3.2 Forming Lexical Chains
</SectionTitle>
<Paragraph position="0"> the text are candidates for inclusion in chains. As pointed out by Halliday and Hasan (1976), repetitive occurrences of closed-class words such as pronouns, prepositions, and verbal auxiliaries are obviously not considered. Also, high-frequency words like good, do, and taking do not normally enter into lexical chains (with some exceptions such as takings used in the sense of earnings). For example, in (13) only the italicized words should be considered as lexical chain candidates: Example 13 My maternal grandfather lived to be 111. Zayde was lucid to the end, but a few years before he died the family assigned me the task of talking to him about his problem with alcohol.</Paragraph>
<Paragraph position="1"> It should be noted that morphological analysis on candidate words was done intuitively, and would actually have to be formally implemented in an automated system.  formed. For this work an abridged version of Roget's Thesaurus (1977) was used. The chains were built by hand. Automation was not possible, for lack of a machine-readable copy of the thesaurus. Given a copy, implementation would clearly be straightforward. It is expected that research with an automated system and a large sample space of text would give valuable information on the fine-tuning of the parameter settings used in the general algorithm.</Paragraph>
<Paragraph position="2"> Five types of thesaural relations between words were found to be necessary in forming chains, but two (the first two below) are by far the most prevalent, constituting  Computational Linguistics Volume 17, Number 1 over 90% of the lexical relationships. The relationships are the following: 1. Two words have a category common in their index entries. For example, residentialness and apartment both have category 189 in their index entries (see Figure 3.1).</Paragraph>
<Paragraph position="3"> 2. One word has a category in its index entry that contains a pointer to a category of the other word. For example car has category 273 in its index entry, and that contains a pointer to category 276, which is a category of the word driving (see Figure 3.2).</Paragraph>
<Paragraph position="4"> 3. A word is either a label in the other word's index entry (see Figure 3.3b), or is in a category of the other word. For example, blind has category 442 in its index entry, which contains the word see (see Figure 3.3a).</Paragraph>
<Paragraph position="5"> 4. Two words are in the same group, and hence are semantically related.</Paragraph>
<Paragraph position="6"> For example, blind has category 442, blindness, in its index entry and see has category 441, vision, in its index entry (see Figure 3.4).</Paragraph>
<Paragraph position="7"> 5. The two words have categories in their index entries that both point to a common category. For example, brutal has category 851, which in turn (i) (2)  word 1 index \] label 1:521 l label 2:589 J label 3:626 J word 2 index I / I label 1:860 7-I label 2:521 word 1 index \] label 2:589 label 3: 626~  I thesaurus category 457 I Figure 3 Continued. Thesaural Relations, parts (4)-(5) has a pointer to category 830. Terrified has category 860 that likewise has a pointer to category 830 (see Figure 3.5).</Paragraph>
<Paragraph position="8"> One must consider how much transitivity to use when computing lexical chains. Specifically, if word a is related to word b, word b is related to word c, and word c is related to word d then is word a related to words c and d? Consider this chain: {cow, sheep, wool, scarf, boots, hat, snow}. If unlimited transitivity were allowed, then cow and snow would be considered related, which is definitely counter intuitive. Our intuition was to allow one transitive link: word a is related to word c but not to word d. It seemed that two or more transitive links would so severely weaken the word relationship as to cause it to be nonintuitive. Our analysis of our sample texts supported this. To summarize, a transitivity of one link is sufficient to successfully compute the intuitive chains. An automated system could be used to test this out extensively, varying the number of transitive links and calculating the consequences. It is likely that it varies slightly with respect to style, author, or type of text.</Paragraph>
<Paragraph position="9"> There are two ways in which a transitive relation involving one link can cause two words to be related. In the first way, if word a is related to word b, and word b is related to word c, then word a is related to word c. In the second way, if word a is related to word b, and word a is related to word c, then word b is related to word c. But lexical chains are calculated only with respect to the text read so far. For example, if word c is related to word a and to word b, then word a and word b are not related, since at the time of processing, they were not relatable. Symmetry was not found to be necessary for computing the lexical chains.</Paragraph>
<Paragraph position="10"> We now consider how many sentences can separate two words in a lexical chain before the words should be considered unrelated. Now, sometimes, several sentences after a chain has clearly stopped, it is returned to. Such chain returns link together larger expanses of text than are contained in single chains or chain segments. Returns  Computational Linguistics Volume 17, Number 1 to existing chains often correspond to intentional boundaries, as they occur after digressions or subintentions, thereby signalling a resumption of some structural text entity.</Paragraph>
<Paragraph position="11"> Intuitively, the distance between words in a chain is a factor in chain formation. The distance will not be &amp;quot;large,&amp;quot; because words in a chain co-relate due to recognizable relations, and large distances would interfere with the recognition of relations. The five texts were analyzed with respect to distance between clearly related words. The analysis showed that there can be up to two or three intermediary sentences between a word and the preceding element of a chain segment with which it can be linked. At distances of four or more intermediary sentences, the word is only able to signal a return to an existing chain. Returns happened after between 4 and 19 intermediary sentences in the sample texts. One significant fact emerged from this analysis: returns consisting of one word only were always made with a repetition of one of the words in the returned-to chain. Returns consisting of more than one word did not necessarily use repetition -- in fact in most cases, the first word in the return was not a repetition.</Paragraph>
<Paragraph position="12"> The question of chain returns and when they can occur requires further research. When distances between relatable words are not tightly bound (as in the case of returns), the chances of incorrect chain linkages increase. It is anticipated that chain return analysis would become integrated with other text processing tools in order to prevent this. Also, we believe that chain strength analysis will be required for this purpose. Intuitively, some lexical chains are &amp;quot;stronger&amp;quot; than others, and possibly only strong chains can be returned to. There are three factors contributing to chain strength.  1. Reiteration -- the more repetitions, the stronger the chain.</Paragraph>
<Paragraph position="13"> 2. Density -- the denser the chain, the stronger it is.</Paragraph>
<Paragraph position="14"> 3. Length -- the longer the chain, the stronger it is.</Paragraph>
<Paragraph position="15">  Ideally, some combination of values reflecting these three factors should result in a chain strength value that can be useful in determining whether a chain is strong enough to be returned to. Also, a strong chain should be more likely to have a structural correspondence than a weak one. It seems likely that chains could contain particularly strong portions with special implications for structure. These issues will not be addressed here.</Paragraph>
<Paragraph position="16">  information is kept for each word in a chain: A word number, which is a sequential, chain-based number for each word so that it can be uniquely identified.</Paragraph>
<Paragraph position="17"> The sentence number in which the word occurs.</Paragraph>
<Paragraph position="18"> The chain created so far.</Paragraph>
<Paragraph position="19"> Each lexical relationship in a chain is represented as (u,v)~ where:  -- the number of the thesaural relationship between the two words (as given in Section 3.2.2) Tq where T stands for transitively related q is the word number through which the transitive relation is formed.</Paragraph>
<Paragraph position="20"> A full example of this notation is shown in Figure 4.</Paragraph>
<Paragraph position="21"> Figure 5 shows the generalized algorithm for computing lexical chains. The pa- null rameter values that we used are shown for the following: * candidate words * thesaural relations * transitivity of word relations * distance between words in a chain.</Paragraph>
<Paragraph position="22">  The only parameter not addressed in this work is which (if any) chains should be eliminated from the chain-finding process.</Paragraph>
</Section>
<Section position="9" start_page="32" end_page="34" type="sub_section">
<SectionTitle>
3.3 Problems and Concerns
</SectionTitle>
<Paragraph position="0"> This section is a discussion of problems encountered during the computation of the lexical chains contained in our corpus of texts. The text example used in this paper is in Section 4.2, and the chains found in the example are in Appendix A.</Paragraph>
<Paragraph position="1"> 3.3.1 Where the Thesaurus Failed to Find Lexical Relations. The algorithm found well over 90% of the intuitive lexical relations in the five examples we studied. The following is an analysis of when the thesaurus failed to find a relationship and why.</Paragraph>
<Paragraph position="2"> One problem was when the relationship between words was due more to their &amp;quot;feel&amp;quot; than their meaning. For example, in chain 6, the intuitive chain {hand-in-hand, matching, whispering, laughing, warm} was not entirely computable. Only the italicized words were relatable. The words in chain 6 are cohesive by virtue of being general, but strong, &amp;quot;good&amp;quot; words related by their goodness, rather than by their specific meanings. Chain 10, {environment, setting, surrounding}, was not thesaurally relatable. Setting was  not in the thesaurus, and while it seems as though environment and surrounding should be thesaurally connected, they were not.</Paragraph>
<Paragraph position="3"> Place names, street names, and people's names are generally not to be found in Roget's Thesaurus (1977). However, they are certainly contained in one's &amp;quot;mental thesaurus.&amp;quot; Chain 1, which contains several major Toronto street names, is a good example of this. These names were certainly related to the rest of chain 1 in the authors' mental thesaurus, since we are residents of Toronto (and indeed the article assumed a knowledge of the geography of the city). In chain 5, the thesaurus did not connect the words pine and trunk with the rest of the chain {virgin, bush, trees, trees}. In a general thesaurus, specific information on, and classification of, plants, animals, minerals, etc., is not available.</Paragraph>
<Paragraph position="4"> To summarize, there were few cases in which the thesaurus failed to confirm an intuitive lexical chain. For those cases in which the thesaurus did fail, three missing knowledge sources became apparent.</Paragraph>
<Paragraph position="5">  1. General semantic relations between words of similar &amp;quot;feeling.&amp;quot; 2. Situational knowledge.</Paragraph>
<Paragraph position="6"> 3. Specific proper names.</Paragraph>
<Paragraph position="7"> 3.3.2 Problems with Distances and Chain Returns. Occasionally the algorithm would cause two chains to merge together, whereas intuition would lead one to keep them  Morris and Hirst Lexical Cohesion separate. We found the following intuitively separate chain beginning in sentence 38: {people, Metropolitan Toronto, people, urban, population, people, population, population, people}. However, the algorithm linked this chain with chain 1, which runs through the entire example and consists of these words and others: {city, suburbs, traffic, community}. Fortunately, this was a rare occurrence. But note that there will be cases in which lexical chains should be merged as a result of the intentional merging of ideas or concepts in the text.</Paragraph>
<Paragraph position="8"> Conversely, there were a few cases of unfortunate chain returns occurring where they were definitely counter intuitive. In chain 3, word 4, wife, was taken as a one-word return to the chain {married, wife, wife}. However, there is no intuitive reason for this.</Paragraph>
<Paragraph position="9"> 4. Using Lexical Chains to Determine Text Structure This section describes how lexical chains formed by the algorithm given in Section 3.2.3 can be used as a tool.</Paragraph>
</Section>
<Section position="10" start_page="34" end_page="35" type="sub_section">
<SectionTitle>
4.1 Lexical Chains and Text Structure
</SectionTitle>
<Paragraph position="0"> Any structural theory of text must be concerned with identifying units of text that are about the same thing. When a unit of text is about the same thing there is a strong tendency for semantically related words to be used within that unit. By definition, lexical chains are chains of semantically related words. Therefore it makes sense to use them as clues to the structure of the text.</Paragraph>
<Paragraph position="1"> This section will concentrate on analyzing correspondences between lexical chains and structural units of text, including:  * the correspondence of chain boundaries to structural unit boundaries; * returns to existing chains and what they indicate about structural units; * lexical chain strength and reliability of predicting correspondences between chains and structural units; * an analysis of problems encountered, and when extra textual information is required to validate the correspondences between lexical chains and  structural components.</Paragraph>
<Paragraph position="2"> The text structure theory chosen for this analysis was that of Grosz and Sidner (1986). it was chosen because it is an attempt at a general domain-independent theory of text structure that has gained a significant acceptance in the field as a good standard approach.</Paragraph>
<Paragraph position="3"> The methodology we used in our analyses was as follows: 1. We determined the lexical chain structure of the text using the algorithm given in Section 3.2.3. (In certain rare cases where the algorithm did not form intuitive lexical chains properly, it is noted, both in Section 3.4 and in the analysis in this section. The intuitive chain was used for the analysis; however the lexical chain data given in Appendix A show the rare mismatches between intuition and the algorithm.)  2. We determined the intentional structure of the text using the theory outlined by Grosz and Sidner.</Paragraph>
<Paragraph position="4">  Computational Linguistics Volume 17, Number 1 . We compared the lexical structure formed in step 1 with the intentional structure formed in step 2, and looked for correspondences between them.</Paragraph>
</Section>
<Section position="11" start_page="35" end_page="37" type="sub_section">
<SectionTitle>
4.2 An Example
</SectionTitle>
<Paragraph position="0"> Example 14 shows one of the five texts that we analyzed. It is the first section of an article in Toronto magazine, December 1987, by Jay Teitel, entitled &amp;quot;Outland. &amp;quot;2 The tables in Appendix A show the lexical chains for the text. (The other four texts and their analyses are given in Morris 1988.)  Example 14 1. PI spent the first 19 years of my life in the suburbs, the initial 14 or so relatively contented, the last four or five wanting mainly to be elsewhere.</Paragraph>
<Paragraph position="1"> 2. The final two I remember vividly: I passed them driving to and from the University of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness. 3. The car's lights never worked -- every dusk turned into a kind of medieval race against darkness, a panicky, mounfful rush north, away from everything I knew was exciting, toward everything I knew was deadly.</Paragraph>
<Paragraph position="2"> 4. I remember looking through the windows at the commuters mired in traffic beside me and actively hating them for their passivity.</Paragraph>
<Paragraph position="3"> 5. I actually punched holes in the white vinyl ceiling of the Volks and then, by way of penance, wrote beside them the names and phone numbers of the girls I would call when I had my own apartment in the city.</Paragraph>
<Paragraph position="4"> 6. One thing I swore to myself: I would never live in the suburbs again.</Paragraph>
<Paragraph position="5"> 7. PMy aversion was as much a matter of environment as it was traffic -- one particular piece of the suburban setting: the &amp;quot;cruel sun.&amp;quot; 8. Growing up in the suburbs you can get used to a surprising number of things -the relentless &amp;quot;residentialness&amp;quot; of your surroundings, the weird certainty you have that everything will stay vaguely new-looking and immune to historic soul no matter how many years pass.</Paragraph>
<Paragraph position="6"> 9. You don't notice the eerie silence that descends each weekday when every sound is drained out of your neighbourhood along with all the people who've gone to work. 10. I got used to pizza, and cars, and the fact that the cultural hub of my community was the collective TV set.</Paragraph>
<Paragraph position="7"> 11. But once a week I would step outside as dusk was about to fall and be absolutely bowled over by the setting sun, slanting huge and cold across the untreed front lawns, reminding me not just how barren and sterile, but how undefended life could be. 12. As much as I hated the suburban drive to school, I wanted to get away from the cruel suburban sun.</Paragraph>
<Paragraph position="8"> 13. PWhen I was married a few years later, my attitude hadn't changed.</Paragraph>
<Paragraph position="9"> 14. My wife was a city girl herself, and although her reaction to the suburbs was less intense than mine, we lived in a series of apartments safely straddling Bloor Street. 15. But four years ago, we had a second child, and simultaneously the school my wife taught at moved to Bathurst Street north of Finch Avenue.</Paragraph>
<Paragraph position="10"> 2 Q Jay Teitel. Reprinted with kind permission of the author.</Paragraph>
<Paragraph position="11"> 36 Morris and Hirst Lexical Cohesion 16. She was now driving 45 minutes north to work every morning, along a route that was perversely identical to the one I'd driven in college.</Paragraph>
<Paragraph position="12"> 17. PWe started looking for a house.</Paragraph>
<Paragraph position="13"> 18. Our first limit was St. Clair -- we would go no farther north.</Paragraph>
<Paragraph position="14"> 19. When we took a closer look at the price tags in the area though, we conceded that maybe we'd have to go to Eglinton -- but that was definitely it.</Paragraph>
<Paragraph position="15"> 20. But the streets whose names had once been magical barriers, latitudes of tolerance, quickly changed to something else as the Sundays passed.</Paragraph>
<Paragraph position="16"> 21. Eglinton became Lawrence, which became Wilson, which became Sheppard.</Paragraph>
<Paragraph position="17"> 22. One wind-swept day in May I found myself sitting in a town-house development north of Steeles Avenue called Shakespeare Estates.</Paragraph>
<Paragraph position="18"> 23. It wasn't until we stepped outside, and the sun, blazing unopposed over a country club, smacked me in the eyes, that I came to.</Paragraph>
<Paragraph position="19"> 24. It was the cruel sun.</Paragraph>
<Paragraph position="20"> 25. We got into the car and drove back to the Danforth and porches as fast as we could, grateful to have been reprieved.</Paragraph>
<Paragraph position="21"> 26. PAnd then one Sunday in June I drove north alone.</Paragraph>
<Paragraph position="22"> 27. This time I drove up Bathurst past my wife's new school, hit Steeles, and kept going, beyond Centre Street and past Highway 7 as well.</Paragraph>
<Paragraph position="23"> 28. I passed farms, a man selling lobsters out of his trunk on the shoulder of the road, a chronic care hospital, a country club and what looked like a mosque.</Paragraph>
<Paragraph position="24"> 29. I reached a light and turned right.</Paragraph>
<Paragraph position="25"> 30. I saw a sign that said Houses and turned right again.</Paragraph>
<Paragraph position="26"> 31. PIn front of me lay a virgin crescent cut out of pine bush.</Paragraph>
<Paragraph position="27"> 32. A dozen houses were going up, in various stages of construction, surrounded by hummocks of dry earth and stands of precariously tall trees nude halfway up their trunks.</Paragraph>
<Paragraph position="28"> 33. They were the kind of trees you might see in the mountains.</Paragraph>
<Paragraph position="29"> 34. A couple was walking hand-in-hand up the dusty dirt roadway, wearing matching blue track suits.</Paragraph>
<Paragraph position="30"> 35. On a &amp;quot;front lawn&amp;quot; beyond them, several little girls with hair exactly the same colour of blond as my daughter's were whispering and laughing together.</Paragraph>
<Paragraph position="31"> 36. The air smelled of sawdust and sun.</Paragraph>
<Paragraph position="32"> 37. PIt was a suburb, but somehow different from any suburb I knew.</Paragraph>
<Paragraph position="33"> 38. It felt warm.</Paragraph>
<Paragraph position="34"> 39. PIt was Casa Drive.</Paragraph>
<Paragraph position="35"> 40. PIn 1976 there were 2,124,291 people in Metropolitan Toronto, an area bordered by Steeles Avenue to the north, Etobicoke Creek on the west, and the Rouge River to the east.</Paragraph>
<Paragraph position="36"> 41. In 1986, the same area contained 2,192,721 people, an increase of 3 percent, all but negligible on an urban scale.</Paragraph>
<Paragraph position="37"> 42. In the same span of time the three outlying regions stretching across the top of Metro -- Peel, Durham, and York -- increased in population by 55 percent, from 814,000 to some 1,262,000.</Paragraph>
<Paragraph position="38"> 43. Half a million people had poured into the crescent north of Toronto in the space of  a decade, during which time the population of the City of Toronto actually declined as did the populations of the &amp;quot;old&amp;quot; suburbs with the exception of Etobicoke and Scarborough.</Paragraph>
<Paragraph position="39"> 44. If the sprawling agglomeration of people known as Toronto has boomed in the past 10 years it has boomed outside the traditional city confines in a totally new city, a new suburbia containing one and a quarter million people.</Paragraph>
</Section>
<Section position="12" start_page="37" end_page="37" type="sub_section">
<SectionTitle>
Computational Linguistics Volume 17, Number 1
4.3 The Correspondences between Lexical and Intentional Structures
</SectionTitle>
<Paragraph position="0"> In Figure 6 we show the intentional structure of the text of Section 4.2, and in Figure 7 we show the correspondences between the lexical chains and intentions of the example.</Paragraph>
<Paragraph position="1"> There is a clear correspondenc.e between chain 1, { .... driving, car's .... }, and intention I (changing attitudes to suburban life). The continuity of the subject matter is reflected by the continuous lexical chain. From sentence 40 to sentence 44, two words, population and people are used repetitively in the chain. Population is repeated three times, and people is repeated five times. If chain strength (indicated by the reiteration) were used to delineate &amp;quot;strong&amp;quot; portions of a chain, this strength information could also be used to indicate structural attributes of the text. Specifically, sentences 40 to</Paragraph>
</Section>
<Section position="13" start_page="37" end_page="38" type="sub_section">
<SectionTitle>
2.2 16 end of 1.1.3.1 16
2.3 24 end of 1.1.3.3 25
</SectionTitle>
<Paragraph position="0"> Correspondences between lexical and intentional structures  Morris and Hirst Lexical Cohesion chain would correspond exactly to a structural unit. In addition, drive was repeated eight times between sentence 2 and sentence 26, corresponding to intention 1.1 (earlier aversion to suburban life). Suburb was repeated eleven times throughout the entire example, indicating the continuity in structure between sentences 1-44. Chain 2.1, {afflicted, darkness .... }, from sentence 2 to sentence 12, corresponds to intentions 1.1.1 (hatred of commuting) and 1.1.2 (hatred of suburbs). More textual information is needed to separate intentions 1.1.1 and 1.1.2. There is a one-word return to chain 2 at sentences 16 and 24, strongly indicating that chain 2 corresponds to intention 1.1, which runs from sentence 1 to sentence 25. Also, segment 2.2 coincides with the end of intention 1.1.3.1 (how life changed), and segment 2.3 coincides with the end of intention 1.1.3.3 (old familiar aversion to suburbs). This situation illustrates how chain returns help indicate the structure of the text. If chain returns were not considered, chain 2 would end at sentence 12, and the structural implications of the two single-word returns would be lost. It is intuitive that the two words perverse and cruel indicate links back to the rest of intention 1.1. The link provided by the last return, cruel, is especially strong, since it occurs after the diversion describing the attempt to find a nice house in the suburbs. Cruel is the third reiteration of the word in chain 2. Chain 3, {married, wife .... }, corresponds to intention 1.1.3.1 (if the unfortunate chain return mentioned in section 3.4.2 is ignored) and chain 4 {conceded, tolerance}, corresponds to intention 1.1.3.2 (expensive houses in Metro Toronto). The boundaries of chain 4 are two sentences inside the boundaries of the intention. The existence of a lexical chain is a clue to the existence of a separate intention, and boundaries within one or two sentences of the intention boundaries are considered to be close matches.</Paragraph>
<Paragraph position="1"> Chain 5, {virgin, pine .... }, corresponds closely to intention 1.2.2 (forested area).</Paragraph>
<Paragraph position="2"> Chain 6, {hand-in-hand, matching .... }, corresponds closely to intention 1.2.3 (pleasant environment). Chains 7, {first, initial, final}, and 8, {night, dusk, darkness}, are a couple of short chains (three words long) that overlap. They collectively correspond to intention 1.1.1 (hatred of commuting). The fact that they are short and overlapping suggests that they could be taken together as a whole.</Paragraph>
<Paragraph position="3"> Chain 9, {environment, setting, surrounding}, corresponds to intention 1.1.2 (hated suburbs). Even though the chain is a lot shorter in length than the intention, its presence is a clue to the existence of a separate intention in its textual vicinity. Since the lexical chain boundary is more than two sentences away from the intention boundary, other textual information would be required to confirm the structure.</Paragraph>
<Paragraph position="4"> Overall, the lexical chains found in this example provide a good clue for the determination of the intentional structure. In some cases, the chains correspond exactly to an intention. It should also be stressed, however, that the lexical structures cannot be used on their own to predict an exact structural partitioning of the text. This of course was never expected. As a good example of the limitations of the tool, intention 1.2 (nice new suburb) starts in sentence 26, but there are no new lexical chains starting there. The only clue to the start of the new intention would be the ending of chain 2 {afflicted, darkness .... }.</Paragraph>
<Paragraph position="5"> This example also provides a good illustration (chain 2) of the importance of chain returns being used to indicate a high-level intention spanning the length of the entire chain (including all segments). Also, the returns coincided with intentional boundaries.</Paragraph>
</Section>
</Section>
</Paper>

