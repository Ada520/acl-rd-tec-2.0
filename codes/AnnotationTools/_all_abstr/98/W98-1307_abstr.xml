<?xml version="1.0" standalone="yes"?>

<Paper uid="W98-1307">
<Title>Learning Finite-State Models for Language Understanding*</Title>
<Section position="2" start_page="0" end_page="69" type="abstr">
<SectionTitle>
1 Introduction
</SectionTitle>
<Paragraph position="0"> Language Understanding (LU) has been the focus of much research work in the last twenty years.</Paragraph>
<Paragraph position="1"> Many classical approaches typically consider LU from a linguistically motivated, generalistic point of view. Nevertheless, it is interesting to note tllat, in contrast with some general-purpose formulations of LU, many applications of interest to industry and business have limited domains; that is, lexicons are of small size and the semantic universe is limited. If we restrict ourselves to these kinds of tasks, many aspects of system design can be dramatically simplified.</Paragraph>
<Paragraph position="2"> In fact, under the limited-domain framework, the ultimate goal Of a system is to driue the actions associated to the meaning conveyed by the sentences issued by the users. Since actions are to be performed by machines, the understanding problem can then be simply formulated as translating the natural language sentences into .?orma/sentences of an adequate (computer) command language in which the actions to be carried out can.be specified. For example, &amp;quot;understanding&amp;quot; natural language (spOken) queries to a database can be seen as &amp;quot;translating&amp;quot; these queries into appropriate computer-language code to access the database. Clearly, under such an assumption, LU can be seen as a possibly simpler case of Language Translation in which the output language is forma/rather than natural Hopefully, these simplifications can lead to new systems that are more compact and faster to build thant those developed under more traditional paradigms. This would entail i) to devise simple and easily understandable models for LU, ii) to formulate LU as some kind of optimal search through an adequate structure based on these models, and iii) to develop techniques to actually learn the LU models from training data of each considered task. All these requirements can be easily met through the use of Finite-State Translation Models.</Paragraph>
<Paragraph position="3"> The capabilities of Finite-State Models (FSM) have been the object of much debate in the past few years. On the one hand, in the Natural Language (NL) community, FSMs have often  been ruled out for many NL processing applications, including LU, even in limited domains. Recently, many NL and Computational Linguistic researchers are (re-)considering the interesting features of FSMs for their use in NL processing applications \[10\].</Paragraph>
<Paragraph position="4"> Undoubtedly, the most attractive feature of FSMs consists in their simplicity: representation is just a matter of setting a network of nodes and links in memory, and parsing can be simply carried out by appropriately following the links of this network, according to the observed input data. More specifically, as it is well known, using Viterbi-like techniques, computing time for parsing is linear with the length of the data sequence to be parsed and, using adequate techniques, such as beam search, it can be easily made independent on the size of the network in practice. \[2\] Simple as they are, FSMs generally need to be huge in order to be useful approximations to complex languages. For instance, an adequate 3--Gram Language Model for the language of the Wall Street Journal is a FSM that may have as many as 20 million edges \[23\]. Obviously, there is no point in trying to manually build such models on the base of a priori knowledge about the language to be modeled: the success lies in the possibility of automatically learning them from large enough sets of training data \[8, 23\]. This is also the case for the finite-state LU models used in the work presented in this paper \[15, 24, 26\].</Paragraph>
</Section>
</Paper>

