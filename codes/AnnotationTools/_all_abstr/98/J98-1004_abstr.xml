<?xml version="1.0" standalone="yes"?>

<Paper uid="J98-1004">
<Title>Automatic Word Sense Discrimination Hinrich Sch{itze* Xerox Palo Alto Research Center</Title>
<Section position="2" start_page="0" end_page="98" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> Word sense disambiguation is the task of assigning sense labels to occurrences of an ambiguous word. This problem can be divided into two subproblems: sense discrimination and sense labeling. Sense discrimination divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not. Sense labeling assigns a sense to each class, and, in combination with sense discrimination, to each occurrence of the ambiguous word. This view of disambiguation as a two-stage process may not be completely general (for example, it may not be appropriate for the iterative process by which a lexicographer arrives at the sense divisions of a dictionary entry), but it seems applicable to most work on disambiguation in computational linguistics.</Paragraph>
<Paragraph position="1"> In this paper, we will address the problem of sense discrimination as defined above. That is, we will not be concerned with the sense-labeling component of word sense disambiguation. Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is. Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation. If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses. Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden.</Paragraph>
<Paragraph position="2"> What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses.</Paragraph>
<Paragraph position="3"> * Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.</Paragraph>
<Paragraph position="4"> In many applications, word sense disambiguation must both discriminate and label occurrences; for example, in order to find the correct translation of an ambiguous word in machine translation or the right pronunciation in a text-to-speech system.</Paragraph>
<Paragraph position="5"> The application of interest to us is information access, i.e., making sense of and finding information in large text databases. For many problems in information access, it is sufficient to solve the discrimination problem only. In one study, we measured document-query similarity based on word senses rather than words and achieved a considerable improvement in ranking relevant documents ahead of nonrelevant documents (Schi.itze and Pedersen 1995). Since the measurement of similarity is a system-internal process, no reference to externally defined senses need be made. Another potentially beneficial application of word sense discrimination in information access is the design of interfaces that take account of ambiguity. If a user enters a query that contains an ambiguous word, a system capable of discrimination can give examples of the different senses of the word in the text database. The user can then decide which sense was intended and only documents with the intended sense would be retrieved.</Paragraph>
<Paragraph position="6"> Again, a reference to external sense definitions is not required for this task.</Paragraph>
<Paragraph position="7"> The algorithm we propose in this paper is context-group discrimination. 1 Context-group discrimination groups the occurrences of an ambiguous word into clusters, where clusters consist of contextually similar occurrences. Words, contexts, and clusters are represented in a high-dimensional, real-valued vector space. Context vectors capture the information present in second-order co-occurrence. Instead of forming a context representation from the words that the ambiguous word directly occurs with in a particular context (first-order co-occurrence), we form the context representation from the words that these words in turn co-occur with in the training corpus. Second-order co-occurrence information is less sparse and more robust than first-order information.</Paragraph>
<Paragraph position="8"> In context-group discrimination, the context of each occurrence of the ambiguous word in the training corpus is represented as a context vector formed from second-order co-occurrence information. The context vectors are then clustered into coherent groups such that occurrences judged similar according to second-order co-occurrence are assigned to the same cluster. Clusters are represented by their centroids, the average of their elements. An occurrence in a test text is disambiguated by computing the second-order representation of the relevant context, and assigning it to the cluster whose centroid is closest to that representation. Since the choice of representation influences the formation of clusters, we will experiment with several representations in this paper, some involving a dimensionality reduction using singular value decomposition (SVD).</Paragraph>
<Paragraph position="9"> Context-group discrimination can be generalized to do a discrimination task that goes beyond the notion of sense that underlies many other contributions to the disambiguation literature. If the ambiguous word's occurrences are clustered into a large number n of clusters (e.g., n = 10), then the clusters can capture fine contextual distinctions. Consider the example of space. For a small number of clusters, only the senses &amp;quot;outer space&amp;quot; and &amp;quot;limited extent in one, two, or three dimensions&amp;quot; are separated. If the word's occurrences are clustered into more clusters, then finer distinctions such as the one between &amp;quot;office space&amp;quot; and &amp;quot;exhibition space&amp;quot; are also discovered. Note that differences between sense entries in dictionaries are often similarly fine-grained.</Paragraph>
<Paragraph position="10">  The basic design of context-group discrimination. Contexts of the ambiguous word in the training set are mapped to context vectors in Word Space (upper dashed arrow) by summing the vectors of the words in the context. The context vectors are grouped into clusters (dotted lines) and represented by sense vectors, their centroids (squares). A context of the ambiguous word (&amp;quot;test context&amp;quot;) is disambiguated by mapping it to a context vector in Word Space (lower dashed arrow ending in circle). The context is assigned to the sense with the closest sense vector (solid arrow).</Paragraph>
<Paragraph position="11"> Even if the contextual distinctions captured by generalized context-group discrimination do not line up perfectly with finer distinctions made in dictionaries, they still help characterize the contextual meaning in which the ambiguous word is used in a particular instance. Such a characterization is useful for the information-access applications described above, among others.</Paragraph>
<Paragraph position="12"> The basic idea of context-group discrimination is to induce senses from contextual similarity. There is some evidence that contextual similarity also plays a crucial role in human semantic categorization. Miller and Charles (1991) found evidence in several experiments that humans determine the semantic similarity of words from the similarity of the contexts they are used in. We hypothesize that, by extension, senses are also based on contextual similarity: a sense is a group of contextually similar occurrences of a word.</Paragraph>
<Paragraph position="13"> The following sections describe the disambiguation algorithm, our evaluation, and the results of the algorithm for a test set drawn from the New York Times News Wire, and discuss the relevance of our approach in the context of other work on word sense disambiguation.</Paragraph>
</Section>
</Paper>

