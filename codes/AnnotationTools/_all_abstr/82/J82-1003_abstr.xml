<?xml version="1.0" standalone="yes"?>

<Paper uid="J82-1003">
<Title>From English to Logic: Context-Free Computation of &amp;quot;Conventional&amp;quot; Logical Translation 1</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> Our ultimate objective is the design of a natural language understanding system whose syntactic, semantic and pragmatic capabilities are encoded in an easily comprehensible and extensible form. In addition, these encodings should be capable of supporting efficient algorithms for parsing and comprehension.</Paragraph>
<Paragraph position="1"> In our view, the achievement of the former objective calls for a careful structural separation of the sub-systems that specify possible constituent structure (syntax), possible mappings from constituent structure to underlying logical form (part of semantics), and possible mappings from logical form to deeper, unambiguous representations as a function of discourse context and world knowledge (part of pragmatics and l Submitted August 1981; revised July 1982.</Paragraph>
<Paragraph position="2"> inference). This sort of view is now widely held, as evidenced by a recent panel discussion on parsing issues (Robinson 1981). In the words of one of the panelists, &amp;quot;I take it to be uncontroversial that, other things being equal, a homogenized system is less preferable on both practical and scientific grounds than one that naturally decomposes.</Paragraph>
<Paragraph position="3"> Practically, such a system is easier to build and maintain, since the parts can be designed, developed, and understood to a certain extent in isolation... Scientifically, a decomposable system is much more likely to provide insight into the process of natural language comprehension, whether by machines or people.&amp;quot; (Kaplan 1981) The panelists also emphasized that structural decomposition by no means precludes interleaving or paral-Copyright 1982 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/82/010026-19501.00 26 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic lelism of the processes that draw on the various kinds of linguistic and non-linguistic knowledge.</Paragraph>
<Paragraph position="4"> Note that we are making a distinction between the logical form that corresponds directly to surface structure on the one hand, and an unambiguous deeper representation on the other. Indeed, at the level of logical form our theory of logical translation admits ambiguities in all of the formal building blocks (terms, functions, predicates, connectives, and quantifiers), as well as in the scopes of quantifiers and coordinators.</Paragraph>
<Paragraph position="5"> For example, logical-form translations may contain terms such as Mary2 and &lt;thel (little2 girl3)&gt;, ambiguous between various referents (e.g., MARY5 and MARY17), and quasi-predicates such as has3, good2, cold5, and recovers l, ambiguous between various proper predicates (e.g., has3: OWNS1, AFFLICTED-WITH .... ; good2: VIRTUOUS, GOOD-TASTING .... ; cold5: COLD1, EMOTIONLESS .... ; and recoversl: RE-COVERS1, REGAINS .... ). In other words, we do not regard the logical form of a sentence as fully determining its meaning - not even its 'literal' meaning; rather, its meaning is determined by its logical form along with the context of its utterance. Thus &amp;quot;She is becoming cold&amp;quot; might convey on one occasion that Lady Godiva is beginning to feel cold, on another that Queen Victoria is becoming emotionless, and on a third that Mount St. Helens is cooling off; but the logical form does no more than specify the feminine gender of the referent and its property of &amp;quot;becoming cold (in some sense) at the time of utterance&amp;quot;. Our primary concern in this paper will be with the semantic rules that define immediate logical form, although we attempt to define this form in a way that minimizes the remaining gap to the deeper representation.</Paragraph>
<Paragraph position="6"> All the experience gained within AI and linguistics suggests that bridging this final gap will be very difficult. Some would take as their lesson that research efforts should concentrate on the last, pragmatic phase of comprehension, where 'the real problems' lie. We believe on the contrary that the only way to make the pragmatic problems tractable is to have a precise conception of the constituent structure and logical form of the natural language input, in terms of which the pragmatic operations can in turn be precisely formulated.</Paragraph>
<Paragraph position="7"> In AI research, the objectives of clarity and extensibility have often been sacrificed to immediate performance goals. One reason for this may have been the need to establish the credibility of a relatively young and controversial discipline. In any case, the state of linguistic theory until fairly recently left no real alternatives. The transformational grammars whose study dominated theoretical linguistics seemed a poor prospect even for the limited goal of describing natural language syntax, because of the subtlety of transformational rules and supplementary devices such as co-indexing procedures, filters and constraints on movement, and the complexity of their interactions.</Paragraph>
<Paragraph position="8"> Moreover, the prospects for writing efficient transformational parsers seemed poor, given that transformational grammars can in principle generate all recursively enumerable languages. But most importantly, generative grammarians developed syntactic theories more or less independently of any semantic considerations, offering no guidance to AI researchers whose primary objective was to compute 'meaning representations' for natural language utterances. Katz and Fodor's markerese (Katz &amp; Fodor 1963) was patently inadequate as a meaning representation language from an AI point of view, and Generative Semantics (Lakoff 1971) never did develop into a formal theory of the relation between surface form and meaning.</Paragraph>
<Paragraph position="9"> Theoretical linguistics took an important new turn with the work of Montague on the logic of English and later expansions and variants of his theory (e.g., see Thomason 1974a, Partee 1976a, and Cresswell 1973).</Paragraph>
<Paragraph position="10"> According to Montague grammar the correspondence between syntactic structure and logical form is much simpler than had generally been supposed: to each lexeme there corresponds a logical term or functor and to each rule of syntactic composition there corresponds a structurally analogous semantic rule of logical composition; this is the so-called rule-to-rule hypothesis \[Bach 1976\]. 2 Furthermore, the translations of all consituents of a particular syntactic category are assigned formal meanings of the same set-theoretic type; for example, all NPs, be they names or definite or indefinite descriptions, are taken to denote property sets. Crucially, the formal semantics of the logical translations produced by the semantic rules of Montague grammar accords by and large with intuitions about entailment, synonymy, ambiguity and other semantic phenomena.</Paragraph>
<Paragraph position="11"> 2 Interestingly enough, this linguistic hypothesis was anticipated by Knuth's work on the semantics of attribute grammars (Knuth 1968). Schwind (1978) has applied Knuth's insights to the development of a formal basis for question answering systems, anticipating some of the work by Gazdar and others on which our own efforts are founded.</Paragraph>
<Paragraph position="12"> There is also some similarity between the rule-to-rule hypothesis and the rule-based approach to the interpretation of syntactic structures that emerged within AI during the 1960's and early 70's. The idea of pairing semantic rules with phrase structure rules was at the heart of DEACON (Craig et al. 1966), a system based on F. B. Thompson's proposal to formalize English by limiting its subject matter to well-defined computer memory structures (Thompson 1966). However, DEACON's semantic rules performed direct semantic evaluation of sorts (via computations over a data base) rather than constructing logical translations. The systems of Winograd (1972) and Woods (1977) constructed input translations prior to evaluation, using semantic rules associated with particular syntactic structures. However, these rules neither corresponded one-to-one to syntactic rules nor limited interpretive operations to composition of logical expressions; for example, they incorporated tests for selectional restrictions and other forms of inference, with unrestricted use of the computational power of LISP.</Paragraph>
<Paragraph position="13"> American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 27 Lenhart K. Schubert and Francis Jeffry Pelletier From English to Logic The chief limitation of Montague's grammar was that it treated only very small, syntactically (though not semantically) simple fragments of English, and efforts were soon under way to extend the fragments, in some cases by addition of a transformational component (Partee 1976b, Cooper &amp; Parsons 1976). At the same time, however, linguists dissatisfied with transformational theory were beginning to develop non-transformational alternatives to traditional generative grammars (e.g., Peters &amp; Ritchie 1969, Bresnan 1978, Lapointe 1977, Brame 1978, Langendoen 1979). A particularly promising theory that emerged from this development, and explicitly incorporates Montague's approach to semantics, is the phrase structure theory advanced by Gazdar and others (Gazdar 1980, 1981, Gazdar, Pullum &amp; Sag 1980, Gazdar &amp; Sag 1980, Sag 1980, Gazdar, Klein, Pullum &amp; Sag, to appear). The theory covers a wide range of the syntactic phenomena that have exercised transformationalists from Chomsky onward, including subcategorization, coordination, passivization, and unbounded dependencies such as those occurring in topicalization, relative clause constructions and comparatives. Yet the grammar itself makes no use of transformations; it consists entirely of phrase structure rules, with a node-admissibility rather than generative interpretation. For example, the rule \[(S) (NP) (VP)\] states that a fragment with root S, left branch NP and right branch VP is an admissible fragment of a syntactic tree. 3 Such phrase structure rules are easy to understand and permit the use of efficient context-free parsing methods.</Paragraph>
<Paragraph position="14"> Moreover, the grammar realizes the rule-to-rule hypothesis, pairing each syntactic rule with a Montaguelike semantic rule that supplies the intensional logic translation of the constituent admitted by the syntactic rule.</Paragraph>
<Paragraph position="15"> It has long been assumed by transformationalists that linguistic generalizations cannot be adequately captured in a grammar devoid of transformations.</Paragraph>
<Paragraph position="16"> Gazdar refutes the assumption by using metagrammatical devices to achieve descriptive elegance. These devices include rule-schemata (e.g., coordination schemata that yield the rules of coordinate structure for all coordinators and all syntactic categories), and metarules (e.g., a passive metarule that takes any transitive-VP rule as 'input' and generates a corresponding passive-VP rule as 'output' by deleting the</Paragraph>
</Section>
</Paper>

