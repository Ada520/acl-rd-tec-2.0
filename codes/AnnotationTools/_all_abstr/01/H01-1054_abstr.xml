<?xml version="1.0" standalone="yes"?>

<Paper uid="H01-1054">
<Title>Multidocument Summarization via Information Extraction</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
PAKISTAN MAY BE PREPARING FOR ANOTHER TEST
</SectionTitle>
<Paragraph position="0"> Thousands of people are feared dead following... (voiceover) ...a powerful earthquake that hit Afghanistan today. The quake registered 6.9 on the Richter scale, centered in a remote part of the country. (on camera) Details now hard to come by, but reports say entire villages were buried by the quake.</Paragraph>
<Paragraph position="1"> human-effect: victim: Thousands of people number: Thousands outcome: dead confidence: medium confidence-marker: feared physical-effect: object: entire villages outcome: damaged confidence: medium confidence-marker: Details now hard to come by / reports say  criteria. The adjusted scores are used to select the most important slots/sentences to include in the summary, subject to the userspecified word limit. In the third and final stage, the summary is generated from the resulting content pool using a combination of top-down, schema-like text building rules and surface-oriented revisions. The extracted sentences are simply listed in document order, grouped into blocks of adjacent sentences.</Paragraph>
<Paragraph position="2"> (2) any intermediate estimates that are lower than the maximum estimate.1 In the content determination stage, scores are assigned to the derived information units based on the maximum score of the underlying units. In the summary generation stage, a handful of text planning rules are used to organize the text for these derived units, highlighting agreement and disagreement across sources.</Paragraph>
<Paragraph position="3">  In order to intelligently merge and summarize scenario templates, we found it necessary to explicitly handle numeric estimates of varying specificity. While we did find specific numbers (such as 3,000) in some damage estimates, we also found cases with no number phrase at all (e.g. entire villages). In between these extremes, we found vague estimates (thousands) and ranges of numbers (anywhere from 2,000 to 5,000). We also found phrases that cannot be easily compared (more than half the region's residents). To merge related damage information, we first calculate the numeric specificity of the estimate as one of the values NONE, VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence of a small set of trigger words and phrases (e.g. several, as many as, from ... to). Next, we identify the most specific current estimates by news source, where a later estimate is considered to update an earlier estimate if it is at least as specific. Finally, we determine two types of derived information units, namely (1) the minimum and maximum estimates across the news sources, and  In our initial attempt to include extracted sentences, we simply chose the top ranking sentences that would fit within the word limit, subject to the constraint that no more than one sentence per cluster could be chosen, in order to help avoid redundancy. We found that this approach often yielded summaries with very poor coherence, as many of the included sentences were difficult to make sense of in isolation. To improve the coherence of the extracted sentences, we have experimented with trying to boost coherence by favoring sentences in the context of the highest-ranking sentences over those with lower ranking scores, following the hypothesis that it is better to cover fewer topics in more depth than to change topics excessively. In particular, we assign a score to a set of sentences by summing the base scores plus increasing coherence boosts for adjacent sentences, sentences that precede ones with an initial Less specific estimates such as &amp;quot;hundreds&amp;quot; are considered lower than more specific numbers such as &amp;quot;5000&amp;quot; when they are lower by more than a factor of 10.</Paragraph>
<Paragraph position="4"> Earthquake strikes Afghanistan A powerful earthquake struck Afghanistan last Saturday at 11:25. The earthquake was centered in a remote part of the country and had a magnitude of 6.9 on the Richter scale.</Paragraph>
<Paragraph position="5"> Earthquake strikes quake-devastated villages in northern Afghanistan A earthquake struck quake-devastated villages in northern Afghanistan Saturday. The earthquake had a magnitude of 6.9 on the Richter scale on the Richter scale.</Paragraph>
<Paragraph position="6"> Damage Estimates of the death toll varied. VOA (06/02/1998) provided the highest estimate of 5,000 dead. CNN (05/31/1998) and CNN (06/02/1998) supplied lower estimates of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) gave the lowest estimate of anywhere from 2,000 to 5,000 dead. People were injured, while thousands more were missing. Thousands were homeless. Quake-devastated villages were damaged. Estimates of the number of villages destroyed varied. CNN (05/31/1998) provided the highest estimate of 50 destroyed, whereas VOA (06/04/1998) gave the lowest estimate of at least 25 destroyed. In Afghanistan, thousands of people were killed.</Paragraph>
<Paragraph position="7"> Damage Estimates of the death toll varied. CNN (06/02/1998) provided the highest estimate of 4,000 dead, whereas ABC (06/01/1998) gave the lowest estimate of 140 dead. In capital: Estimates of the number injured varied.</Paragraph>
<Section position="1" start_page="0" end_page="0" type="sub_section">
<SectionTitle>
Selected News Excerpts
</SectionTitle>
<Paragraph position="0"> CNN (06/01/98): Thousands are dead and thousands more are still missing. Red cross officials say the first priority is the injured. Getting medicine to them is difficult due to the remoteness of the villages affected by the quake. PRI (06/01/98): We spoke to the head of the international red cross there, Bob McCaro on a satellite phone link. He says it's difficult to know the full extent of the damage because the region is so remote. There's very little infrastructure. PRI (06/01/98): Bob McCaro is the head of the international red cross in the neighboring country of Pakistan. He's been speaking to us from there on the line. APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid. Figure 4. 200 word summary of actual IE output, with emphasis on Red Cross</Paragraph>
</Section>
<Section position="2" start_page="0" end_page="0" type="sub_section">
<SectionTitle>
Further Details
</SectionTitle>
<Paragraph position="0"> Heavy after shocks shook northern afghanistan. More homes were destroyed. More villages were damaged. Landslides or mud slides hit the area. Another massive quake struck the same region three months earlier. Some 2,300 victims were injured.</Paragraph>
</Section>
<Section position="3" start_page="0" end_page="0" type="sub_section">
<SectionTitle>
Selected News Excerpts
</SectionTitle>
<Paragraph position="0"> ABC (05/30/98): PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following... ABC (06/01/98): RESCUE WORKERS CHALLENGED IN AFGHANISTAN There has been serious death and devastation overseas. In Afghanistan... CNN (06/02/98): Food, water, medicine and other supplies have started to arrive. But a U.N. relief coordinator says it's a &amp;quot;scenario from hell&amp;quot;. Figure 3. 200 word summary of simulated IE output, with emphasis on damage cases. We then perform a randomized local search for a good set of sentences according to these scoring criteria.</Paragraph>
<Paragraph position="1">  The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex's Exemplars Framework [13]. The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations. In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a &amp;quot;rough draft&amp;quot; of the summary text. In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text. In the second generation step, the Exemplars text polishing component makes use of this markup to trigger  surfacepronoun, and sentences that preceded ones with strongly connecting discourse markers such as however, nevertheless, etc. We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such fioriented revision rules that smooth the text into a more polished form. A distinguishing feature of our text polishing approach is the use of a bootstrapping tool to partially automate the acquisition of application-specific revision rules from examples.</Paragraph>
<Paragraph position="2">  Figures 3 and 4 show two sample summaries that were included in our evaluation (see Section 3 for details). The summary in Figure 3 was generated from simulated output of the IE system, with preference given to damage information; the summary in Figure 4 was generated from the actual output of the current IE system, with preference given to information including the words Red Cross. While the summary in Figure 3 does a reasonable job of reporting the various current estimates of the death toll, the estimates of the death toll shown in Figure 4 are less accurate, because the IE system failed to extract some reports, and the Summarizer failed to correctly merge others. In particular, note that the lowest estimate of 140 dead attributed to ABC is actually a report about the number of school children killed in a particular town. Since no location was given for this estimate by the IE system, the Summarizer's simple heuristic for localized damaged reports -- namely, to consider a damage report to be localized if a location is given that is not in the same sentence as the initial disaster description -- did not work here. The summary in Figure 3 also suffered from some problems with merging: the inclusion of a paragraph about thousands killed in Afghanistan is due to an incorrect classification of this report as a localized one (owing to an error in sentence boundary detection), and the discussion of the number of villages damaged should have included a report of at least 80 towns or villages damaged. Besides the problems related to slot extraction and merging mentioned above, the summaries shown in Figures 3 and 4 suffer from relatively poor fluency. In particular, the summaries could benefit from better use of descriptive terms from the original articles, as well as better methods of sentence combination and rhetorical structuring.</Paragraph>
<Paragraph position="3"> Nevertheless, as will be discussed further in Section 4, we suggest that the summaries show the potential for our techniques to intelligently combine information from many articles on the same natural disaster.</Paragraph>
<Paragraph position="4"> earlier version of the Summarizer uses the simulated output of the IE system as its input, including the relief annotations; in the second variant (RIPTIDES-SIM2), the current version of the Summarizer uses the simulated output of the IE system, without the relief annotations; and in the third variant (RIPTIDES-IE), the Summarizer uses the actual output of the IE system as its input.2 Summaries generated by the RIPTIDES variants were compared to a Baseline system consisting of a simple, sentence-extraction multidocument summarizer relying only on document position, recency, and word overlap clustering. (As explained in the previous section, we have found that word overlap clustering provides a bare bones way to help determine what information is repeated in multiple articles, thereby indicating importance to the document set as a whole, as well as to help reduce redundancy in the resulting summaries.) In addition, the RIPTIDES and Baseline system summaries were compared against the summaries of two human authors. All of the summaries were graded with respect to content, organization, and readability on an A-F scale by three graduate students, all of whom were unfamiliar with this project. Note that the grades for RIPTIDES-SIM1, the Baseline system, and the two human authors were assigned during a first evaluation in October, 2000, whereas the grades for RIPTIDESSIM2 and RIPTIDES-IE were assigned by the same graders in an update to this evaluation in April, 2001. Each system and author was asked to generate four summaries of different lengths and emphases: (1) a 100-word summary of the May 30 and May 31 articles; (2) a 400-word summary of all test articles, emphasizing specific, factual information; (3) a 200-word summary of all test articles, focusing on the damage caused by the quake, and excluding information about relief efforts, and (4) a 200-word summary of all test articles, focusing on the relief efforts, and highlighting the Red Cross's role in these efforts. The results are shown in Tables 1 and 2. Table 1 provides the overall grade for each system or author averaged across all graders and summaries, where each assigned grade has first been converted to a number (with A=4.0 and F=0.0) and the average converted back to a letter grade. Table 2 shows the mean and standard deviations of the overall, content, organization, and readability scores for the RIPTIDES and the Baseline systems averaged across all graders and summaries. Where the differences vs. the Baseline system are significant according to the t-test, the p-values are shown. Given the amount of development effort that has gone into the system to date, we were not surprised that the RIPTIDES variants fared poorly when compared against the manually written summaries, with RIPTIDES-SIM2 receiving an average grade of C, vs. A- and B+ for the human authors.</Paragraph>
<Paragraph position="5"> Nevertheless, we were pleased to find that RIPTIDES-SIM2 scored a full grade ahead of the Baseline summarizer, which received a D, and that</Paragraph>
</Section>
</Section>
</Paper>

