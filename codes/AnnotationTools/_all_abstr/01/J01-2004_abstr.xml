<?xml version="1.0" standalone="yes"?>

<Paper uid="J01-2004">
<Title>Probabilistic Top-Down Parsing and Language Modeling</Title>
<Section position="2" start_page="0" end_page="250" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> With certain exceptions, computational linguists have in the past generally formed a separate research community from speech recognition researchers, despite some obvious overlap of interest. Perhaps one reason for this is that, until relatively recently, few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems. In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists. While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task.</Paragraph>
<Paragraph position="1"> This paper will examine language modeling for speech recognition from a natural language processing point of view. Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed. A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility. Two features of our top-down parsing approach will emerge as key to its success.</Paragraph>
<Paragraph position="2"> First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for  * Department of Cognitive and Linguistic Sciences, Box 1978, Brown University, Providence, RI 02912 (~) 2001 Association for Computational Linguistics  Computational Linguistics Volume 27, Number 2 each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar. A left-to-right parser whose derivations are not rooted, i.e., with derivations that can consist of disconnected tree fragments, such as an LR or shift-reduce parser, cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar, because their derivations include probability mass from unrooted structures. Only at the point when their derivations become rooted (at the end of the string) can generative string probabilities be calculated from the grammar. These parsers can calculate word probabilities based upon the parser state--as in Chelba and Jelinek (1998a)--but such a distribution is not generative from the probabilistic grammar.</Paragraph>
<Paragraph position="3"> A parser that is not left to right, but which has rooted derivations, e.g., a head-first parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order. For example, suppose that there are two possible verbs that could be the head of a sentence. For a head-first parser, some derivations will have the first verb as the head of the sentence, and the second verb will be generated after the first; hence the second verb's probability will be conditioned on the first verb. Other derivations will have the second verb as the head of the sentence, and the first verb's probability will be conditioned on the second verb. In such a scenario, there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule. Of course, the joint probability can be used as a language model, but it cannot be interpolated on a word-by-word basis with, say, a trigram model, which we will demonstrate is a useful thing to do.</Paragraph>
<Paragraph position="4"> Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis II 1970).</Paragraph>
<Paragraph position="5"> A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model. Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search. In contrast, an Earley or left-corner parser will underspecify certain connections between constituents in the left context, and if some of the underspecified information is used in the conditional probability model,- it will have to become specified. Of course, this can be done, but at the expense of search efficiency; the more that this is done, the less benefit there is from the underspecification. A top-down parser will, in contrast, derive an efficiency benefit from precisely the information that is underspecified in these other approaches.</Paragraph>
<Paragraph position="6"> Thus, our top-down parser makes it very easy to condition the probabilistic grammar on an arbitrary number of values extracted from the rooted, fully specified derivation. This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive. The top-down guidance that is provided makes this approach quite efficient in practice.</Paragraph>
<Paragraph position="7"> The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition. There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</Paragraph>
<Paragraph position="8">  Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</Paragraph>
</Section>
</Paper>

