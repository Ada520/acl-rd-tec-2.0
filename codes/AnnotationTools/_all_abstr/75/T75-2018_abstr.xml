<?xml version="1.0" standalone="yes"?>

<Paper uid="T75-2018">
<Title>H T E R IS MARTHA. N I C H O L A S ALSO L I V E S IN THE SAME LAND. N I C H O L A S IS OF M I R A C U L O U S BIRTH. B A L D A K HAS A MAGIC STEED. A BEAR A P P E A R S IN THE D I S T A N T PROVINCE. THE BEAR S E I Z E S THE MAGIC STEED. B A L D A K CALLS FOR HELP FROM NICHOLAS. N I C H O L A S D E C I D E S TO SEARCH FOR THE MAGIC STEED. N I C H O L A S LEAVES ON A SEARCH. N I C H O L A S MEETS A JUG ALONG THE WAY. THE JUG IS F I G H T I N G WITH ELENA OVER A MAGIC BOW. THE JUG ASKS N I C H O L A S TO DIVIDE THE MAGIC BOW. N I C H O L A S TRICKS THE D I S P U T A N T S INTO LEAVING 85 IV. LOGICAL QUANTIFICATION, PARSING, P R E S U P P O S I T I O N A L A N A L Y S I S SEMANTIC</Title>
<Section position="1" start_page="0" end_page="0" type="abstr">
<SectionTitle>
META-COMPILING TEXT GRAMMARS AS A MODEL FOR HUMAN BEHAVIOR Sheldon Klein
</SectionTitle>
<Paragraph position="0"> (Propp 1968) which generated 50 Russian fairytales, according to the r u l e s of his text grammar, at an average speed of 128 words a second, again including plot computations and specification of deep structure as well as surface syntax (Klein et al 1974, Klein et all 1975). Our earliest automatic text generation work used syntactic dependency network/graphs with 2-valued labelling of edges as an approximation to semantic network/graphs with multi-valued labelling of edges (Klein &amp; Simmons 1963, Klein 1965a, 1965b). Our work on automatic inference of grammars includes the world's first program for learning context free, phrase structure grammars, for both natural and artificial languages, and the first program for learning transformational grammars (Klein 1967, Klein et al 1968, Klein &amp; Kuppin 1970). More recent inference work includes the formulation of techniques for automatic inference of generative semantic grammars (Klein 1973) and for the ontogeny of Pidgin and Creole languages (Klein &amp; Rozencvejg 1974). In formulating components for automatic inference of rules in the meta-symbolic simulation system, we find that the common notation for the semantics of the non-verbal behavioral simulation r u l e s and natural language means that the same learning heuristics may be used to infer behavioral rules as well as linguistic rules. The implication is that the totality of human verbal and non-verbal behavior, in complex social groups, both synchronically and diachronically, may now be modelled within the same notational framework. What for us started as a generalized device for testing varying theoretical models as part of an effort to model language change and variation (Klein 1974a, Klein &amp; Rozencvejg 1974) now appears as the basis for a higher level theory of the linguistic basis of human behavior (Klein 1974b).</Paragraph>
<Paragraph position="2"> In our efforts to model the totality of synchronic and diachronic language behavior in complex social groups, we developed a meta-symbolic simulation system that includes a powerful behavioral simulation programming language that models, generates and manipulates events in the notation of a semantic network that changes through time, and a generalized, semantics-to-surface structure generation mechanism that can describe changes in the semantic universe in the syntax of any natural language for which a grammar is supplied. Because the system is a meta-theoretical device, it can handle generative semantic grammars formulated within a variety of theoretical frameworks. A key feature of the system is that the semantic deep structure of the non-verbal, behavioral rules may be represented in the same network notation as the semantics for natural language grammars, and, as a consequence, provide non-verbal context for linguistic rules. We are also experimenting with a natural language meta-compiling capability, that is, the use of the semantic network to generate productions in the simulation language itself -- productions in the form of &amp;quot;texts&amp;quot; that may themselves be compiled as new behavioral rules during the flow of the simulation -- rules that may themselves control the process of deriving new rules. This feature permits non-verbal behavioral rules to be derived from natural language conversational inputs, and through inference techniques identical with those for inferring natural language generative semantic grammars. The total system has the power of at least the 2nd order predicate calculus, and will facilitate the formulation of highly abstract meta-models of discourse, including the logical quantification of such models. Achievements with the generative portion of the system include a text grammar model that generates 2100 word murder mystery stories in less than 19 seconds each, complete with calculation of the plot and specification of the deep structure as well as the surface syntax (Klein et al 1973). The speed of this generation is 100 to 1000 times faster than other existing programs using transformational grammars. (The algorithm for the semantics-to-surface structure generative component is such that processing time increases only linearly as a f u n c t i o n of sentence length and syntactic complexity.) More recent achievements include models of portions of Levi-Strauss&amp;quot; mythology work in The Raw &amp; the Cooked (Levi-Strauss 1969) and a model for Propp's Morphology of the</Paragraph>
<Paragraph position="4"> II.</Paragraph>
</Section>
</Paper>

