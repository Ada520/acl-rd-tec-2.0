<?xml version="1.0" standalone="yes"?>

<Paper uid="J04-4003">
<Title>c(c) 2004 Association for Computational Linguistics Fast Approximate Search in Large Dictionaries Stoyan Mihov [?]</Title>
<Section position="2" start_page="0" end_page="454" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> In this article, we face a situation in which we receive some input in the form of strings that may be garbled. A dictionary that is assumed to contain all possible correct input strings is at our disposal. The dictionary is used to check whether a given input is correct. If it is not, we would like to select the most plausible correction candidates from the dictionary. We are primarily interested in applications in the area of natural language processing in which the background dictionary is very large and fast selection of an appropriate set of correction candidates is important. By a &amp;quot;dictionary,&amp;quot; we mean any regular (finite or infinite) set of strings. Some possible concrete application scenarios are the following: * The dictionary describes the set of words of a highly inflectional or agglutinating language (e.g., Russian, German, Turkish, Finnish, Hungarian) or a language with compound nouns (German). The dictionary is used by an automated or interactive spelling checker.</Paragraph>
<Paragraph position="1">  * The dictionary is multilingual and describes the set of all words of a family of languages. It is used in a system for postcorrection of results of OCR in which scanned texts have a multilingual vocabulary.</Paragraph>
<Paragraph position="2"> [?] Linguistic Modelling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, 25A, Akad. G. Bonchev Str., 1113 Sofia, Bulgaria. E-mail: stoyan@lml.bas.bg + Centrum f &amp;quot;ur Informations-und Sprachverarbeitung, Ludwig-Maximilians-Universit&amp;quot;at-M &amp;quot;unchen,  Oettingenstr. 67, 80538 Munchen, Germany. E-mail: schulz@cis.uni-muenchen.de Submission received: 12 July 2003; Revised submission received: 28 February 2004; Accepted for publication: 25 March 2004  Computational Linguistics Volume 30, Number 4 * The dictionary describes the set of all indexed words and phrases of an Internet search engine. It is used to determine the plausibility that a new query is correct and to suggest &amp;quot;repaired&amp;quot; queries when the answer set returned is empty.</Paragraph>
<Paragraph position="3"> * The input is a query to some bibliographic search engine. The dictionary contains titles of articles, books, etc.</Paragraph>
<Paragraph position="4"> The selection of an appropriate set of correction candidates for a garbled input P is often based on two steps. First, all entries W of the dictionary are selected for which the distance between P and W does not exceed a given bound k. Popular distance measures are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar, and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992, 1994) Second, statistical data, such as frequency information, may be used to compute a ranking of the correction candidates. In this article, we ignore the ranking problem and concentrate on the first step. For selection of correction candidates we use the standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned applications, the number of correction candidates becomes huge for large values of k. Hence small bounds are more realistic.</Paragraph>
<Paragraph position="5"> In light of this background, the algorithmic problem discussed in the article can be described as follows: Given a pattern P, a dictionary D, and a small bound k, efficiently compute the set of all entries W in D such that the Levenshtein distance between P and W does not exceed k.</Paragraph>
<Paragraph position="6"> We describe a basic method and two refinements for solving this problem. The basic method depends on the new concept of a universal deterministic Levenshtein automaton of fixed degree k. The automaton of degree k may be used to decide, for arbitrary words U and V, whether the Levenshtein distance between U and V does not exceed k. The automaton is &amp;quot;universal&amp;quot; in the sense that it does not depend on U and V. The input of the automaton is a sequence of bitvectors computed from U and V. Though universal Levenshtein automata have not been discussed previously in the literature, determining Levenshtein neighborhood using universal Levenshtein automata is closely related to a more complex table-based method described by the authors Schulz and Mihov (2002). Hence the main advantage of the new notion is its conceptual simplicity. In order to use the automaton for solving the above problem, we assume that the dictionary is given as a determininistic finite-state automaton. The basic method may then be described as a parallel backtracking traversal of the universal Levenshtein automaton and the dictionary automaton. Backtracking procedures of this form are well-known and have been used previously: for example, by Oflazer (1996) and the authors Schulz and Mihov (2002).</Paragraph>
<Paragraph position="7"> For the first refinement of the basic method, a filtering method used in the field of approximate text search is adapted to the problem of approximate search in a dictionary. In this approach, an additional &amp;quot;backwards&amp;quot; dictionary D [?]R (representing the set of all reverses of the words of a given dictionary D) is used to reduce approximate search in D with a given bound k [?] 1 to related search problems for smaller bounds</Paragraph>
<Paragraph position="9"> . As for the basic method, universal Levenshtein automata are used to control the search. Ignoring very short input words and correction bound k = 1,  Mihov and Schulz Fast Approximate Search in Large Dictionaries this approach leads to a drastic increase in speed. Hence the &amp;quot;backwards dictionary method&amp;quot; can be considered the central contribution of this article. The second refinement, which is only interesting for bound k = 1 and short input words, also uses a filtering method from the field of approximate text search (Muth and Manber 1996; Mor and Fraenkel 1981). In this approach, &amp;quot;dictionaries with single deletions&amp;quot; are used to reduce approximate search in a dictionary D with bound k = 1 to a conventional lookup technique for finite-state transducers. Dictionaries with single deletions are constructed by deleting the symbol at a fixed position n in all words of a given dictionary.</Paragraph>
<Paragraph position="10"> For the basic method and the two refinements, detailed evaluation results are given for three dictionaries that differ in terms of the number and average length of entries: a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 symbols), a dictionary of German with 3,871,605 entries (dominated by compound nouns, average length 18.74 symbols), and a dictionary representing a collection of 1,200,073 book titles (average length 47.64 symbols). Tests were restricted to distance bounds k = 1, 2, 3. For the approach based on backwards dictionaries, the average correction time for a given input word--including the displaying of all correction suggestions-is between a few microseconds and a few milliseconds, depending on the dictionary, the length of the input word, and the bound k. Correction times over one millisecond occur only in a few cases for bound k = 3 and short input words. For bound k = 1, which is important for practical applications, average correction times did not exceed 40 microseconds.</Paragraph>
<Paragraph position="11"> As a matter of fact, correction times are a joint result of hardware improvements and algorithmic solutions. In order to judge the quality of the correction procedure in absolute terms, we introduce an &amp;quot;idealized&amp;quot; correction algorithm in which any kind of blind search and superfluous backtracking is eliminated. Based on an analysis of this algorithm, we believe that using purely algorithmic improvements, our correction times can be improved only by a factor of 50-250, depending on the kind of dictionary used. This factor represents a theoretical limit in the sense that the idealized algorithm probably cannot be realized in practice.</Paragraph>
<Paragraph position="12"> This article is structured as follows. In Section 2, we collect some formal preliminaries. In Section 3, we briefly summarize some known techniques from approximate string search in a text. In Section 4, we introduce universal deterministic Levenshtein automata of degree k and describe how the problem of deciding whether the Levenshtein distance between two strings P and W does not exceed k can be efficiently solved using this automaton. Since the method is closely related to a table-based approach introduced by the authors (Schulz and Mihov 2002), most of the formal details have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method, the refined approach based on backwards dictionaries, and the approach based on dictionaries with single deletions. Evaluation results are given for the three dictionaries mentioned above. In Section 8 we briefly comment on the difficulties that we encountered when trying to combine dictionary automata and similarity keys (Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha 1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in Section 9.</Paragraph>
<Paragraph position="13"> The problem considered in this article is well-studied. Since the number of contributions is enormous, a complete review of related work cannot be given here. Relevant references with an emphasis on spell-checking and OCR correction are Blair (1960), Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari, Hull, and Choudhari (1983), Srihari (1985), Takahashi et al. (1990), Kukich (1992), Zobel  Computational Linguistics Volume 30, Number 4 and Dart (1995), and Dengel et al. (1997). Exact or approximate search in a dictionary is discussed, for example, in Wells et al. (1990), Sinha (1990), Bunke (1993), Oflazer (1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate search in texts is described in Section 3.</Paragraph>
</Section>
</Paper>

