<?xml version="1.0" standalone="yes"?>

<Paper uid="P81-1021">
<Title>SOME I33UE3 IH P&amp;RSING AHD NATURAL LINGUAGE UNDERSTANDING</Title>
<Section position="2" start_page="0" end_page="97" type="abstr">
<SectionTitle>
PREAMBLE
</SectionTitle>
<Paragraph position="0"> Our response to the questions posed to this panel is influenced by a number of beliefs (or biasesl) which we have developed in the course of building and analyzin~ the operation of several natural language understanding (NLU) systems. \[I, 2, 3, 12\] While the emphasis of the panel i~ on parslnK, we feel that the recovery of the syntactic structure of a natural lan~unKe utterance must be viewed as part of a larger process of reeoverlnK the meaning, intentions and goals underlying its generation. Hence it is inappropriate to consider designing or evaluatln~ natural language parsers or Erem,~ra without taking into account the architecture of the whole ~LU system of which they're a part. I This is the premise from which Our beliefs arise, beliefs which concern two thinks: o the distribution of various types of knowledge, in particular syntactic knowledge, amonK the modules of an NLU system o the information and control Flow emonK those modules.</Paragraph>
<Paragraph position="1"> As to the first belief, in the HLU systems we have worked on, most syntactic information is localized in a &amp;quot;syntactic module&amp;quot;, although that module does not produce a rallied data structure representing the syntactlo description of an utterance. Thus, if &amp;quot;parslnK&amp;quot; is taken as requlrln~ the production of such a rallied structure, then we do not believe in its necessity. However we do believe in the existence of a module which provides syntactic information to those other parts of the system whose decisions ride on it.</Paragraph>
<Paragraph position="2"> As to the second belief, we feel that syntax, semantics and prattles effectively constitute parallel but interacting processors, and that information such as local syntactic relations is determined by Joint decisions -monk them. Our experience shows that with mlnir&amp;quot;al loss of efficiency, one can design these processors to interface cleanly with one another, so as to allow independent design, implementatlon and modification. We spell out these beliefs in slightly more detail below, and at greater length in \[~\].</Paragraph>
<Paragraph position="3"> 1We are not claiming that the only factors shaping a parser or a gr~-mar, beyond syntaotlo conslderatlofls, are thlrLKs llke meanlng, intention, etc. There are clearly mechanical and memory factors, aa well an laziness - a speoXer's penchant for trylnK to get away with the mdniEal level of effort needed to accomplish the taskf</Paragraph>
<Section position="1" start_page="97" end_page="97" type="sub_section">
<SectionTitle>
The Comoutatiom~l Persneetive
</SectionTitle>
<Paragraph position="0"> The first set of question~ to this panel concern the computational perspective, and the useful purposes served by distinguishing parsing from interpretation.</Paragraph>
<Paragraph position="1"> We believe that syntactic knowledge plays an important role in NLU. In particular, we believe that there is a significant type of utterance description that can be determined on purely syntactic grounds 2, albeit not necessarily uniquely. This description can be used to guide semantic and discourse level structure recovery processes such as interpretation, anaphoric resolution, focus tracking, given/new distinctions, ellipsis resolution, etc. in a manner that is independent of the lexical and conceptual content of the utterance. There are several advantages to factoring out such knowledge from the re,~-~nder of the NLU system and prowlding a * syntactic module&amp;quot; whose interactions with the rest of the system provide information on the syntactic structure of an utterance. The first advantage is to simplify system building, an we know fl-om experience \[I, 2, 3, 4, 5, 12\]. Once the pattern of communication between processors is settled, it is easier to attach a new semnntlcs to the hooks already provided in the Kr~,mar than to build a new semantic processor. In addition, because each module ban only to consider a portion of the constraints implicit in the data (e.g. syntactic constraints, semantic constraints and discourse context), each module can be designed to optimize its own processing and provide an efficient system.</Paragraph>
<Paragraph position="2"> The panel has also been charged wlth _ ~oslderlng paa'allel processing as a challenge to its views on parsing. Thls touches on our beliefs about the Interaction among the modules that comprise the HLU system. To respond to this issue, we first want to dlstlngulsh between two types of parallelism: one, in which many instances of the same thin6 are done at once ~ (an in an array of parallel adders-) and another, in which the many thinks done slmul~aneously can be different. Supporting this latter type of parallelism doesn*t change our view of parsing, but rather underlies it. We believe that the Interconnected processes involved in NLU must support a banjo o~eratinK pri~iple that Herman and Bobrow \[14\] have called &amp;quot;The Principle of Continually Available Output&amp;quot;:, (CAO). This states that the Interactlng processes muat~ benin to provide output over a wide range of resource allocations, even before their analyses are complete, and even before all input data is available. We take this position for two rensons: one, it facilitates computational efficiency, and two, it seems to be closer to human parsing ~rocesses (a point which we will get to in answerlnK the next question).</Paragraph>
<Paragraph position="3"> The requirement that syntactic analysis, semantic interpretation and discourse processlng must be able to operate in (pseudo-)parallel, obeying the CAO</Paragraph>
</Section>
<Section position="2" start_page="97" end_page="97" type="sub_section">
<SectionTitle>
categories/features and ordering Information
</SectionTitle>
<Paragraph position="0"> principle, has sparked our interest in the design of calrs of processes which can pass forward and backward unet~Ll In/ormatlon/advlce/questlons as soon as possible. The added potential for interaction of such processors can increase the capability and efficiency of the overall HLU process. Thus, for example, if the syntactic module makes its intermediate decisions available to semantics and~or pragmatlcs, then those processors can evaluate those decisions, guide syntax's future behavior and, in addition, develop in parallel their own analyses. Having sent on its latest assertlon/advlce/question, whether syntax then decides to continue on with something else or walt for a response will depend on the particular kind of message sent. Thus, the parsers and grammars that concern us are ones able to work with other appropriately designed compoconts to support CAO. While the equipment we are USing to implement and test our ideas is serial, we take very seriously the notion of parallelism.</Paragraph>
<Paragraph position="1"> Finally under the heading of &amp;quot;Computational Perspective&amp;quot;, we are anked about what might motivate our trying to make parsing procedures simulate what we suspect human parsing processes to be like. One motivation for us is the belief that natural language is so tuned to the part extraordinary, part banal cognitive capabilities of human beings that only by simulating human parsing processes can we cover all and only the language phenomena that we are called upon to process. A particular (extraordinary) aspect of hu~an cognitive (and hence, parsing) behavior that we want to explore and eventually simulate is people's ability to respond even under degraded data or resource limitations. There are examples of listeners initiating reasonable responses to an utterance even before the utterance is complete, and in some case even before a complete syntactic unit has been heard.</Paragraph>
<Paragraph position="2"> Simultaneous translation is ode notable example \[8\], and another is provided by the performance of subjects in a verbally guided assembly task reported by P. Cohen \[6\]. Such an ability to produce output before all input data is available (or before enough processing resources have been made available to produce the best possible response) is what led Norman and Bobrow to formulate their CAO Principle. Our interest is in architectures for NLU systems which support CAO and in * search strategies through such architectures for an opti~&amp;quot;l interpretation.</Paragraph>
<Paragraph position="3"> The LimnLiStlC ~rs~etlve We have been asked to comment on legitimate inferences about human linsulstic competence and performance that we can draw from our experiences with mechanical parsing of formal grammar. Our response is that whatever parsing is for natural languages, it is still only part of a larger process. Just because we know what parsing is in formal language systems, we do not secsssarily know what role it plays is in the context Of total communication. Simply put, formal notions of parsing underconstraln the goals of the syntactic component of an NLU system. Efficiency meanures, based on the resources required for generation of one or all complete parses for s sentence, without semantic or pra~e~-tlc Intera~tlon, do not secessarily specify desirable properties of a natural language syntactic analysis component.</Paragraph>
<Paragraph position="4"> As for whether the efficiency of parsing algorlthm~ for CF or regular grammars suggest that the core of NL igremmars la CF or regular, we want to dlstlngulsh that part of perception (and hence, syntactic analysis) which groups the stimulus into recognizable units from that part which fills in gaps in in/ormatlon (inferentially) on the baals of such groups. Results in CF grammar theory says that grouping is not best dose purely bottom-up, that there are advantages to</Paragraph>
<Paragraph position="6"> uslng predictive mechanlsms a~ well \[9, 7\]. Thls snggests two things for parsing natural language: I. There is a level of evidence and a process for using it that is worEing to suggest groups.</Paragraph>
<Paragraph position="7"> 2. There is another filtering, inferenclng mechanism that maEes predictions and diagnoses on the basis of those groups.</Paragraph>
<Paragraph position="8"> It is possible that the grouping mechanism may make use of strategies applicable to CF parsing, such as well-formed substrlng tables or charts, without requiring the overall language specification be CF. In our current RUS/PSI-ELONE system, grouping is a function of the syntactic module: its output consists of suggested groupings. These snggestlons may be at abstract, specific or disjunctive. For example, an abstract description m~ht be &amp;quot;this is the head of an NP, everything to its left is a pre-modifler&amp;quot;. Here there is co comment about exactly how these pre-modlflers group. A disjunctive description would consist of an explicit enumeration of all the possibilities at some point (e.g., &amp;quot;this is either a time prepositional phrase (PP) or an agentive PP or a locative PP, etc.&amp;quot;). Disjunctive descriptions allow us to prune.</Paragraph>
<Paragraph position="9"> possibilities via cane a~alysls.</Paragraph>
<Paragraph position="10"> In short, we believe in using as much evidence from formal systemn a~ seems understandable and reasonable, to constrain what the system should be doing.</Paragraph>
<Paragraph position="11"> The Interaetlons Finally, we have been asked about the nature of the relationship between a gr~mar and a procedure for applying it. On the systems building side, cur feeling is that while one should be able to take a grammar and convert it to a recognition or generation procedure \[I0\], it is likely that such procedures will embody a whole set of principles that are control structure related, and not part of the grammar. For example, a gr',-mr seed not specify in what order to look for thln~s or in what order decisions should be made. Thus, one may not be able to reconstruct the grammar unlcuelv from a procedure for applying it.</Paragraph>
<Paragraph position="12"> On the other hand, on the b,m- parsing side, we definitely feel that natural language is strongly tuned to both people's means of production and their means of recognition, and that principles llke MnDonalds ' Zndeliblllty Pr&amp;quot;Inoiple \[13\] or Marcus' Determinism Hypothesis \[11\] shape what are (and are not) seen an sentences of the language.</Paragraph>
</Section>
</Section>
</Paper>

