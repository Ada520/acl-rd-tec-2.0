<?xml version="1.0" standalone="yes"?>

<Paper uid="H93-1077">
<Title>PROJECT GOALS BoozoAllen &amp; Hamilton's Advanced Decision Systems</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
RECENT RESULTS
</SectionTitle>
<Paragraph position="0"> Our most recent results are those from the first ARPA sponsored TREC (Text Retrieval Conference) held in November 1992.</Paragraph>
<Paragraph position="1"> The TREC corpus represents a significant challenge for our approach. Our previous results with a small corpus, while encouraging, did not allow us to evaluate how well the technique might do with realistically sized document collections. Our conclusion based on the results we have from TREC is that CART does exhibit some interesting behaviors on a realistic corpus, and that, despite the small size of the training sets and the restricted choice of features, for some topics it produces competitive results. So although the overall performance is moderate (relative to the better performing systems at TREC), we believe that the absolute performance (given that the system is totally automatic) is at least encouraging and definitely acceptable in several instances.</Paragraph>
<Paragraph position="2"> Some specific observations on the performance of the current implementation of the CART algorithm as used for TREC are: * Relying on the re-substitution estimates for the terminal nodes is a very weak method for producing an output ranking. A scheme that makes use of surrogate split information to generate a post hoc ranking shows much promise as a technique for improving our scores in the TREC context.</Paragraph>
<Paragraph position="3"> * While our approach is totally automatic, we restricted ourselves to using as features only those words that appear in the information need statement. This is obviously a severe limitation since the use of even simple query expansion techniques (e.g., stemming and/or a synonym dictionary) is likely to provide a richer and more effective set of initial features.</Paragraph>
<Paragraph position="4"> * Using words as features is possibly too &amp;quot;lowlevel&amp;quot; to ever allow stable, robust classification trees to be produced. At a minimum, we probably need to consider working with concepts rather than individual words. Not only would this reduce the size of the feature space but would probably result in more intuitive trees.</Paragraph>
<Paragraph position="5"> * We need to work with much bigger and more representative training sets. Our preliminary experiment in this area shows, not surprisingly, that adding more training examples can lead to dramatic changes in the classification trees.</Paragraph>
</Section>
</Paper>

