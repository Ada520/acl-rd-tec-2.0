<?xml version="1.0" standalone="yes"?>

<Paper uid="J80-1001">
<Title>Cascaded ATN Grammars</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> ATN grammars, as presented in Woods (1970), are a form of augmented pushdown store automata, augmented to carry a set of register contents in addition to state and stack information and to permit arbitrary computational tests and actions associated with the state transitions. Conceptually, an ATN consists of a network of states with connecting arcs between them. Each arc indicates a kind of constituent that can cause a transition between the states it connects. The states in the network can be conceptually divided into &amp;quot;levels&amp;quot; corresponding to the different constituents that can be recognized.</Paragraph>
<Paragraph position="1"> Each such level has a start state and one or more final states. Transitions are of three basic types, as indicated by three different types of arc. A WRD (or CAT) transition corresponds to the consumption of a single word from the input string, a JUMP transition corresponds to a transition from one state to another without consuming any of the in.put string, and a PUSH transition corresponds to the consumption of a phrase parsed by a subordinate invocation of some level of the network to recognize a constituent.</Paragraph>
<Paragraph position="2"> ATN's have the advantage of being a class of automata into which ordinary context-free phrase structure and &amp;quot;augmented&amp;quot; phrase structure grammars have a straightforward embedding, but which permit various transformations to be performed to produce grammars that can be more efficient than the original. Such transformations can reduce the number of states or arcs in the grammar or can reduce the number of alternative hypotheses that need to be explicitly considered during parsing. (Some transformations tend to reduce both, but in general there is a tradeoff between the two). Both kinds of efficiency result from a principle that I have called &amp;quot;factoring&amp;quot;, which amounts to merging common parts of alternative paths in order to reduce the number of alternative combinations explicitly enumerated. The former (&amp;quot;conceptual factoring&amp;quot;) resuits from factoring common parts of the grammar to make the grammar as compact as possible, while the latter (&amp;quot;hypothesis factoring&amp;quot;) results from arranging the grammar so asto factor common parts of the hypotheses that will be enumerated at parse time.</Paragraph>
<Paragraph position="3"> Conceptual factoring promotes ease of human comprehension of the grammar and should facilitate learning of grammars by machine. Hypothesis factoring promotes efficiency of run time execution. In this paper, I will present a generalization of the notion of ATN grammar, called a cascaded ATN or CATN, that capitalizes further on the principle of factoring in a manner similar to serial decomposition of finite state machines. A CATN consists of a sequence of ATN transducers each of which takes its input from the output of the previous. An ATN transducer is an ATN that includes among its actions an output operation (&amp;quot;TRANSMIT&amp;quot;) which can be executed on arcs to generate elements of an output sequence. Such an ATN cascade gains a factoring advantage from merging common computations at early stages of the cascade.</Paragraph>
<Paragraph position="4"> Copyright 1980 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright * notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/80/010001-12501.00 American Journal of Computational Linguistics, Volume 6, Number 1, January-March 1980 1 William A. Woods Cascaded ATN Grammars Cascaded ATN's are analogous to serial decomposition of finite state machines and carry many of the advantages of such decomposition into the domain of more general recognition automata. The normal decomposition of natural language description into levels of phonology, lexicon, syntax, semantics, and pragmatics can be viewed as a cascade of ATN transducers - one for each of the individual levels. Viewing natural language understanding as parsing with such a cascade has computational advantages and also provides an efficient, systematic framework for characterizing the relationships between different levels of analysis due to conceptual factoring. The factoring advantages of cascade decompositions can thus serve as a partial explanation of why such a componential description of natural language understanding has arisen.</Paragraph>
<Paragraph position="5"> 2. Factoring in ATN's and Phrase Structure Grammars As discussed above, the principle of factoring involves the merging of common parts of alternative paths through an ATN or similar structure in order to minimize the number of combinations. This can be done either to reduce the size of the grammar or to reduce the number of alternative hypotheses considered at parse time. Conceptual factoring attempts to reduce the size of the grammar by minimizing the number of places in the grammar where the same or similar constituents are recognized. Frequently such factoring results from &amp;quot;hiding&amp;quot; some of the differences between two paths in registers so that the paths are otherwise the same and can be merged.</Paragraph>
<Paragraph position="6"> For example, in order to represent number agreement between a subject and a verb, one could have two distinct paths through the grammar - one to pick up a singular subject and correspondingly infleeted verb, and one to pick up a plural subject and its verb. By keeping the number of the subject in a register, however, one can merge these two paths so there is only one push to pick up the subject noun phrase and one push to pick up the main verb.</Paragraph>
<Paragraph position="7"> In other cases, conceptual factoring results from merging common initial, final, and/or medial sequences of paths across a constituent that are not the same, but which share subsequences. For example, an interrogative sentence can start with an auxiliary verb followed by the subject noun phrase, while a declarative can start with a noun phrase followed by the auxiliary. In either case, however, the subsequent constituents that can make up the sentence are the same and the grammar paths to recognize them can be merged. Moreover, in either case there can be initial prepositional phrases before either the subject or the auxiliary and again these can be merged. When one begins to represent the details of supporting auxiliaries that are present in interrogatives but not in declaratives, the commonalities these modalities have with imperatives, and the interaction of all three with the various possibilities following the verb (depending on whether it is transitive or intransitive, takes an indirect object or complement, etc.), this kind of factoring becomes increasingly more important.</Paragraph>
<Paragraph position="8"> In ordinary phrase structure grammars (PSG's), the only mechanism for capturing the kinds of merging discussed above is the mechanism of recursion or &amp;quot;pushing&amp;quot; for constituent phrases. In order to capture the equivalent of the above merging of commonality between declaratives and interrogatives, one would have to treat the subject-auxiliary pair as a constituent of some kind (an organization that is linguistically counter-intuitive). Alternatively, one can capture such factoring in a PSG by emulating an ATN - e.g., by constructing a phrase structure rule for every arc in the ATN and treating the states at the ends of the arc as constituents. Specifically, an arc from s l to s2 that picks up a phrase p can be represented by a phrase structure rule sl --&gt; p s2, and a final state s3 can be expressed by an &amp;quot;e rule&amp;quot; s3 --&gt; e (where e represents the &amp;quot;empty string&amp;quot;). In either case, one is forced to introduce a &amp;quot;push&amp;quot; to a lower level of recursion where it is not necessary for an ATN, and to introduce a kind of &amp;quot;constituent&amp;quot; that is motivated solely by principles of factoring and not necessarily by any linguistic criteria of constituenthood.</Paragraph>
<Paragraph position="9"> A phrase structure grammar emulating an ATN as in the above construction will contain all of the factoring that the ATN contains, but will not make a distinction between the state name and the phrase name. Failure to make this distinction masks the intuitions of state transition that lead to some of the ATN optimization transformations and the conceptual understanding of the operation of ATN's as parsing automata. The difference here is a lot like the difference between the way that LISP implements list structure in terms of an underlying binary branching &amp;quot;cons&amp;quot; cell and the way that it is appropriate to view lists for conceptual reasons. For exactly the same kinds of reasons, it is appropriate to think of certain sequences of constituents that make up a phrase as sequences of immediate constituents rather than as a right-recursive nest of binary branching phrases.</Paragraph>
<Paragraph position="10"> From the perspective of hypothesis factoring, the distinction made in an ATN between states that can be recursively pushed to and states that merely mark intermediate stages in the recognition of a constituent sequence permits a distinction between that part of a grammar that is essentially finite state (and hence amenable to certain kinds of optimization) and that which is inherently recursive. This permits such operations as mechanically eliminating unneces-</Paragraph>
</Section>
</Paper>

