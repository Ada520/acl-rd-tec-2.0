<?xml version="1.0" standalone="yes"?>

<Paper uid="J80-3001">
<Title>Computing Story Trees</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> One of the most difficult tasks in the field of computational linguistics is that of processing (parsing or understanding) bodies of connected textual material, from simple narratives like fairy tales and children's stories, to complex technical articles like textbooks and encyclopedia articles. When effective parsers were created capable of processing single sentences (Woods, 1970), (Schank, 1975b), (Norman and Rumelhart, 1975), (Winograd, 1972), it was quickly realized that these same techniques were not in themselves adequate for the larger task of processing sequences of sentences. The understanding of paragraphs involves more knowledge than and different knowledge from that necessary for sentences, and the structures produced by a text parser need not look like the structures of the sentences parsed individually.</Paragraph>
<Paragraph position="1"> However, the original impetus for current trends in text processing was the effort to solve problems of reference at the sentential level, in particular anaphora and ellipsis (Charniak, 19722). For example, in the paragraph John wanted to marry Mary. He asked her if she would marry him, but she refused.</Paragraph>
<Paragraph position="2"> John threatened to foreclose the mortgage on the house where Mary's old sick father lived.</Paragraph>
<Paragraph position="3"> They were married in June.</Paragraph>
<Paragraph position="4"> Simple-minded syntactic techniques are generally insufficient to resolve referents of the form of the &amp;quot;they&amp;quot; in the last sentence above. The human understander - and potentially the computer understander as well - requires real-world knowledge about threats, familial ties and marriage to realize that &amp;quot;they&amp;quot; refers to John and Mary.</Paragraph>
<Paragraph position="5"> Experiments with text processing led to such procedural constructs as frames (Minsky, 1975; Charniak and Wilks, 1976; Bobrow and Winograd, 1977), scripts and plans (Schank and Abelson, 1977), focus spaces (Grosz, 1977), and partitioned networks (Hendrix, 1976), among others. These efforts involved conceptual structures consisting of large, cognitively unified sets of propositions. They modelled understanding as a process of filling in or matching the slots in a particular structure with appropriate entities derived from input text.</Paragraph>
<Paragraph position="6"> There have also been rule-based approaches to the text processing problem, most notably the template/paraplate notion of Wilks (1975), and the story grammars of Rumelhart (1975). Although both approaches (procedures and rules) have their merits, it is a rule-based approach which will be presented here. This paper describes a rule-based computational model for text comprehension, patterned after the theory of macrostructures proposed by Kintsch and van Dijk (1978). The rules are notationally and conceptually derived from the Horn clause, especially as described by Kowalski (1979). Each rule consists of sets of thematically, causally, or temporally related propositions. The rules are organized into a network with the macrostructures becoming more generalized approaching the root. The resulting structure, called the Story Tree, represents a set of textual structures. Copyright 1980 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.  The process of generating from this network consists of choosing one of the rules to serve as the root of a particular story tree and recursively instantiating its descendants until terminal propositions are produced (Simmons and Correira, 1979). These propositions form the text of the generated story, requiring only a final phase to produce English sentences from the propositions. Conversely, a text is understood if its input sentences, parsed into propositions, can be mapped onto rules and these rules recursively mapped onto more abstract rules until a single node (the root) is achieved. Parsing and generating use the same rules in a similar manner for performing their respective tasks, and the rules lend themselves to a uniform tree structure possessing an inherent summarizing property.</Paragraph>
<Paragraph position="7"> 2. Macrostructures and Story Grammar Rules In this section the fundamental notion of macrostructure, as proposed and used by Kintsch and van Dijk, is presented and then analyzed from a computational, rather than a psychological, standpoint. An effective representation for macrostructures, derived from Horn clauses and organized into story trees, is described, as well as a data base for the representation. null</Paragraph>
<Section position="1" start_page="0" end_page="0" type="sub_section">
<SectionTitle>
2.1 Macrostructures
</SectionTitle>
<Paragraph position="0"> Kintsch and van Dijk (1975) present a system for organizing an entire discourse into a hierarchy of macrostructures, which are essentially metapropositions.</Paragraph>
<Paragraph position="1"> The lowest level of a discourse textual representation is the set of input propositions that corresponds semantically to the text sentences, clauses and/or phrases. Propositions are conjoined by links of implication: if proposition A implies proposition B, then A and B are connected, and the link is marked with the strength of the connection, ranging from (barely) possible to (absolutely) necessary. The propositions and their connections reside in a text base. A text base can be either explicit, if all the implied information necessary for coherence is made explicit, or implicit, if propositions that can be assumed to be known or implied are omitted, m text is an explicit data base by itself, and all summaries of that text are implicit data bases. A college physics text would have a much more explicit text base than after-dinner conversation. The simple narrative texts examined in this paper have a text base between these two &amp;quot;extremes.&amp;quot; The sense in which &amp;quot;coherence&amp;quot; is used above is not defined precisely. Kintsch and van Dijk argue that coherence, or &amp;quot;semantic well-formedness&amp;quot;, in a text requires, for each proposition in the text, that it be linked with one or more preceding propositions. This connection must exist for some reader in some context constrained by conventions for knowledge-sharing and assumption-sharing valid for that person in that context. null The result of this linking is a linear text base which is then mapped into a hierarchical structure in which propositions high in the structure are more likely to be recalled (via summaries) than those low in the structure. At the top of the hierarchy can be found propositions corresponding to rhetorical categories, such as &amp;quot;problem&amp;quot; and &amp;quot;solution,&amp;quot; or narrative categories, such as &amp;quot;introduction,&amp;quot; &amp;quot;complication,&amp;quot; and &amp;quot;re solution.&amp;quot; Kintsch and van Dijk introduce a number of rules for relating these macrostructures to sets of input textual propositions: information reduction (generalization), deletion (of less important propositions), integration (combining events with their pre- and postconditions), and construction (which relates complex propositions to their component sub-propositions).</Paragraph>
<Paragraph position="2"> There are two conditions that are always true regarding these macrostructures: a macrostructure must be implied by its subordinate propositions (i.e. encountering the subordinate propositions implies the existence of the macrostructure), and ordered sets of macrostructures collected together form a meaningful summary of the text. Kintsch and van Dijk believe that it is primarily macrostructures that are retained when a text is understood by a human reader and that the macrostructures are created as the text is being processed.</Paragraph>
</Section>
<Section position="2" start_page="0" end_page="0" type="sub_section">
<SectionTitle>
2.2 Macrostructures as Computational Constructs
</SectionTitle>
<Paragraph position="0"> As evidence in support of their theory, Kintsch and van Dijk present a number of psychological experiments in recall and summary with human subjects using as a text a 1600-word narrative taken from Boccaccio's Decameron (the Rufolo story). As a computational entity, a macrostructure is a node in a story tree whose immediate descendants consist of the subordinate propositions by which the node is implied, and is itself a descendant of the macrostructure it (partially) implies. Every macrostructure in this tree is the root of a derivation tree whose terminals are simple propositions.</Paragraph>
<Paragraph position="1"> Each level of the tree shares the attribute of summarizability, i.e. a summary of the text may be extracted from any level of the tree, becoming less specific as the summary level approaches the root. The lowest level summary is the original text itself; the highest level (the root) is a title for the text.</Paragraph>
<Paragraph position="2"> The ability to give meaningful (coherent) summaries for a text is one attribute of comprehension for that text, and any procedure yielding trees possessing the summary property can be said to partially understand the text. Consideration must also be given to classification schemas and rules for paraphrase, ana136 American Journal of Computational Linguistics, Volume 6, Number 3-4, July-December 1980 Alfred Correira Computing Story Trees phora, and question-answering. Furthermore, given an appropriate data base internalizing the relationships between a macrostructure and its subordinate macrostructures or simple propositions (microstructures) and a summary derived from a story tree, it is possible for a procedure to reconstruct to a certain degree of detail the original text from which the tree was derived. The degree of detail recovered is directly dependent on the relative distance from the nodes forming the summary to the original input propositions (the leaves) of the text tree.</Paragraph>
<Paragraph position="3"> How is this subordinate relationship among propositions to be described formally to a computational process? One simple formulation is in the form of a rule A &lt;= B,C,D meaning &amp;quot;you may assert the truth (presence) of macrostructure A if you can find the (nearly) contiguous propositions B, C, and D present in the input text.&amp;quot; &amp;quot;Nearly&amp;quot; means that an allowable level of &amp;quot;noise,&amp;quot; perhaps in the form of irrelevant side information, may be present between the specified propositions (a problem not addressed here).</Paragraph>
<Paragraph position="4"> This rule form closely resembles in structure and meaning the Horn clause notation. The general clause has the format C\[1\] ..... C\[m\] &lt;= A\[1\] ..... A\[n\] where C\[1\] ..... C\[m\] are elementary propositions forming the consequent, and A\[1\] ..... A\[n\] are elementary propositions forming the antecedent. If the propositions in a clause contain the variables x\[1\] ..... x\[i\], then the clause has the interpretation for all x\[1\] ..... x\[i\],</Paragraph>
<Paragraph position="6"> If the subscript m for a clause is zero or one, then that clause is referred to as a Horn clause. If m=l and n=0, the Horn clause is called an assertion.</Paragraph>
<Paragraph position="7"> There are several differences between the Kowalski logic and the logic adopted here. One of these has to do with the ordering of the antecedent propositions.</Paragraph>
<Paragraph position="8"> In a true Horn clause, the ordering is irrelevant and A &lt;= B,C,D is as good a rule as A &lt;= C,D,B, etc., i.e. the antecedents can be proved in any order. The ordering in the system described here is governed by rules of coherence. For example, the rule:</Paragraph>
</Section>
</Section>
</Paper>

