<?xml version="1.0" standalone="yes"?>

<Paper uid="C96-2118">
<Title>An ascription-based approach to Speech Acts</Title>
<Section position="2" start_page="0" end_page="699" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> The use of simplistic belief models has accompanied complex accounts of speech acts where highly nested belief sels accompany any speech act. We believe that by utilising a more sophisticated view of mental attitudes, a simpler and more elegant theory of speech acts can be constructed. Also, as previous work has pointed out (Wilks et al, 1991) past models have failed to differentiate explicitly between the speaker's and hearer's belief sets. Such a failure causes problems in dealing with misconceptions and badly formed plans (Pollack, 1990).</Paragraph>
<Paragraph position="1"> This paper augments ViewGen, a computer program originally developed by Ballim and Wilks (1991) to model the beliefs and meta-beliefs of a system using nested belief structures. ViewGen is able to reason about its own and other agent's beliefs using belief ascription and inference techniques, The current version of ViewGen is implemented in Quintus Prolog.</Paragraph>
<Paragraph position="2"> The structure of this paper is as follows: in Section 2, we review and discuss previous speech act approaches and their representation of mental attitudes. We argue that precomputed highly nested belief structures aren't necessary. In Section 3, we describe how ViewGen represents mental attitudes and computes nested structures by a process of ascrip- null tion and in Section 4, show how such techniques can be used to represent speech acts for use in planning and plan recognition. Finally, in Section 5, we discuss some implications and future directions of our work 2. Speech acts and mental attitudes It is clear that any understanding of an utterance must involve reference to the attitudes of the speaker. For example, the full understanding of the utterance &amp;quot;Do you know where Thomas is?&amp;quot; depends upon whether the speaker already knows where Thomas is and whether he or she believes the hearer knows.</Paragraph>
<Paragraph position="3"> Speech act based AI approaches normally make reference to mental attitudes and often provide links between the surface form of the utterance and the mental attitudes of both the speaker and hearer. For example, Appelt (1985) describes a system which generates discourse from an intensional logic representation of a set of beliefs. However, as pointed out by Pollack (1990), they have typically used relatively simple models of such attitudes. In particular, previous approaches have lacked any way to model the propagation of belief within the system itself and instead have made use of precomputed and fixed nestings of mental attitudes.</Paragraph>
<Paragraph position="4"> One widely used concept in speech act accounts is mutual belief. Following work in philosophy by Lewis (1969), Clark and Marshall (1981) introduced the notion of mutual belief to account for hearer attitudes. A proposition P is a mutual belief if shared by two agents A and B such that:  etc., ad infinitum There cannot be a logical limit to the number of levels of regression since, as Schiffer (1972) argued, for any level of nested belief, a dialogue example can be constructed which requires an additional level of belief nesting. Because of this potentially infinite regression, it has proven difficult to use an axiomatic definition of mutual belief based in terms of simple belief in computational implementations. Alternative approaches have either avoided defining axioms for mutual belief, e.g. Taylor and Whitehill (1981) or defined it as a primitive operator without reference to simple beliefs, e.g. Cohen and Levesque (1985).</Paragraph>
<Paragraph position="5">  Despite such work, it appears that the mutual belief hypothesis, i.e. that agents compute potentially infinite nestings of belief in comprehension, appears to be too strong a hypothesis to be realistic. It is impossible that agents perform this kind of potentially infinite nesting during real dialogue and no clear constraint can be given on how many iterations would be necessary in a real dialogue situation. Though examples can be artificially created which require n levels of nesting for large n, during a study of dialogue corpora, Lee (1994) found no need for highly nested belief models. In fact, it appears that no dialogue exchange required more than a two level belief nesting. Also, mistakes in assuming what was common to both agents in a dialogue occurred but were quickly repaired through the use of corrections and repetitions and other dialogue control acts. Similar results have been reported by Taylor and Carletta (1994) in analysing the HCRC Map Task corpus.</Paragraph>
<Paragraph position="6"> Rather than compute nested beliefs to some fixed level during comprehension. It is far more plausible that agents compute nested representations on so that highly nested belief representations are only constructed if required in the dialogue. This is the basic principle behind ViewGen.</Paragraph>
</Section>
</Paper>

