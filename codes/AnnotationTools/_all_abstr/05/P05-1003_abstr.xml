<?xml version="1.0" standalone="yes"?>

<Paper uid="P05-1003">
<Title>Logarithmic Opinion Pools for Conditional Random Fields</Title>
<Section position="1" start_page="0" end_page="1" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit. The standard approach to regularising CRFs involves a prior distribution over the model parameters, typically requiring search over a hyperparameter space. In this paper we address the overfitting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual &amp;quot;expert&amp;quot; CRF distributions. We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs). We apply the LOP-CRF to two sequencing tasks. Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.</Paragraph>
<Paragraph position="1">  Introduction In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004). In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation. As a consequence, it is now standard to use some form of overfitting reduction in CRF training. Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005). These advanced methods tend to be difficult to implement and are often computationally expensive. Consequently, due to its ease of implementation, the current standard approach to reducing overfitting in CRFs is the use of a prior distribution over the model parameters, typically a Gaussian. The disadvantage with this method, however, is that it requires adjusting the value of one or more of the distribution's hyperparameters. This usually involves manual or automatic tuning on a development set, and can be an expensive process as the CRF must be retrained many times for different hyperparameter values. In this paper we address the overfitting problem in CRFs from a different perspective.</Paragraph>
<Paragraph position="2"> We factor the CRF distribution into a weighted product of individual expert CRF distributions, each focusing on a particular subset of the distribution. We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs), and provide a procedure for learning the weight of each expert in the product. The LOPCRF framework is &amp;quot;parameter-free&amp;quot; in the sense that it does not involve the requirement to adjust hyperparameter values. LOP-CRFs are theoretically advantageous in that their Kullback-Leibler divergence with a given distribution can be explicitly represented as a function of the KL-divergence with each of their expert distributions. This provides a well-founded framework for designing new overfitting reduction schemes: Proceedings of the 43rd Annual Meeting of the ACL, pages 1825, Ann Arbor, June 2005. c 2005 Association for Computational Linguistics filook to factorise a CRF distribution as a set of diverse experts. We apply LOP-CRFs to two sequencing tasks in NLP: named entity recognition and part-of-speech tagging. Our results show that combination of unregularised expert CRFs with an unregularised standard CRF under a LOP can outperform the unregularised standard CRF, and attain a performance level that rivals that of the regularised standard CRF. LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.</Paragraph>
<Paragraph position="3"> E p(o,s) [ fk ] - E p(s|o) [ fk ] = 0, k ~ In general this cannot be solved for the k in closed form so numerical routines must be used. Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems. In all our experiments we use the LMVM method to train the CRFs. For CRFs with general graphical structure, calculation of E p(s|o) [ fk ] is intractable, but for the linear chain case Lafferty et al. (2001) describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models.</Paragraph>
<Paragraph position="4"> Conditional Random Fields A linear chain CRF defines the conditional probability of a state or label sequence s given an observed sequence o via1 : 1 exp Z(o)</Paragraph>
<Section position="1" start_page="1" end_page="1" type="sub_section">
<SectionTitle>
Logarithmic Opinion Pools
</SectionTitle>
<Paragraph position="0"> where T is the length of both sequences, k are parameters of the model and Z(o) is the partition function that ensures (1) represents a probability distribution. The functions f k are feature functions representing the occurrence of different events in the sequences s and o. The parameters k can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences. The log-likelihood is given by: L ( ) = = - ~ p(o, s) log</Paragraph>
<Paragraph position="2"> In this paper an expert model refers a probabilistic model that focuses on modelling a specific subset of some probability distribution. The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004). In this paper we restrict ourselves to sequence models. Given a set of sequence model experts, indexed by , with conditional distributions p (s  |o) and a set of non-negative normalised weights w , a logarithmic opinion pool 2 is defined as the distribution: pLOP (s  |o) = 1 [p (s  |o)]w ZLOP (o) (2) where p(o, s) and p(o) are empirical distributions ~ ~ defined by the training set. At the maximum likelihood solution the model satisfies a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: this paper we assume there is a one-to-one mapping between states and labels, though this need not be the case.</Paragraph>
<Paragraph position="3"> with w 0 and w = 1, and where ZLOP (o) is the normalisation constant: ZLOP (o) = [p (s  |o)]w (1999) introduced a variant of the LOP idea called Product of Experts, in which expert distributions are multiplied under a uniform weight distribution.</Paragraph>
</Section>
</Section>
</Paper>

