<?xml version="1.0" standalone="yes"?>

<Paper uid="P02-1026">
<Title>Entropy Rate Constancy in Text</Title>
<Section position="1" start_page="0" end_page="0" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> We present a constancy rate principle governing language generation. We show that this principle implies that local measures of entropy (ignoring context) should increase with the sentence number. We demonstrate that this is indeed the case by measuring entropy in three di erent ways. We also show that this e ect has both lexical (which words are used) and non-lexical (how the words are used) causes.</Paragraph>
</Section>
</Paper>

