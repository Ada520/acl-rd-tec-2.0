<?xml version="1.0" standalone="yes"?>

<Paper uid="J02-4004">
<Title>c(c) 2002 Association for Computational Linguistics Efficiently Computed Lexical Chains as an Intermediate Representation for Automatic Text Summarization</Title>
<Section position="2" start_page="0" end_page="489" type="abstr">
<SectionTitle>
1. Introduction
</SectionTitle>
<Paragraph position="0"> The overall motivation for the research presented in this article is the development of a computationally efficient system to create summaries automatically. Summarization has been viewed as a two-step process. The first step is the extraction of important concepts from the source text by building an intermediate representation of some sort.</Paragraph>
<Paragraph position="1"> The second step uses this intermediate representation to generate a summary (Sparck Jones 1993).</Paragraph>
<Paragraph position="2"> In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document. We present a linear-time algorithm for lexical chain computation and offer an evaluation that indicates that such chains are a promising avenue of study as an intermediate representation in the summarization process.</Paragraph>
<Paragraph position="3"> Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process. Attempts to determine the benefit of this proposal have been faced with a number of difficulties. First, previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated, but with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997).</Paragraph>
<Paragraph position="4"> Because of this, computing lexical chains for documents of any reasonable size has been impossible. We present here an algorithm for computing lexical chains that is linear in space and time. This algorithm makes the computation of lexical chains computationally feasible even for large documents.</Paragraph>
<Paragraph position="5">  [?] Department of Computer and Information Sciences, Newark, DE 19711. E-mail: silber@udel.edu + Department of Computer and Information Sciences, Newark, DE 19711. E-mail: mccoy@mail.eecis.  Computational Linguistics Volume 28, Number 4 A second difficulty faced in evaluating Barzilay and Elhadad's proposal is that it is a proposal for the first stage of the summarization process, and it is not clear how to evaluate this stage independent of the second stage of summarization. A second contribution of this article is a method for evaluating lexical chains as an intermediate representation. The intuition behind the method is as follows. The (strong) lexical chains in a document are intended to identify important (noun) concepts in the document. Our evaluation requires access to documents that have corresponding human-generated summaries. We run our lexical chain algorithm both on the document and on the summary and examine (1) how many of the concepts from strong lexical chains in the document also occur in the summary and (2) how many of the (noun) concepts appearing in the summary are represented in strong lexical chains in the document. Essentially, if lexical chains are a good intermediate representation for text summarization, we expect that concepts identified as important according to the lexical chains will be the concepts found in the summary. Our evaluation of 24 documents with summaries indicates that indeed lexical chains do appear to be a promising avenue of future research in text summarization.</Paragraph>
<Section position="1" start_page="488" end_page="489" type="sub_section">
<SectionTitle>
1.1 Description of Lexical Chains
</SectionTitle>
<Paragraph position="0"> The concept of lexical chains was first introduced by Morris and Hirst. Basically, lexical chains exploit the cohesion among an arbitrary number of related words (Morris and Hirst 1991). Lexical chains can be computed in a source document by grouping (chaining) sets of words that are semantically related (i.e., have a sense flow). Identities, synonyms, and hypernyms/hyponyms (which together define a tree of &amp;quot;is a&amp;quot; relations between words) are the relations among words that might cause them to be grouped into the same lexical chain. Specifically, words may be grouped when:  tree. (The truck is fast. The car is faster.) In computing lexical chains, the noun instances must be grouped according to the above relations, but each noun instance must belong to exactly one lexical chain. There are several difficulties in determining which lexical chain a particular word instance should join. For instance, a particular noun instance may correspond to several different word senses, and thus the system must determine which sense to use (e.g., should a particular instance of &amp;quot;house&amp;quot; be interpreted as sense 1, dwelling, or sense 2, legislature). In addition, even if the word sense of an instance can be determined, it may be possible to group that instance into several different lexical chains because it may be related to words in different chains. For example, the word's sense may be identical to that of a word instance in one grouping while having a hypernym/hyponym relationship with that of a word instance in another. What must happen is that the words must be grouped in such a way that the overall grouping is optimal in that it creates the longest/strongest lexical chains. It is our contention that words are grouped into a single chain when they are &amp;quot;about&amp;quot; the same underlying concept.</Paragraph>
<Paragraph position="1">  Silber and McCoy Efficient Lexical Chains for Summarization</Paragraph>
</Section>
</Section>
</Paper>

