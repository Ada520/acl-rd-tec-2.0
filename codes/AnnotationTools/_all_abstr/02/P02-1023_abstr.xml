<?xml version="1.0" standalone="yes"?>

<Paper uid="P02-1023">
<Title>Improving Language Model Size Reduction using Better Pruning Criteria</Title>
<Section position="2" start_page="1" end_page="1" type="abstr">
<SectionTitle>
Abstract
</SectionTitle>
<Paragraph position="0"> Reducing language model (LM) size is a critical issue when applying a LM to realistic applications which have memory constraints. In this paper, three measures are studied for the purpose of LM pruning. They are probability, rank, and entropy. We evaluated the performance of the three pruning criteria in a real application of Chinese text input in terms of character error rate (CER). We first present an empirical comparison, showing that rank performs the best in most cases.</Paragraph>
<Paragraph position="1"> We also show that the high-performance of rank lies in its strong correlation with error rate. We then present a novel method of combining two criteria in model pruning. Experimental results show that the combined criterion consistently leads to smaller models than the models pruned using either of the criteria separately, at the same CER.</Paragraph>
</Section>
</Paper>

