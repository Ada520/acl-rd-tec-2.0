<?xml version="1.0" standalone="yes"?>

<Paper uid="W97-1303">
<Title>United Kingdom</Title>
<Section position="2" start_page="0" end_page="16" type="abstr">
<SectionTitle>
ABSTRACT
</SectionTitle>
<Paragraph position="0"> The paper discusses the significance of factors in anaphora resolution and on the basis of a comparative study argues that what matters is not only a good set of reliable factors but also the strategy for their application. The objective of the study was to find out how well the same set of factors worked within two different computational strategies. To this end, we tuned two anaphora resolution approaches to use the same core set of factors. The first approach uses constraints to discount implausible candidates and then consults preferences to rank order the most likely candidate. The second employs only preferences and does not discard any candidate but assumes initially that the candidate examined is the antecedent; on the basis of uncertainty reasoning formula this hypothesis is either rejected or accepted.</Paragraph>
<Paragraph position="1"> The last section of the paper addresses some related unresolved issues which need further research. null I. Approaches and factors in anaphora resolution Approaches to anaphora resolution usually rely on a set of &amp;quot;anaphora resolution factors&amp;quot;. Factors used frequently in the resolution process include gender and number agreement, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity etc. These factors can be &amp;quot;eliminating&amp;quot; i.e. discounting certain noun phrases from the set of possible candidates (such as gender and number constraints, c-command constraints, semantic consistency) or &amp;quot;preferential&amp;quot;, giving more preference to certain candidates and less to others (such as parallelism, salience). Computational linguistics literature uses diverse terminology for these - for example E.</Paragraph>
<Paragraph position="2"> Rich and S. LuperFoy (\[Rich &amp; LuperFoy 88\]) refer to the &amp;quot;eliminating&amp;quot; factors as &amp;quot;constraints&amp;quot;, and to the preferential ones as &amp;quot;proposers&amp;quot;, whereas Carbonell and Brown (\[Carbonell &amp; Brown 88\] use the terms &amp;quot;constraints&amp;quot; and &amp;quot;preferences&amp;quot;. Other authors argue that all factors should be regarded as preferential, giving higher preference to more restrictive factors and lower to less &amp;quot;absolute&amp;quot; ones, calling them simply &amp;quot;factors&amp;quot; (\[PreuB et al. 94\]), &amp;quot;attributes&amp;quot; (\[P6rez 94\]) or &amp;quot;symptoms&amp;quot; (\[Mitkov 95\]).</Paragraph>
<Paragraph position="3"> The impact of different factors and/or their co-ordination have already been described in the literature (e.g. \[Carter 90\], \[Dagan et al. 91\]). In his work David Carter argues that a flexible control structure based on numerical scores assigned to preferences allows greater co-operation between factors as opposed to a more limited depth-first architecture. His discussion is grounded in comparisons between two different implemented systems - SPAR (\[Carter 87\]) and the SRI Core Language Engine (\[Alshawi 90\]). I. Dagan, J. Justeson, Sh. Lappin, H. Leass and A. Ribak (\[Dagan et al. 91\] attempt to determine the relative importance of distinct informational factors by comparing a syntactically-based salience algorithm for pronominal anaphora resolution (RAP) (\[Lappin &amp; Leass 94\]) with a procedure for reevaluating the decisions of the algorithm on the basis of statistically modelled lexical semantic/pragmatic preferences (\[Dagan 92\]). Their results suggest that syntactically measured salience preferences are dominant in anaphora resolution.</Paragraph>
<Paragraph position="4"> While a number of approaches use a similar set of factors, the &amp;quot;computational strategies&amp;quot; for the application of these factors may differ (by &amp;quot;computational strategy&amp;quot; we mean here the way antecedents are computed, tracked down, i.e. the algorithm, formula for assigning antecedents and not computational issues related to programming languages, complexity etc.). Some approaches incorporate a traditional model which discounts unlikely candidates until a minimal set of plausible candidates is obtained (then make use of center or focus, for instance), whereas others compute the most likely candidate on the basis of statistical or AI techniques/models. This observation led us to term the approaches to anaphora resolution &amp;quot;traditional knowledge-based&amp;quot; and &amp;quot;alternative&amp;quot;  (\[Mitkov 96\]). In the experiment ~ described below, we have kept the set of factors constant and sought to explore which of two approaches, different in terms of &amp;quot;computational strategy&amp;quot; (\[Mitkov 94a\], \[Mitkov 95\]) was the more successful.</Paragraph>
<Paragraph position="5"> In the first of the two approaches, constraints rule out impossible candidates and those left are further evaluated according to various preferences and heuristics but above all the &amp;quot;opinion&amp;quot; of the discourse module which strongly suggests the center of the previous clause as the most likely antecedent. The second approach regards all candidates as equal to start with and seeks to collect evidence about how plausible each candidate is on the basis of the presence/absence of certain symptoms (influence/non-influence of certain factors).</Paragraph>
<Paragraph position="6"> All factors (symptoms) are unconditional preferences (i.e. there are no &amp;quot;absolute&amp;quot;, &amp;quot;ruling out&amp;quot; factors) and are assigned numerical values. Candidates are proposed or rejected as antecedents by an uncertainty reasoning hypothesis verification formula. From the results obtained, we shall see that some of our conclusions coincide with Carter's.</Paragraph>
<Paragraph position="7"> Further, we shall see that to achieve improved performance, a compromise, two-engine approach incorporating both strategies is an even better option. null The results of this study have an implication for building a practical anaphora resolution system: what matters is not only the careful selection of factors, but also the choice of approach (e.g. traditional or statistic, AI etc.) or combination of approaches. null 2. Comparing two different approaches using the same factors Before discussing the results of our comparative study, we shall briefly outline the approaches which served as a basis for the experiment.</Paragraph>
<Section position="1" start_page="14" end_page="14" type="sub_section">
<SectionTitle>
2.1 The integrated anaphora resolution
</SectionTitle>
<Paragraph position="0"> approach (\[Mitkov 94a\]) The Integrated Approach (IA) relies on both constraints and preferences, with constraints discounting implausible candidates, and preferences working towards the selection of the most likely antecedent. The IA is built on modules consisting of different types of rule-based knowledge - syntactic, semantic, domain, discourse and heuristic (\[Mitkov 94a\]).</Paragraph>
<Paragraph position="1"> The syntactic module, for example, knows that the anaphor and antecedent must agree in number, gender and person. It checks whether the c-command constraints hold and establishes disjoint reference. In cases of syntactic parallelism, it prefers the noun phrase with the same syntactic role ~The idea for this study was suggested by Allan Ramsey as the anaphor as the most probable antecedent. It knows when cataphora is possible and can indicate syntactically topicalised noun phrases, which are more likely to be antecedents than non-topicalised ones.</Paragraph>
<Paragraph position="2"> The semantic module checks for semantic consistency between the anaphor and the possible antecedent. It filters out semantically incompatible candidates following verb semantics or animacy of the candidate. In cases of semantic parallelism, it prefers the noun phrase which has the same semantic role as the anaphor as the most likely antecedent. null The syntactic and semantic modules are enhanced by a discourse module which plays a very important role because it keeps a track of the centers of each discourse segment (it is the center which is, in most cases, the most probable candidate for an antecedent). Based on empirical studies from the sublanguage of computer science, we have developed a statistical approach to determine the probability of a noun (verb) phrase to be the center of a sentence. Unlike other approaches known to us, our method is able to propose the center with a high probability in every discourse sentence, including the first. The approach uses an inference engine based on Bayes' formula which draws an inference in the light of some new piece of evidence. This formula calculates the new probability, given the old probability plus some new piece of evidence (\[Mitkov 94b\]).</Paragraph>
<Paragraph position="3"> The domain knowledge module is a small knowledge base of the concepts of the domain considered, while the heuristic knowledge module is a set of useful rules (e.g. the antecedent is likely to be located in the current sentence or in the previous one) which can forestall certain impractical search procedures.</Paragraph>
<Paragraph position="4"> The referential expression filter plays an important role in filtering out expressions where 'it' is not anaphoric (e.g. &amp;quot;it is important&amp;quot;, &amp;quot;it is necessary&amp;quot;, &amp;quot;it should be pointed out&amp;quot; etc.). The IA operates as follows. Syntax and semantic constraints (agreement, configurational, semantic consistency) reduce the set of all candidates to the set of possible ones. If the latter consists of more than one noun phrase, then preferences are activated. Highest preference (score) is given to noun phrases which are the center of the previous clause, but syntactic parallelism, semantic parallelism and referential distance also contribute (though less significantly) to the overall score.</Paragraph>
</Section>
<Section position="2" start_page="14" end_page="16" type="sub_section">
<SectionTitle>
2.2 The uncertainty reasoning approach
</SectionTitle>
<Paragraph position="0"> (\[Mitkov 95\]).</Paragraph>
<Paragraph position="1"> The Uncertainty Reasoning Approach (URA) uses AI uncertainty reasoning techniques. Uncertainty reasoning was selected as an alternative because:  * In Natural Language Understanding, the program is likely to estimate the antecedent of an anaphor on the basis of incomplete information: even if information about constraints and preferences is available, one can to assume that a Natural Language Understanding program is not able to understand the input completely ; * The necessary initial constraint and preference scores are determined by humans; therefore the scores are originally subjective and should be regarded as uncertain facts.</Paragraph>
<Paragraph position="2"> The uncertainty reasoning approach makes use of &amp;quot;standard&amp;quot; anaphor resolution &amp;quot;symptoms&amp;quot; such as agreement, c-command constraints, parallelism, topicalisation, verb-case roles, but also of further symptoms based on empirical evidence, such as subject preference, domain concept preference, object preference, section head preference, reiteration preference, definiteness preference, main clause preference etc. Note that this strategy does not regard factors as absolute constraints; all symptoms are in practice preferences with numerical values assigned.</Paragraph>
<Paragraph position="3"> More specifically, the presence/absence of a certain symptom corresponds to an appropriate score - certainty factor (CF) which is attached to it. For instance, the presence of a certain symptom s assigns CFsp r (0&lt;CFspr&lt;l), whereas the absence corresponds to CFsa b (-l&lt;CFsab_&lt;0). For easier reference and brevity, we associate with the symptom s only the certainty factor CF s which we regard as a two-value function (CF s ~ {CFsp r,  CFsab})-The antecedent searching procedure employs an uncertainty reasoning strategy: the search for an antecedent is regarded as an affirmation (or rejection) of the hypothesis that a certain noun phrase is the correct antecedent. The certainty factor (CF) serves as a quantitative approximation of the hypothesis. The presence/absence of each symptom s causes recalculation (increase or decrease) of the global hypothesis certainty factor CFhy p until: CFhyp &gt; CFthreshold for affirmation or CFhy p &lt; CFmin for rejection of the hypothesis. Hypothesis verification operates from right to left: first the closest to the anaphor noun phrase is tried. If this noun phrase does not survive the hypothesis of being the antecedent, the next rightmost is tried and so on.</Paragraph>
<Paragraph position="4"> We use a hypothesis verification formula for recalculation of the hypothesis on the basis of presence (in our case also of absence) of certain symptoms. Our formula is a modified version of van Melle's formula in (\[Buchanan &amp; Shortliffe 841).</Paragraph>
<Paragraph position="5">  CFhy p (s, CFol d) =</Paragraph>
<Paragraph position="7"> CFol d &lt;0 or CFs&gt;O, CFol d &lt;0 or - CFhy p (s, CFol d) ~ CF s &lt;0, CFol d &lt;0 where CFhy p (s, CFold) is the hypothesis certainty factor, contributed by the presence/absence of symptom s and the current (old) hypothesis certainty factor CFol d. As an illustration, suppose a certain NP has reached a CF=0.5 after testing the presence of some symptoms (e.g. syntactic agreement) and that the symptom s with CF=0.45 holds. Then CFhyp (s, CFold) = 0.5+0.45-0.5*0.45=0.725 2.3. The same set of factors but different computational strategies The objective of the study was to compare the IA and the URA with both using the same repertoire of factors to see what was the imPact of the different computational strategies.</Paragraph>
<Paragraph position="8">  We used the same set of factors in both approaches - the factors selected were deemed to be a &amp;quot;core set&amp;quot; from the point of view of both approaches. The factors used in our experiment were:  Anaphors and their antecedents must agree in number and gender.</Paragraph>
<Paragraph position="9"> Syntactic parallelism Preference is given to antecedents with the same syntactic function as the anaphor. null The programmer, combined successfully Prolog. 1 j with C, but he i had combined itj with Pascal last time.</Paragraph>
<Paragraph position="10"> The programmer i combined successfully Prolog with Cj, but he i had combined Pascal with itj last time.</Paragraph>
<Paragraph position="11"> Topicalisation Topicalised structures are given preferential treatment as possible antecedents. It was Ruslan. who convinced me to go to Madrid. ! Why did he i do it? Semantic consistency If satisfied by the anaphor, semantic consistency constraints must be satisfied also by its antecedent.</Paragraph>
<Paragraph position="12"> Vincent removed the diskette from the computer i and then disconnected it i.</Paragraph>
<Paragraph position="13"> Vincent removed the diskette i from the computer and then copied it i.</Paragraph>
<Paragraph position="14"> Semantic parallelism Those antecedents are favoured which have the same semantic role as the anaphor. null Vincent gave the diskette to Sody i. Kim also gave him i a letter.</Paragraph>
<Paragraph position="15"> Vincent i gave the diskette to Sody. He i also gave Kim a letter.</Paragraph>
</Section>
<Section position="3" start_page="16" end_page="16" type="sub_section">
<SectionTitle>
Subjects
</SectionTitle>
<Paragraph position="0"> From the list of potential candidates the subject of the previous sentence (clause) is the preferred antecedent; the second preferred antecedent is the direct object.</Paragraph>
<Paragraph position="1"> Domain concepts The NP representing a domain concept is preferred to NPs which are not domain concepts.</Paragraph>
<Paragraph position="2"> The last two preferences can be illustrated by the example: When the Prolog system i finds a solution to a query, it i will print the values given to variables used in the query.</Paragraph>
<Paragraph position="3"> Object preference indicated by verbs If the verb is a member of the Verb_set = {discuss, present, illustrate, summarise, examine, describe, define, show, check, develop, review, report, outline, consider, investigate, explore, assess, analyse, synthesise, study, survey, deal, cover}, then consider the object as the preferred antecedent.</Paragraph>
<Paragraph position="4"> Object preference indicated by nouns If the subject is &amp;quot;chapter&amp;quot;, &amp;quot;section&amp;quot;, &amp;quot;table&amp;quot;, &amp;quot;document&amp;quot;, &amp;quot;paper&amp;quot; etc. or a personal pronoun 'T', &amp;quot;we&amp;quot;, &amp;quot;you&amp;quot;, then consider the object as the preferred antecedent. null This table shows a minimal configurationi; it i does not leave much room for additional applications or other software for which you may require additional swap space.</Paragraph>
<Paragraph position="5"> Repetition Repeated NPs are considered to be the preferred candidate for antecedent.</Paragraph>
</Section>
</Section>
</Paper>

