<?xml version="1.0" standalone="yes"?>

<Paper uid="W97-1402">
<Title>Referring in Multimodal Systems: The Importance of User Expertise and System Features</Title>
<Section position="2" start_page="0" end_page="0" type="abstr">
<SectionTitle>
1 Introduction
</SectionTitle>
<Paragraph position="0"> Multimodal communication is used frequently and efficiently by humans to identify objects in physical space. By combining different modalities (e.g.</Paragraph>
<Paragraph position="1"> speech and gestures), multimodal references act as efficient tools for coping with the complexity of the physical space -as conveyed by visual perceptionwhich can be communicate only partially by verbal expressions (Glenberg and McDaniel, 1992). Therefore, multimodal references can easily substitute too complex, too detailed, ambiguous, or undetermined verbal expressions. In particular, they simplify referent identification when the speaker or the hearer do not know the name of the target, or how to describe it.</Paragraph>
<Paragraph position="2"> For a long time, face-to-face communication has been considered a reliable model for natural language based human-computer interaction (hencepirical work is available on what actually happens in multimodal HCI and how communication features cohabit with modern graphical interfaces (Oviatt, 1996; De Angeli et al., 1996; Oviatt et al., 1997; Siroux et al., 1995). Moreover, with only a few exceptions (Buxton, 1991; Brennan, 1991; Stock, 1995), direct manipulation interfaces have been seen as an antagonist of conversational interfaces, hindering a desirable synergy between the two communication styles.</Paragraph>
<Paragraph position="3"> We believe that in multimodal HCI users find strategies that overcome natural language communication or direct manipulation paradigm alone, creating a new mixed communication form that makes the best use of both (Oviatt, 1996; Oviatt et al., 1997).</Paragraph>
<Paragraph position="4"> The new communication, even if similar in principle to face-to-face communication, might be carried out in a far different fashion because one partner is a computer. As a matter of fact, some studies showed that people do adopt conversational and social rules when interacting with computers (Nass et al., 1994) but humans also design their utterances with the special partner in mind (Brennan, 1991).</Paragraph>
<Paragraph position="5"> Empirical studies on natural language human-computer interaction confirm general HCI peculiarities. Talkingto a computer humans maintain a conversational framework but tend to simplify the syntactic structure, to reduce utterances length, lexicon richness and use of pronouns (Jonsson and Dahlback, 1988; Dahlback and Jonsson, 1989; Oviatt, 1995). In other words users select a simplified register to interact with computers even if the (simulated) system has human capabilities (De Angeli, 1991).</Paragraph>
<Paragraph position="6"> These results suggest that face-to-face communication is not an adequate model for HCI. Therefore empirical studies are needed to develop predictive models of multimodal communication in HCI.</Paragraph>
<Paragraph position="7"> Empirical research becomes even more important when a multimodal system reproduces an unnatural modality combination, such as writing combined with pointing. Pointing while writing is highly different from pointing while speaking. In the first case, Referring in Multimodal Systems 15 in fact, multimodal communication is hampered by a single-modality-production constraint. Indeed, the requirement of moving the dominant hand back and forth between the keyboard and a pointing device implies a substitution of the natural parallel synchronization pattern (Levelt et al., 1985) with an unnatural sequential one. Nevertheless, the obligation of using the same effector for writing and pointing does not seem to have any inhibitory effect on multimodal input production (De Angeli et al., 1996).</Paragraph>
<Paragraph position="8"> In general, deixis 1 was found to be the most frequent referent identification strategy adopted by users to indicate objects. However, its occurrence depends strongly on the effort needed to indicate the target by a pure verbal reference. Moreover, we found a relevant percentage of redundant references 2, a strategy mentioned in the relevant literature only for speech and pointing (Siroux et al., 1995) and pretty different from common face-to-face communication strategies (De Angeli et al., 1996).</Paragraph>
<Paragraph position="9"> Following the iterative design principles (Nielsen, 1993), we assume that experimental research, in the form of early simulations, should improve multi-modal design by allowing to formulate reliable guidelines. From this point of view, the purpose of this paper is to investigate the effect of user expertise and system features on multimodal interaction in order to infer useful guidelines for future systems.</Paragraph>
<Paragraph position="10"> 2 HCI issues in multimodal referring In a HCI context where the user is required to write and point, we analyze the communication strategies that users spontaneously adopted at the very beginning of interaction. The main purpose of analyzing users' spontaneous behavior is to develop design guidelines that might be taken into account when developing multimodal systems that have to support successful interaction from the very beginning, like &amp;quot;walk-up-and-use&amp;quot; interfaces.</Paragraph>
<Paragraph position="11"> Results of a simulation experiment have been analyzed to answer the following question: Is multimodal interaction really instinctive, i.e. do naive users perform as experienced ones? In general, multimodal systems appear to improve HCI by allowing humans to communicate in a more spontaneous way (Oviatt and Olsen, 1994; Oviatt, 1996). Therefore, one could infer that multimodal communication is the best interaction style for naive 1Deixis concerns the ways in which languages encode or grammaticalize features of the context of utterance or speech event (Levinson, 1983). Among the various types we consider here only space or place deixis used in a gestural way.</Paragraph>
<Paragraph position="12"> 2We defined redundant reference as multimodal references composed by a full linguistic reference and a not needed additional pointing.</Paragraph>
<Paragraph position="13"> users. However, some authors suggest that language based interaction is mainly suitable for experienced users (Hutchins et al., 1986; Gentner and Nielsen, 1996). Indeed the opacity of language allows very flexible interaction, but requires previous knowledge 3. We believe that experience, defined as computer science literacy, may increase the efficient use of multimodality. The notion of efficiency is defined, following (Mac Aogain and Reilly, 1990), as the capacity of the multimodal input to derive important semantic parts from information channels other than language, i.e. from pointing. In other words, efficiency is operationalized as the proportion of written input replaced by the gestural one.</Paragraph>
</Section>
</Paper>

