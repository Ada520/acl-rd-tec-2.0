<?xml version="1.0" encoding="UTF-8"?><Paper acl-id="H05-1117"><Title>Automatically Evaluating Answers to Definition Questions</Title><Section end_page="0" position="1" start_page="0" type="abstr"><SectionTitle>Abstract</SectionTitle><S position="0"> Following recent developments in the <term class="tech">automatic evaluation of machine translation and document summarization</term>, we present a similar approach, implemented in a measure called <term class="other">POURPRE</term>, for automatically evaluating answers to <term class="other">definition questions</term>.</S><S position="1"> Until now, the only way to assess the correctness of answers to such <term class="other">questions</term> involves manual determination of whether an <term class="other">information nugget</term> appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the <term class="other">TREC 2003</term> and <term class="other">TREC 2004</term> <term class="tech">QA</term> tracks indicate that rankings produced by our metric correlate highly with official rankings, and that <term class="other">POURPRE</term> outperforms direct application of existing metrics.</S></Section></Paper>