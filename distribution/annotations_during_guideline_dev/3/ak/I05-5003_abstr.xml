<?xml version="1.0" encoding="UTF-8"?><Paper acl-id="I05-5003"><Title>Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence</Title><Section end_page="0" position="1" start_page="0" type="abstr"><SectionTitle>Abstract</SectionTitle><S position="0"> The task of <term class="tech">machine translation (MT) evaluation</term> is closely related to the task of <term class="tech">sentence-level semantic equivalence classification</term>. This paper investigates the utility of applying standard <term class="tech">MT evaluation methods</term> (<term class="tool">BLEU</term>, <term class="tool">NIST</term>, <term class="tool">WER</term> and <term class="tool">PER</term>) to building <term class="tech">classifiers</term> to predict <term class="other">semantic equivalence</term> and <term class="other">entailment</term>. We also introduce a novel <term class="tech">classification method</term> based on <term class="tool">PER</term> which leverages <term class="other">part of speech information</term> of the words contributing to the word matches and non-matches in the sentence. Our results show that <term class="tech">MT evaluation techniques</term> are able to produce useful features for <term class="tech">paraphrase classification</term> and to a lesser extent <term class="other">entailment</term>. Our technique gives a substantial improvement in <term class="other">paraphrase classification accuracy</term> over all of the other models used in the experiments.</S></Section></Paper>