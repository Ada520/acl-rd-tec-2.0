<?xml version="1.0" encoding="UTF-8" standalone="yes"?><Paper acl-id="H05-1117">   <Title>Automatically Evaluating Answers to Definition Questions</Title>   <Section end_page="0" position="1" start_page="0" type="abstr">      <SectionTitle>Abstract</SectionTitle>      <S position="0"> Following recent developments in the <term class="tech">automatic evaluation</term> of <term class="tech">machine translation</term> and <term class="tech">document summarization</term>, we present a similar <term class="tech">approach</term>, implemented in a <term class="other">measure</term> called <term class="other">POURPRE</term>, for automatically <term class="tech">evaluating answers to definition questions</term>.</S>      <S position="1"> Until now, the only way to assess the <term class="other">correctness of answers</term> to such <term class="other">questions</term> involves manual determination of whether an <term class="other">information nugget</term> appears in a <term class="other">system's response</term>. The lack of <term class="tech">automatic methods for scoring system output</term> is an impediment to progress in the field, which we address with this work. Experiments with the <term class="other">TREC 2003 and TREC 2004 QA tracks</term> indicate that <term class="other">rankings</term> produced by our <term class="other">metric</term> correlate highly with official <term class="other">rankings</term>, and that <term class="other">POURPRE</term> outperforms direct application of existing <term class="other">metrics</term>.</S>   </Section></Paper>